{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab960516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:41.419324Z",
     "iopub.status.busy": "2025-05-19T21:57:41.419216Z",
     "iopub.status.idle": "2025-05-19T21:57:41.916174Z",
     "shell.execute_reply": "2025-05-19T21:57:41.915790Z",
     "shell.execute_reply.started": "2025-05-19T21:57:41.419312Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "mplbackend = 'agg'\n",
    "assert mplbackend in ('agg', 'retina')\n",
    "if (mplbackend == 'agg'):\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "elif (mplbackend == 'retina') and (importlib.util.find_spec(\"matplotlib_inline\") is not None):\n",
    "    import matplotlib_inline\n",
    "    matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "elif (mplbackend == 'retina'):\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('retina')\n",
    "else:\n",
    "    raise ValueError(f'undefined matplotlib backend {mplbackend}')\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8043e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:41.917649Z",
     "iopub.status.busy": "2025-05-19T21:57:41.917484Z",
     "iopub.status.idle": "2025-05-19T21:57:45.623054Z",
     "shell.execute_reply": "2025-05-19T21:57:45.622512Z",
     "shell.execute_reply.started": "2025-05-19T21:57:41.917636Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import math\n",
    "import fnmatch\n",
    "import pathlib\n",
    "import warnings\n",
    "import datetime\n",
    "import tensorboardX\n",
    "from torch import nn\n",
    "from os.path import isdir\n",
    "from os.path import exists\n",
    "from textwrap import dedent\n",
    "import matplotlib.pyplot as plt\n",
    "from pyinstrument import Profiler\n",
    "\n",
    "from partnn.io_cfg import PROJPATH\n",
    "from partnn.io_cfg import data_dir\n",
    "from partnn.io_cfg import configs_dir\n",
    "from partnn.io_cfg import results_dir\n",
    "from partnn.io_cfg import storage_dir\n",
    "from partnn.io_utils import DataWriter\n",
    "from partnn.io_utils import eval_formula\n",
    "from partnn.io_utils import preproc_cfgdict\n",
    "from partnn.io_utils import hie2deep, deep2hie\n",
    "from partnn.io_utils import parse_refs, get_subdict, resio\n",
    "from partnn.tch_utils import BatchRNG, EMA, profmem, make_nn\n",
    "from partnn.tch_utils import BatchParamGrouper, BatchLRScheduler\n",
    "from partnn.aerometrics import AeroMeasures\n",
    "\n",
    "from ruamel import yaml as ruyaml\n",
    "from partnn.notebooks.n20_utils import unflatten_cfg, get_snrtspltidxs, load_histdata, Timer\n",
    "from partnn.notebooks.n20_utils import make_pp, Evaluation, Visualization, DataPrepper, get_spltcfgs \n",
    "from partnn.notebooks.n20_utils import get_lbldims, load_riopp, load_rioenc, VAELabelNN, IIDLossMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047cff28",
   "metadata": {},
   "source": [
    "# Loading Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f028dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:45.623867Z",
     "iopub.status.busy": "2025-05-19T21:57:45.623646Z",
     "iopub.status.idle": "2025-05-19T21:57:47.311793Z",
     "shell.execute_reply": "2025-05-19T21:57:47.311303Z",
     "shell.execute_reply.started": "2025-05-19T21:57:45.623853Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "json_cfgtreeid = '02_adhoc/20_mlpcnd.yml'\n",
    "# ! rm -rf \"./20_vaehist/results/20_mlpcnd_00.h5\"\n",
    "# ! rm -rf \"./20_vaehist/storage/20_mlpcnd_00\"\n",
    "\n",
    "json_cfgpath = f'{configs_dir}/{json_cfgtreeid}'\n",
    "if json_cfgpath.endswith('.json'):\n",
    "    with open(json_cfgpath, 'r') as fp:\n",
    "        json_cfgdict = json.load(fp, object_pairs_hook=dict)\n",
    "elif json_cfgpath.endswith('.yml'):\n",
    "    with open(json_cfgpath, \"r\") as fp:\n",
    "        json_cfgdict = dict(ruyaml.safe_load(fp))\n",
    "else:\n",
    "    raise RuntimeError(f'unknown config extension: {json_cfgpath}')\n",
    "\n",
    "json_cfgdict['io/config_id'] = '20_mlpcnd_00'\n",
    "json_cfgdict['io/config_id'] = '20_mlpcnd_00'\n",
    "json_cfgdict['io/results_dir'] = './20_vaehist/results'\n",
    "json_cfgdict['io/storage_dir'] = './20_vaehist/storage'\n",
    "json_cfgdict['io/tch/device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Applying the looping processes\n",
    "all_cfgdicts1 = preproc_cfgdict(json_cfgdict)[:10]\n",
    "# Parsing all the \"/ref\"-, \"/def\"-, and \"/pop\"-ending keys\n",
    "all_cfgdicts = [parse_refs(cfgdict, trnsfrmtn='hie', pathsep=' -> ', \n",
    "    cfg_path=json_cfgtreeid) for cfgdict in all_cfgdicts1]\n",
    "# Dropping the trailing references section\n",
    "all_refdicts = [get_subdict(cfgdict, prefix='refs', pop=True) \n",
    "    for cfgdict in all_cfgdicts]\n",
    "\n",
    "cfg_dict_input = all_cfgdicts[9]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc804a38",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "def main(cfg_dict_input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0be62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:47.312448Z",
     "iopub.status.busy": "2025-05-19T21:57:47.312319Z",
     "iopub.status.idle": "2025-05-19T21:57:47.325962Z",
     "shell.execute_reply": "2025-05-19T21:57:47.325589Z",
     "shell.execute_reply.started": "2025-05-19T21:57:47.312435Z"
    }
   },
   "outputs": [],
   "source": [
    "    ###############################################################################\n",
    "    ###################### Processing the Config Dictionary #######################\n",
    "    ###############################################################################\n",
    "    cfg_dict = cfg_dict_input.copy()\n",
    "\n",
    "    #########################################################\n",
    "    #################### Ignored Options ####################\n",
    "    #########################################################\n",
    "    cfgdesc = cfg_dict.pop('desc', None)\n",
    "    cfgdate = cfg_dict.pop('date', None)\n",
    "\n",
    "    #########################################################\n",
    "    ################### Mandatory Options ###################\n",
    "    #########################################################\n",
    "    prob_type = cfg_dict.pop('problem')\n",
    "    rng_seed_list = cfg_dict.pop('rng_seed/list')\n",
    "\n",
    "    opt_type = cfg_dict.pop('opt/type')\n",
    "    n_epochs = cfg_dict.pop('opt/epoch')\n",
    "    n_mb = cfg_dict.pop('opt/mb/n')\n",
    "    schdlr_type = cfg_dict.pop('opt/schdlr/type', None)\n",
    "    schdlr_hparams = get_subdict(cfg_dict, 'opt/schdlr', pop=True)\n",
    "    mb_rndmztn = cfg_dict.pop('opt/rndmztn')\n",
    "    assert mb_rndmztn in ('shuffle', 'iid', 'none')\n",
    "    opt_hparams = get_subdict(cfg_dict, 'opt', pop=True)\n",
    "\n",
    "    #########################################################\n",
    "    ###################### Data Options #####################\n",
    "    #########################################################\n",
    "    data_cfg = get_subdict(cfg_dict, 'data', pop=False)\n",
    "    data_tree = cfg_dict.pop('data/tree')\n",
    "    data_chems = cfg_dict.pop('data/chems', None)\n",
    "    data_binsi1 =  cfg_dict.pop('data/bins/idx/start', 0)\n",
    "    data_binsi2 =  cfg_dict.pop('data/bins/idx/end', None)\n",
    "\n",
    "    split_cfg = get_subdict(cfg_dict, 'split', pop=True)\n",
    "    #########################################################\n",
    "    ################## Neural Spec Options ##################\n",
    "    #########################################################\n",
    "    nn_mdlscfg = get_subdict(cfg_dict, 'nn/modules', pop=True)\n",
    "    pp_mdlscfg = get_subdict(cfg_dict, 'pp/modules', pop=True)\n",
    "    asgn_mdlscfg = get_subdict(cfg_dict, 'asgn/modules', pop=True)\n",
    "    metric_mdlscfg = hie2deep(get_subdict(cfg_dict, 'metric/modules', pop=True), maxdepth=1)\n",
    "\n",
    "    enc_type = cfg_dict.pop('nn/enc/type')\n",
    "    enc_hparamsraw = get_subdict(cfg_dict, 'nn/enc', pop=True)\n",
    "\n",
    "    dec_type = cfg_dict.pop('nn/dec/type')\n",
    "    dec_hparamsraw = get_subdict(cfg_dict, 'nn/dec', pop=True)\n",
    "\n",
    "    ltnt_dimraw = cfg_dict.pop('nn/ltnt/dim')\n",
    "    ltnt_sigtnsfm = cfg_dict.pop('nn/ltnt/sig/tnsfm', 'exp')\n",
    "\n",
    "    ppnn_type = cfg_dict.pop('nn/pp/type')\n",
    "    ppnn_hparamsraw = get_subdict(cfg_dict, 'nn/pp', pop=True)\n",
    "\n",
    "    lblpp_type = cfg_dict.pop('nn/lbl/pp/type', None)\n",
    "    lblpp_hparamsraw = get_subdict(cfg_dict, 'nn/lbl/pp', pop=True)\n",
    "\n",
    "    lblnn_type = cfg_dict.pop('nn/lbl/nn/type', None)\n",
    "    lblnn_hparamsraw = get_subdict(cfg_dict, 'nn/lbl/nn', pop=True)\n",
    "\n",
    "    #########################################################\n",
    "    ################### Training Criteria ###################\n",
    "    #########################################################\n",
    "    w_kl = cfg_dict.pop('cri/kl/w', None)\n",
    "    w_klmu = cfg_dict.pop('cri/kl/mu/w', w_kl)\n",
    "    w_klsig = cfg_dict.pop('cri/kl/sig/w', w_kl)\n",
    "\n",
    "    rcnst_cri = cfg_dict.pop('cri/rcnst/metric')\n",
    "    ppcri_inpspc = cfg_dict.pop('cri/rcnst/ppinp/space', 'x')\n",
    "    ppcri_rndtrp = cfg_dict.pop('cri/rcnst/ppinp/rndtrp', True)\n",
    "    ppcri_type = cfg_dict.pop('cri/rcnst/pp/type')\n",
    "    ppcri_hparamsraw = get_subdict(cfg_dict, 'cri/rcnst/pp', pop=True)\n",
    "\n",
    "    w_indzy = cfg_dict.pop('cri/indzy/w', None)\n",
    "    n_histindzy = cfg_dict.pop('cri/indzy/hist/n', None)\n",
    "    cri_indzymtrccfg = get_subdict(cfg_dict, 'cri/indzy/mtrc', pop=True)\n",
    "\n",
    "    #########################################################\n",
    "    ################## Evaluation Profiles ##################\n",
    "    #########################################################\n",
    "    evalcfgs = hie2deep(get_subdict(cfg_dict, 'eval', pop=True), maxdepth=1)\n",
    "    perfcfgs = hie2deep(get_subdict(cfg_dict, 'perf', pop=True), maxdepth=1)\n",
    "    vizcfgs = hie2deep(get_subdict(cfg_dict, 'viz', pop=True), maxdepth=1)\n",
    "\n",
    "    mplopts_cfg = get_subdict(cfg_dict, 'mplopts', pop=True)\n",
    "    mplopts_cfg = hie2deep(mplopts_cfg, maxdepth=1)\n",
    "\n",
    "    #########################################################\n",
    "    ################# I/O Logistics Options #################\n",
    "    #########################################################\n",
    "    config_id = cfg_dict.pop('io/config_id')\n",
    "    results_dir = cfg_dict.pop('io/results_dir')\n",
    "    storage_dir = cfg_dict.pop('io/storage_dir', None)\n",
    "    io_avgfrq = cfg_dict.pop('io/avg/frq')\n",
    "    ioflsh_period = cfg_dict.pop('io/flush/frq')\n",
    "    chkpnt_period = cfg_dict.pop('io/ckpt/frq')\n",
    "    device_name = cfg_dict.pop('io/tch/device')\n",
    "    dtype_name = cfg_dict.pop('io/tch/dtype')\n",
    "    iomon_period = cfg_dict.pop('io/mon/frq')\n",
    "    io_cmprssnlvl = cfg_dict.pop('io/cmprssn_lvl')\n",
    "    eval_bs = cfg_dict.pop('io/eval/bs', None)\n",
    "    eval_bsnn = cfg_dict.pop('io/eval/bs/nn', eval_bs)\n",
    "    eval_bspp = cfg_dict.pop('io/eval/bs/pp', eval_bs)\n",
    "    eval_enbl = cfg_dict.pop('io/eval/enbl', True)\n",
    "    do_logtb_ = cfg_dict.pop('io/strg/logtb', None)\n",
    "    do_profile_ = cfg_dict.pop('io/strg/profile', None)\n",
    "    do_savefigs_ = cfg_dict.pop('io/strg/savefigs', None)\n",
    "\n",
    "    # disabling the evaluation if necessary.\n",
    "    evalcfgs = evalcfgs if eval_enbl else dict()\n",
    "    perfcfgs = perfcfgs if eval_enbl else dict()\n",
    "    vizcfgs = vizcfgs if eval_enbl else dict()\n",
    "\n",
    "    dtnow = datetime.datetime.now().isoformat(timespec='seconds')\n",
    "    cfg_tree = '/'.join(config_id.split('/')[:-1])\n",
    "    cfg_name = config_id.split('/')[-1]\n",
    "\n",
    "    # Making sure no other options are left unused.\n",
    "    if len(cfg_dict) > 0:\n",
    "        msg_ = 'The following settings were left unused:\\n'\n",
    "        for key, val in cfg_dict.items():\n",
    "            msg_ += f'  {key}: {val}'\n",
    "        raise RuntimeError(msg_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13296103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:47.326509Z",
     "iopub.status.busy": "2025-05-19T21:57:47.326385Z",
     "iopub.status.idle": "2025-05-19T21:57:47.331448Z",
     "shell.execute_reply": "2025-05-19T21:57:47.331082Z",
     "shell.execute_reply.started": "2025-05-19T21:57:47.326496Z"
    }
   },
   "outputs": [],
   "source": [
    "    ###############################################################################\n",
    "    ################ Initializing the Training Classes and Objects ################\n",
    "    ###############################################################################\n",
    "    # Derived options and assertions\n",
    "    eval_bsnn = n_mb if eval_bsnn is None else eval_bsnn\n",
    "    eval_bspp = n_mb if eval_bspp is None else eval_bspp\n",
    "\n",
    "    assert (lblnn_type is None) == (lblpp_type is None), dedent(f'''\n",
    "        A label {\"pp\" if (lblpp_type is not None) else \"nn\"} was provided, \n",
    "        but a label {\"nn\" if (lblpp_type is not None) else \"pp\"} was not \n",
    "        specified. Either both or neither of them have to be provided.''')\n",
    "\n",
    "    is_condvae = (lblpp_type is not None)\n",
    "    if is_condvae:\n",
    "        assert lblnn_type is not None\n",
    "\n",
    "    if is_condvae and (lblnn_type == 'pretrained'):\n",
    "        lblnn_hparams = lblnn_hparamsraw.copy()\n",
    "        lblnn_enctype = lblnn_hparams.pop('enc/type')\n",
    "        assert lblnn_enctype == 'vae', f'unsupported lbl/nn/enc/type = {lblnn_enctype}'\n",
    "        y_sigscale = lblnn_hparams.pop('enc/sigscale')\n",
    "        y_nodename = lblnn_hparams.pop('enc/nodename')\n",
    "    elif is_condvae:\n",
    "        lblnn_enctype, lblnn_hparams, y_sigscale = 'plain', None, 0\n",
    "    else:\n",
    "        lblnn_enctype, lblnn_hparams, y_sigscale = None, None, None\n",
    "\n",
    "    #########################################################\n",
    "    ########### I/O-Related Options and Operations ##########\n",
    "    #########################################################\n",
    "    name2dtype = dict(float64=(torch.double, torch.complex128), \n",
    "        float32=(torch.float32, torch.complex64),\n",
    "        float16=(torch.float16, torch.complex32))\n",
    "    tch_device = torch.device(device_name)\n",
    "    tch_dtype, tch_cdtype = name2dtype[dtype_name]\n",
    "\n",
    "    has_storage = storage_dir is not None\n",
    "    do_logtb = bool(do_logtb_) if do_logtb_ is not None else has_storage\n",
    "    do_profile = bool(do_profile_) if do_profile_ is not None else has_storage\n",
    "    do_savefigs = bool(do_savefigs_) if do_savefigs_ is not None else has_storage\n",
    "\n",
    "    do_logtb = do_logtb and has_storage\n",
    "    do_profile = do_profile and has_storage\n",
    "    do_savefigs = do_savefigs and has_storage\n",
    "\n",
    "    assert not(do_logtb) or (storage_dir is not None)\n",
    "    assert not(do_profile) or (storage_dir is not None)\n",
    "    assert not(do_savefigs) or (storage_dir is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6ef92",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ba5c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:47.332002Z",
     "iopub.status.busy": "2025-05-19T21:57:47.331884Z",
     "iopub.status.idle": "2025-05-19T21:57:47.377895Z",
     "shell.execute_reply": "2025-05-19T21:57:47.377503Z",
     "shell.execute_reply.started": "2025-05-19T21:57:47.331990Z"
    }
   },
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    ################ Defining the VAE Problem ###############\n",
    "    #########################################################\n",
    "    assert prob_type == 'vaehist'\n",
    "\n",
    "    data_path = f'{data_dir}/{data_tree}'\n",
    "    assert data_path.endswith('.nc')\n",
    "\n",
    "    data_hist = load_histdata(data_path, data_chems, data_binsi1, data_binsi2)\n",
    "    \n",
    "    n_snr = data_hist['n_snr']\n",
    "    n_t = data_hist['n_t']\n",
    "    n_chem = data_hist['n_chem']\n",
    "    n_bins = data_hist['n_bins']\n",
    "    n_waveref = data_hist['n_waveref']\n",
    "\n",
    "    m_chmprthst = data_hist['m_chmprthst']\n",
    "    assert m_chmprthst.shape == (n_snr, n_t, n_chem, n_bins)\n",
    "    m_prthst = data_hist['m_prthst']\n",
    "    assert m_prthst.shape == (n_snr, n_t, n_bins)\n",
    "    n_prthst = data_hist['n_prthst']\n",
    "    assert n_prthst.shape == (n_snr, n_t, n_bins)\n",
    "    rho_chm = data_hist['rho_chm']\n",
    "    assert rho_chm.shape == (n_chem,)\n",
    "    kappa_chm = data_hist['kappa_chm']\n",
    "    assert kappa_chm.shape == (n_chem,)\n",
    "    len_wvref = data_hist['len_wvref']\n",
    "    assert len_wvref.shape == (n_waveref,)\n",
    "    refr_wvchmref = data_hist['refr_wvchmref']\n",
    "    assert refr_wvchmref.shape == (n_waveref, n_chem)\n",
    "    chem_species = data_hist['chem_species']\n",
    "    assert len(chem_species) == n_chem\n",
    "    d_histbins = data_hist['d_histbins']\n",
    "    assert d_histbins.shape == (n_bins + 1,)\n",
    "\n",
    "    #######################################\n",
    "    #   Making the Data Variables Dict    #\n",
    "    #######################################\n",
    "    # `datanp_dict` is an input variable (str) to array (np.ndarray) mapping.\n",
    "    datanp_dict = dict(\n",
    "        n_prthst=n_prthst.reshape(n_snr * n_t, 1, n_bins),\n",
    "        m_prthst=m_prthst.reshape(n_snr * n_t, 1, n_bins),\n",
    "        m_chmprthst=m_chmprthst.reshape(n_snr * n_t, n_chem, n_bins))\n",
    "\n",
    "    # `aero_cstinfo` contains the aerosol constants used within ccn, inp, and optical evaluation metrics\n",
    "    aero_cstinfo = dict(chem_species=chem_species, n_chem=n_chem, n_bins=n_bins, n_waveref=n_waveref)\n",
    "    aero_cstinfo['rho_chm'] = torch.from_numpy(rho_chm).to(device=tch_device, dtype=tch_dtype)\n",
    "    aero_cstinfo['kappa_chm'] = torch.from_numpy(kappa_chm).to(device=tch_device, dtype=tch_dtype)\n",
    "    aero_cstinfo['len_wvref'] = torch.from_numpy(len_wvref).to(device=tch_device, dtype=tch_dtype)\n",
    "    aero_cstinfo['refr_wvchmref'] = torch.from_numpy(refr_wvchmref).to(device=tch_device, dtype=tch_cdtype)\n",
    "\n",
    "    # `data_dict` is an input variable (str) to tensor (torch.tensor) mapping.\n",
    "    data_dict = {key: torch.from_numpy(varnp).to(device=tch_device, dtype=tch_dtype)\n",
    "        for key, varnp in datanp_dict.items()}\n",
    "\n",
    "    # Preparing the data hash for later caching\n",
    "    hash_data = {key: data_cfg for key in data_dict}\n",
    "\n",
    "    # Example:\n",
    "    #   data_dims == {\n",
    "    #       'n_prthst': (1, n_bins),\n",
    "    #       'm_prthst': (1, n_bins),\n",
    "    #       'm_chmprthst': (n_chem, n_bins)\n",
    "    #   }\n",
    "    data_dims = {key: tuple(varnp.shape[1:]) for key, varnp in datanp_dict.items()}\n",
    "\n",
    "    # The shape variables\n",
    "    shapevars = dict(n_snr=n_snr, n_t=n_t, n_chem=n_chem, n_bins=n_bins,\n",
    "        chem_species=chem_species, device=device_name, dtype=dtype_name)\n",
    "\n",
    "    # Adding the aeromeasures pp config\n",
    "    pp_mdlscfg['aeromeasures'] = {'class': AeroMeasures, 'hparams/aerocst': aero_cstinfo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944dffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:47.666627Z",
     "iopub.status.busy": "2025-05-19T21:57:47.666472Z",
     "iopub.status.idle": "2025-05-19T21:57:47.739724Z",
     "shell.execute_reply": "2025-05-19T21:57:47.739241Z",
     "shell.execute_reply.started": "2025-05-19T21:57:47.666612Z"
    }
   },
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    ########### Constructing the Batch RNG Object ###########\n",
    "    #########################################################\n",
    "    n_seeds = len(rng_seed_list)\n",
    "    rng_seeds = np.array(rng_seed_list)\n",
    "    rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "        device=tch_device, dtype=tch_dtype,\n",
    "        unif_cache_cols=1_000_000,\n",
    "        norm_cache_cols=5_000_000)\n",
    "    rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "\n",
    "    erng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "        device=tch_device, dtype=tch_dtype,\n",
    "        unif_cache_cols=0,\n",
    "        norm_cache_cols=0)\n",
    "    erng.seed(np.broadcast_to(rng_seeds, erng.shape))\n",
    "\n",
    "    vrng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "        device=tch_device, dtype=tch_dtype,\n",
    "        unif_cache_cols=0,\n",
    "        norm_cache_cols=0)\n",
    "    vrng.seed(np.broadcast_to(rng_seeds, vrng.shape))\n",
    "\n",
    "    mrng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "        device=tch_device, dtype=tch_dtype,\n",
    "        unif_cache_cols=0,\n",
    "        norm_cache_cols=0)\n",
    "    mrng.seed(np.broadcast_to(rng_seeds, mrng.shape))\n",
    "\n",
    "    #########################################################\n",
    "    ######## Instantiating Train/Test Split Indecis #########\n",
    "    #########################################################\n",
    "    split_vars = split_cfg['vars']\n",
    "    split_idxdict = get_snrtspltidxs(split_cfg, n_snr, n_t, n_seeds,\n",
    "        rng, tch_device, pop_opts=True)\n",
    "    trn_spltidxs = split_idxdict['split_idxs']\n",
    "    n_trn = trn_spltidxs.shape[-1]\n",
    "    n_tst = n_snr * n_t - n_trn\n",
    "    assert trn_spltidxs.shape == (n_seeds, n_trn)\n",
    "    tst_spltidxs = split_idxdict['negsplit_idxs']\n",
    "    assert tst_spltidxs.shape == (n_seeds, n_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3999c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:47.740351Z",
     "iopub.status.busy": "2025-05-19T21:57:47.740223Z",
     "iopub.status.idle": "2025-05-19T21:57:49.225460Z",
     "shell.execute_reply": "2025-05-19T21:57:49.224960Z",
     "shell.execute_reply.started": "2025-05-19T21:57:47.740339Z"
    }
   },
   "outputs": [],
   "source": [
    "    #######################################\n",
    "    #  Constructing the NN pre-processor  #\n",
    "    #######################################\n",
    "    ppnn_hparams = {hparam: eval_formula(val, shapevars, catch=True) for hparam, val in ppnn_hparamsraw.items()}\n",
    "    nn_pp = make_pp(ppnn_type, ppnn_hparams, pp_mdlscfg, tch_device, tch_dtype, tch_cdtype)\n",
    "    nn_pp.infer(data_dict, trn_spltidxs, n_seeds, eval_bspp, hash_data=hash_data)\n",
    "    un_dims = nn_pp.get_dims(data_dims, types='output', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "    xn_dims = nn_pp.get_dims(data_dims, types='input', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "\n",
    "    datannpp_dims = {**data_dims, **un_dims}\n",
    "    ppcri_hparams = {hparam: eval_formula(val, shapevars, catch=True) for hparam, val in ppcri_hparamsraw.items()}\n",
    "    cri_pp = make_pp(ppcri_type, ppcri_hparams, pp_mdlscfg, tch_device, tch_dtype, tch_cdtype)\n",
    "    cri_pp.infer(data_dict, trn_spltidxs, n_seeds, eval_bspp, hash_data=hash_data)\n",
    "    uc_dims = cri_pp.get_dims(datannpp_dims, types='output', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "    xc_dims = cri_pp.get_dims(datannpp_dims, types='input', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "\n",
    "    lbl_pp, ul_dims, xl_dims = None, dict(), dict()\n",
    "    if is_condvae and (lblpp_type == 'pretrained'):\n",
    "        fpidx_lblpp = lblpp_hparamsraw.pop('fpidx')\n",
    "        resdir_lblpp = lblpp_hparamsraw.pop('resdir', None)\n",
    "        rio_lbl = resio(fpidx=fpidx_lblpp, resdir=resdir_lblpp)\n",
    "        lbl_ppinfo = load_riopp(rio_lbl, device_name, dtype_name, tch_device, tch_dtype, tch_cdtype)\n",
    "        rng_seed_list_lblpp, lbl_pp = lbl_ppinfo.pop('rng_seed_list'), lbl_ppinfo.pop('pp')\n",
    "        assert rng_seed_list == rng_seed_list_lblpp, f'loaded label pp rng seeds do not match'\n",
    "        xl_dims = lbl_pp.get_dims(data_dims, types='input', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "        ul_dims = lbl_pp.get_dims(data_dims, types='output', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "        assert len(lblpp_hparamsraw) == 0, f'unused options: {lblpp_hparamsraw}'\n",
    "    elif is_condvae:\n",
    "        lblpp_hparams = {hparam: eval_formula(val, shapevars, catch=True) for hparam, val in lblpp_hparamsraw.items()}\n",
    "        lbl_pp = make_pp(lblpp_type, lblpp_hparams, pp_mdlscfg, tch_device, tch_dtype, tch_cdtype)\n",
    "        lbl_pp.infer(data_dict, trn_spltidxs, n_seeds, eval_bspp, hash_data=hash_data)\n",
    "        xl_dims = lbl_pp.get_dims(data_dims, types='input', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "        ul_dims = lbl_pp.get_dims(data_dims, types='output', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "\n",
    "    for x_node, x_dtupl in xl_dims.items():\n",
    "        x_dtupn = xn_dims.get(x_node, x_dtupl)\n",
    "        assert x_dtupl == x_dtupn\n",
    "    xnl_dims = {**xn_dims, **xl_dims}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf95d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.226249Z",
     "iopub.status.busy": "2025-05-19T21:57:49.226000Z",
     "iopub.status.idle": "2025-05-19T21:57:49.277465Z",
     "shell.execute_reply": "2025-05-19T21:57:49.277063Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.226236Z"
    }
   },
   "outputs": [],
   "source": [
    "    #########################################################\n",
    "    ########### Evaluation Point Sampling Options ###########\n",
    "    #########################################################\n",
    "    eparams = dict()\n",
    "    for eid, eopts in evalcfgs.items():\n",
    "        epp_cfg = dict()\n",
    "\n",
    "        # TODO: The inferred parameters must be run on $f^-1(f(x))$ and\n",
    "        # not $x$ directly for the baseline shifts and scales to be most\n",
    "        # accurate in practice. That being said, this seems good enough.\n",
    "        epp_type = eopts.pop('pp/type')\n",
    "        epp_inpspc = eopts.pop('ppinp/space', 'x')\n",
    "        epp_rndtrp = eopts.pop('ppinp/rndtrp', True)\n",
    "        epp_hparamsraw = get_subdict(eopts, 'pp', pop=True)\n",
    "        \n",
    "        epp_hparams = {hparam: eval_formula(val, shapevars, catch=True) \n",
    "            for hparam, val in epp_hparamsraw.items()}\n",
    "        epp = make_pp(epp_type, epp_hparams, pp_mdlscfg, tch_device, tch_dtype, tch_cdtype)\n",
    "\n",
    "        epp.infer(data_dict, trn_spltidxs, n_seeds, eval_bspp, hash_data=hash_data)\n",
    "        ue_dims = epp.get_dims(datannpp_dims, types='output', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "        xe_dims = epp.get_dims(datannpp_dims, types='input', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "\n",
    "        # Getting the split configuration dictionary\n",
    "        n_evlpnts = eopts.pop('n', None)\n",
    "        e_sigscale = eopts.pop('sigscale', None)\n",
    "        e_spltcfgsraw = get_subdict(eopts, 'splits', pop=True)\n",
    "        e_spltcfgs = get_spltcfgs(e_spltcfgsraw, eid, n_evlpnts, e_sigscale, is_condvae, lblnn_enctype)\n",
    "\n",
    "        # Determining the split data indecis `e_spltidxs`\n",
    "        for e_split, e_spltcfg in e_spltcfgs.items():\n",
    "            if e_split == 'train':\n",
    "                e_spltidxs = trn_spltidxs\n",
    "            elif e_split == 'test':\n",
    "                e_spltidxs = tst_spltidxs\n",
    "            elif e_split == 'snr':\n",
    "                with torch.no_grad():\n",
    "                    e_snridxlst_ = eopts.pop('snr/idxs')\n",
    "                    n_esnridxlst = len(e_snridxlst_)\n",
    "\n",
    "                    assert all(ii_snr < n_snr for ii_snr in e_snridxlst_)\n",
    "                    assert all(ii_snr >= -n_snr for ii_snr in e_snridxlst_)\n",
    "\n",
    "                    # Translating negative scenario indices\n",
    "                    e_snridxlst = [ii_snr % n_snr for ii_snr in e_snridxlst_]\n",
    "                    n_esplit = n_esnridxlst * n_t\n",
    "\n",
    "                    e_splitidxs1 = torch.tensor(e_snridxlst).to(device=tch_device, dtype=torch.long)\n",
    "                    assert e_splitidxs1.shape == (n_esnridxlst,)\n",
    "\n",
    "                    ii_t = torch.arange(n_t, device=tch_device, dtype=torch.long)\n",
    "                    assert ii_t.shape == (n_t,)\n",
    "\n",
    "                    e_splitidxs2 = n_t * e_splitidxs1.reshape(n_esnridxlst, 1) + ii_t.reshape(1, n_t)\n",
    "                    assert e_splitidxs2.shape == (n_esnridxlst, n_t)\n",
    "\n",
    "                    e_splitidxs3 = e_splitidxs2.reshape(n_esplit)\n",
    "                    assert e_splitidxs3.shape == (n_esplit,)\n",
    "\n",
    "                    e_spltidxs = e_splitidxs3.reshape(1, n_esplit).expand(n_seeds, n_esplit)\n",
    "                    assert e_spltidxs.shape == (n_seeds, n_esplit)\n",
    "            elif e_split == 'normal':\n",
    "                e_spltidxs = None\n",
    "            else:\n",
    "                raise ValueError(f'undefined e_split={e_split}')\n",
    "            \n",
    "            assert e_spltcfg['idxs'] is None\n",
    "            e_spltcfg['idxs'] = e_spltidxs\n",
    "            assert e_spltcfg['iidxs'] is None\n",
    "            e_spltcfg['iidxs'] = None\n",
    "\n",
    "        eparams[eid] = dict()\n",
    "        eparams[eid]['pp'] = epp\n",
    "        eparams[eid]['inpspc'] = epp_inpspc\n",
    "        eparams[eid]['rndtrp'] = epp_rndtrp\n",
    "        eparams[eid]['udims'] = ue_dims\n",
    "        eparams[eid]['xdims'] = xe_dims\n",
    "        eparams[eid]['frq'] = eopts.pop('frq')\n",
    "        eparams[eid]['rndmztn'] = eopts.pop('rndmztn')\n",
    "        eparams[eid]['store'] = get_subdict(eopts, 'store', pop=True)\n",
    "        eparams[eid]['splits'] = e_spltcfgs\n",
    "\n",
    "        assert len(eopts) == 0, f'unused eval options for {eid}: {eopts}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8f135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.278015Z",
     "iopub.status.busy": "2025-05-19T21:57:49.277893Z",
     "iopub.status.idle": "2025-05-19T21:57:49.281813Z",
     "shell.execute_reply": "2025-05-19T21:57:49.281434Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.278005Z"
    }
   },
   "outputs": [],
   "source": [
    "    # Piping the matplotlib-related options dictionary\n",
    "    for vid, vcfg in vizcfgs.items():\n",
    "        mplopt_tmplts = vcfg.pop('mplopts/plan', [])\n",
    "        if isinstance(mplopt_tmplts, str):\n",
    "            mplopt_tmplts = [mplopt_tmplts]\n",
    "        v_mplopts = dict()\n",
    "        for mpo_tmplt in mplopt_tmplts:\n",
    "            assert mpo_tmplt in mplopts_cfg, dedent(f'''\n",
    "                The Matplotlib option template \"{mpo_tmplt}\" \n",
    "                visualization \"{vid}\". However, this template\n",
    "                was never defined:\n",
    "                    available templates: {mplopts_cfg.keys()}\n",
    "            ''')\n",
    "            v_mplopts.update(mplopts_cfg[mpo_tmplt])\n",
    "        mpo_ovrrd = get_subdict(vcfg, 'mplopts', pop=True)\n",
    "        mpo_ovrrd = dict() if mpo_ovrrd is None else mpo_ovrrd.copy()\n",
    "        v_mplopts.update(mpo_ovrrd)\n",
    "        v_mplopts['type'] = vcfg['type']\n",
    "        vcfg['mplopts'] = v_mplopts\n",
    "        vcfg.setdefault('splits', None)\n",
    "\n",
    "    # Converting str values to tuples\n",
    "    for prf_id in perfcfgs:\n",
    "        prf_cartcfg = perfcfgs[prf_id]\n",
    "        assert set(prf_cartcfg) == {'srctrg', 'asgn', 'node', 'metric', 'stat'}\n",
    "        prf_cartcfg = {optn: (val,) if isinstance(val, str) else val \n",
    "            for optn, val in prf_cartcfg.items()}\n",
    "        perfcfgs[prf_id] = prf_cartcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebd9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.282334Z",
     "iopub.status.busy": "2025-05-19T21:57:49.282214Z",
     "iopub.status.idle": "2025-05-19T21:57:49.331384Z",
     "shell.execute_reply": "2025-05-19T21:57:49.331011Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.282322Z"
    }
   },
   "outputs": [],
   "source": [
    "    # The latent dimension\n",
    "    ltnt_dim = eval_formula(ltnt_dimraw, shapevars, catch=True)\n",
    "\n",
    "    # Initializing the encoder\n",
    "    enc_hparams = dict()\n",
    "    for hparam, val in enc_hparamsraw.items():\n",
    "        enc_hparams[hparam] = eval_formula(val, shapevars, catch=True)\n",
    "\n",
    "    encoder = make_nn(name=enc_type, hparams=enc_hparams, config=nn_mdlscfg, \n",
    "        n_seeds=n_seeds, rng=rng, device=tch_device, dtype=tch_dtype)\n",
    "\n",
    "    # Initializing the decoder\n",
    "    dec_hparams = dict()\n",
    "    for hparam, val in dec_hparamsraw.items():\n",
    "        dec_hparams[hparam] = eval_formula(val, shapevars, catch=True)\n",
    "\n",
    "    decoder = make_nn(name=dec_type, hparams=dec_hparams, config=nn_mdlscfg, \n",
    "        n_seeds=n_seeds, rng=rng, device=tch_device, dtype=tch_dtype)\n",
    "\n",
    "    # Setting the optimizer\n",
    "    model_parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "\n",
    "    # Getting the seed-splitted parameters\n",
    "    if schdlr_type is not None:\n",
    "        pgrouper = BatchParamGrouper(params=model_parameters, shape=(n_seeds,))\n",
    "        optim_params = pgrouper.param_groups\n",
    "    else:\n",
    "        pgrouper = None\n",
    "        optim_params = model_parameters\n",
    "\n",
    "    # Instantiating the optimizer\n",
    "    if opt_type == 'adam':\n",
    "        opt = torch.optim.Adam(optim_params, **opt_hparams)\n",
    "    elif opt_type == 'sgd':\n",
    "        opt = torch.optim.SGD(optim_params, **opt_hparams)\n",
    "    else:\n",
    "        raise NotImplementedError(f'opt/dstr=\"{opt_type}\" not implmntd')\n",
    "\n",
    "    # Instantiating the learning rate scheduler\n",
    "    if schdlr_type is not None:\n",
    "        scheduler = BatchLRScheduler(kind=schdlr_type, hparams=schdlr_hparams, \n",
    "            optimizer=opt, n_seeds=n_seeds, n_grps=pgrouper.n_grps)\n",
    "    else:\n",
    "        scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d7a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.331903Z",
     "iopub.status.busy": "2025-05-19T21:57:49.331789Z",
     "iopub.status.idle": "2025-05-19T21:57:49.359756Z",
     "shell.execute_reply": "2025-05-19T21:57:49.359392Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.331892Z"
    }
   },
   "outputs": [],
   "source": [
    "    # Initializing the label network\n",
    "    lblnn, y_dims = None, dict()\n",
    "    if is_condvae and (lblnn_type == 'pretrained'):\n",
    "        fpidx_lblnn = lblnn_hparams.pop('fpidx')\n",
    "        resdir_lblnn = lblnn_hparams.pop('resdir', None)\n",
    "\n",
    "        rio_lblnn = resio(fpidx=fpidx_lblnn, resdir=resdir_lblnn)\n",
    "        lblnn_info = load_rioenc(rio_lblnn, device_name, dtype_name, tch_device, tch_dtype)\n",
    "\n",
    "        rng_seed_list_lblnn = lblnn_info.pop('rng_seed_list')\n",
    "        assert rng_seed_list == rng_seed_list_lblnn, f'loaded label nn rng seeds do not match'\n",
    "\n",
    "        assert lblnn_enctype == 'vae'\n",
    "        lblnn = VAELabelNN(**lblnn_info, y_node=y_nodename)\n",
    "\n",
    "        tmprng = BatchRNG(shape=(n_seeds,), lib='torch', device=tch_device, dtype=tch_dtype,\n",
    "            unif_cache_cols=0, norm_cache_cols=0)\n",
    "        tmprng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "        lblnnkwsfk = dict(n_seeds=n_seeds, n_mb=n_mb, rng=tmprng, sigscale=y_sigscale)  \n",
    "        y_dims = get_lbldims(lblnn, ul_dims, n_seeds, n_mb, tch_device, tch_dtype, lblnnkws=lblnnkwsfk)\n",
    "        \n",
    "        assert len(lblnn_hparams) == 0, f'unused options: {lblnn_hparams}'\n",
    "    elif is_condvae:\n",
    "        assert (lblnn_enctype, lblnn_hparams) == ('plain', None)\n",
    "        lblnn_hparams = dict()\n",
    "        for hparam, val in lblnn_hparamsraw.items():\n",
    "            lblnn_hparams[hparam] = eval_formula(val, shapevars, catch=True)\n",
    "            \n",
    "        lblnn = make_nn(name=lblnn_type, hparams=lblnn_hparams, config=nn_mdlscfg,\n",
    "            n_seeds=n_seeds, rng=rng, device=tch_device, dtype=tch_dtype)\n",
    "        y_dims = get_lbldims(lblnn, ul_dims, n_seeds, n_mb, tch_device, tch_dtype)\n",
    "        assert len(y_dims) > 0\n",
    "\n",
    "    if w_indzy is not None:\n",
    "        assert is_condvae, dedent(f'''\n",
    "            An iid z and y loss weight was provided, but the \n",
    "            model is non-conditional. The iid z and y loss can \n",
    "            only make sense for conditional vae models:\n",
    "                cri/indzy/w: {w_indzy}''')\n",
    "        iid_lossfinder = IIDLossMaker(n_histindzy, cri_indzymtrccfg, rng, tch_device, tch_dtype)\n",
    "        # If you're in absolute need of efficiency, you can set `has_indzyloss = (w_indzy != 0)`\n",
    "        has_indzyloss = True\n",
    "    else:\n",
    "        assert n_histindzy is None, f'undefined cri/indzy/hist/n={n_histindzy}'\n",
    "        assert len(cri_indzymtrccfg) == 0, dedent(f'''\n",
    "            unused cri/indzy/mtrc options: {cri_indzymtrccfg}''')\n",
    "        iid_lossfinder, has_indzyloss = None, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495caf49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.360299Z",
     "iopub.status.busy": "2025-05-19T21:57:49.360180Z",
     "iopub.status.idle": "2025-05-19T21:57:49.471950Z",
     "shell.execute_reply": "2025-05-19T21:57:49.471585Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.360289Z"
    }
   },
   "outputs": [],
   "source": [
    "    if results_dir is not None:\n",
    "        pathlib.Path(os.sep.join([results_dir, cfg_tree])\n",
    "                    ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if storage_dir is not None:\n",
    "        cfgstrgpnt_dir = os.sep.join([storage_dir, cfg_tree, cfg_name])\n",
    "        pathlib.Path(cfgstrgpnt_dir).mkdir(parents=True, exist_ok=True)\n",
    "        strgidx = sum(isdir(f'{cfgstrgpnt_dir}/{x}') for x in os.listdir(cfgstrgpnt_dir))\n",
    "        dtnow_ = dtnow[2:].replace('-', '').replace(':', '').replace('.', '')\n",
    "        cfgstrg_dir = f'{cfgstrgpnt_dir}/{strgidx:02d}_{dtnow_}'\n",
    "        pathlib.Path(cfgstrg_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        yaml.Dumper.ignore_aliases = lambda *args: True\n",
    "        with open(f'{cfgstrg_dir}/config.yml', 'w') as fp:\n",
    "            yaml.dump(cfg_dict_input, fp, sort_keys=False, default_flow_style=None)\n",
    "    else:\n",
    "        cfgstrg_dir = None\n",
    "\n",
    "    if do_savefigs:\n",
    "        assert cfgstrg_dir is not None\n",
    "        fig_dir = f'{cfgstrg_dir}/figures'\n",
    "        pathlib.Path(fig_dir).mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        fig_dir = None\n",
    "\n",
    "    if do_logtb:\n",
    "        import logging\n",
    "        if 'tbwriter' in locals():\n",
    "            locals()['tbwriter'].close()\n",
    "        tbwriter = tensorboardX.SummaryWriter(cfgstrg_dir)\n",
    "        logging.getLogger(\"tensorboardX.x2num\").setLevel(logging.CRITICAL)\n",
    "    else:\n",
    "        tbwriter = None\n",
    "        \n",
    "    if do_profile:\n",
    "        profiler = Profiler()\n",
    "        profiler.start()\n",
    "    else:\n",
    "        profiler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d479a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.472454Z",
     "iopub.status.busy": "2025-05-19T21:57:49.472337Z",
     "iopub.status.idle": "2025-05-19T21:57:49.516179Z",
     "shell.execute_reply": "2025-05-19T21:57:49.515787Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.472443Z"
    }
   },
   "outputs": [],
   "source": [
    "    # Data writer construction\n",
    "    hdfpth = None\n",
    "    if results_dir is not None:\n",
    "        hdfpth = f'{results_dir}/{cfg_tree}/{cfg_name}.h5'\n",
    "    dwriter = DataWriter(flush_period=ioflsh_period*n_seeds, \n",
    "        compression_level=io_cmprssnlvl)\n",
    "\n",
    "    # Data Prepper Construction\n",
    "    dprepper = DataPrepper(n_seeds, rng_seed_list, io_avgfrq, chkpnt_period,\n",
    "        iomon_period, device_name, dtype_name, dtnow, cfg_tree, cfg_name)\n",
    "\n",
    "    # Identifying the hyper-parameter from etc config columns\n",
    "    hppats = ['problem', 'opt/*', 'data/tree', 'split/*', 'nn/*', 'cri/*', 'eval/*']\n",
    "\n",
    "    etcpats = ['desc', 'date', 'rng_seed/list', 'data/*', 'io/*', 'eval/*/viz/*/mplopts/*', \n",
    "        'nn/modules/*', 'pp/modules/*', 'mplopts/*', 'perf/*', 'metric/modules/*', \n",
    "        'asgn/modules/*', 'viz/*']\n",
    "\n",
    "    # Making the hyper-param and etc dictionaries\n",
    "    hp_dict, etc_dict = DataPrepper.make_hpetcopts(cfg_dict_input, \n",
    "        hppats, etcpats, n_seeds, device_name, dtnow)\n",
    "\n",
    "    # Pushing the static output data once\n",
    "    # TODO: store the label static nn parameters\n",
    "    pp_statics = {'mdl/cri': cri_pp, 'mdl/nn': nn_pp}\n",
    "    pp_statics.update({f'eval/{eid}': eopts['pp'] for eid, eopts in eparams.items()})\n",
    "    if is_condvae:\n",
    "        pp_statics.update({'mdl/lbl': lbl_pp})\n",
    "    dprepper.push_static(hp_dict, etc_dict, cfg_dict_input, shapevars, pp_statics)\n",
    "\n",
    "    # Evaluation tools\n",
    "    trn_sttime = time.time()\n",
    "    dprepper.trn_sttime = trn_sttime\n",
    "    last_perfdict = dict()\n",
    "    ema = EMA(gamma=0.999, gamma_sq=0.998)\n",
    "    timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03beda3",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a003cc",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-19T21:57:49.516866Z",
     "iopub.status.busy": "2025-05-19T21:57:49.516743Z",
     "iopub.status.idle": "2025-05-19T22:18:52.218737Z",
     "shell.execute_reply": "2025-05-19T22:18:52.218255Z",
     "shell.execute_reply.started": "2025-05-19T21:57:49.516855Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    ###############################################################################\n",
    "    ######################## Training and Evaluation Loop #########################\n",
    "    ###############################################################################\n",
    "    ii_trn = None\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        timer.tic('train')\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        #######################################\n",
    "        #  Sampling training data and indices #\n",
    "        #######################################\n",
    "\n",
    "        # First, we sample random indices to select from `trn_spltidxs`.\n",
    "        iistrt, iiend = epoch * n_mb, (epoch + 1) * n_mb\n",
    "        ii_mbidxs_ = torch.arange(iistrt, iiend, device=tch_device).remainder(n_trn).to(dtype=torch.long)\n",
    "        assert ii_mbidxs_.shape == (n_mb,)\n",
    "\n",
    "        renew_ii = (ii_mbidxs_ == 0).any().item()\n",
    "        if renew_ii and (mb_rndmztn == 'none'):\n",
    "            ii_trn_ = torch.arange(n_trn, device=tch_device, dtype=torch.long)\n",
    "            assert ii_trn_.shape == (n_trn,)\n",
    "\n",
    "            ii_trn = ii_trn_.reshape(1, n_trn).expand(n_seeds, n_trn)\n",
    "            assert ii_trn.shape == (n_seeds, n_trn)\n",
    "        if renew_ii and (mb_rndmztn == 'shuffle'):\n",
    "            ii_trn_ = rng.rand(n_seeds, n_trn)\n",
    "            assert ii_trn_.shape == (n_seeds, n_trn)\n",
    "\n",
    "            ii_trn = ii_trn_.argsort(dim=-1).detach()\n",
    "            assert ii_trn.shape == (n_seeds, n_trn)\n",
    "        elif renew_ii and (mb_rndmztn == 'iid'):\n",
    "            ii_trn = (rng.rand(n_seeds, n_trn) * n_trn).to(torch.long)\n",
    "            assert ii_trn.shape == (n_seeds, n_trn)\n",
    "        elif not renew_ii:\n",
    "            assert ii_trn.shape == (n_seeds, n_trn)\n",
    "        else:\n",
    "            raise ValueError(f'undefined mini-batch randomization = {mb_rndmztn}')\n",
    "\n",
    "        ii_mbidxs = ii_mbidxs_.reshape(1, n_mb).expand(n_seeds, n_mb)\n",
    "        assert ii_mbidxs.shape == (n_seeds, n_mb)\n",
    "\n",
    "        ii_mb = torch.take_along_dim(ii_trn, ii_mbidxs, dim=-1)\n",
    "        assert ii_mb.shape == (n_seeds, n_mb)\n",
    "\n",
    "        # First, we sample random indices to select from `trn_spltidxs`.\n",
    "        ii_mb = (rng.rand(n_seeds, n_mb) * n_trn).to(torch.long)\n",
    "        assert ii_mb.shape == (n_seeds, n_mb)\n",
    "\n",
    "        # Next, we select those from the training split indices.\n",
    "        i_mb = torch.take_along_dim(trn_spltidxs, ii_mb, dim=1)\n",
    "        assert i_mb.shape == (n_seeds, n_mb)\n",
    "\n",
    "        # Now, we collect these indices from the full data.\n",
    "        xnl_mbs = dict()\n",
    "        for x_node, (n_xchnls, n_xlen) in xnl_dims.items():\n",
    "            x_mb_ = data_dict[x_node][i_mb.ravel()]\n",
    "            assert x_mb_.shape == (n_seeds * n_mb, n_xchnls, n_xlen)\n",
    "            x_mb = x_mb_.reshape(n_seeds, n_mb, n_xchnls, n_xlen)\n",
    "            assert x_mb.shape == (n_seeds, n_mb, n_xchnls, n_xlen)\n",
    "            xnl_mbs[x_node] = x_mb\n",
    "\n",
    "        # The encoder's pp input mini-batches\n",
    "        xn_mbs = {x_node: xnl_mbs[x_node] for x_node in xn_dims}\n",
    "        # The labeler's pp input mini-batches\n",
    "        xl_mbs = {x_node: xnl_mbs[x_node] for x_node in xl_dims}\n",
    "\n",
    "        # Next, we apply the input pre-processing transformations\n",
    "        # before feeding it to the encoder.\n",
    "        un_mbs, u_shaper = nn_pp.forward(xn_mbs, full=False)\n",
    "        for u_node, (n_uchnls, n_ulen) in un_dims.items():\n",
    "            u_mb = un_mbs[u_node]\n",
    "            assert u_mb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "        #######################################\n",
    "        #         Applying the Labeler        #\n",
    "        #######################################\n",
    "        if is_condvae:\n",
    "            with torch.no_grad():\n",
    "                ul_mbs, u_shaper = lbl_pp.forward(xl_mbs, full=False)\n",
    "                for u_node, (n_uchnls, n_ulen) in ul_dims.items():\n",
    "                    u_mb = ul_mbs[u_node]\n",
    "                    assert u_mb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "                lblnnkws = dict()\n",
    "                if lblnn_enctype == 'vae':\n",
    "                    lblnnkws.update(n_seeds=n_seeds, n_mb=n_mb, rng=rng, sigscale=y_sigscale)\n",
    "                assert lblnn_enctype in ('plain', 'vae')\n",
    "            \n",
    "                y_mbs = lblnn(**ul_mbs, **lblnnkws)\n",
    "                assert isinstance(y_mbs, dict)\n",
    "        else:\n",
    "            y_mbs = dict()\n",
    "\n",
    "        for y_node, (n_ychnls, n_ylen) in y_dims.items():\n",
    "            y_mb = y_mbs[y_node]\n",
    "            assert y_mb.shape == (n_seeds, n_mb, n_ychnls, n_ylen)\n",
    "\n",
    "        #######################################\n",
    "        #         Applying the Encoder        #\n",
    "        #######################################\n",
    "        # Encoding the pre-processed input\n",
    "        u_enc_ = encoder(**un_mbs, **y_mbs)\n",
    "        assert u_enc_.shape == (n_seeds, n_mb, ltnt_dim * 2)\n",
    "\n",
    "        u_enc = u_enc_.reshape(n_seeds, n_mb, 2, ltnt_dim)\n",
    "        assert u_enc.shape == (n_seeds, n_mb, 2, ltnt_dim)\n",
    "\n",
    "        # Decoupling the mu and logsigma values out of the encoder\n",
    "        mu_mb = u_enc[:, :, 0, :]\n",
    "        assert mu_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "        if ltnt_sigtnsfm == 'exp':\n",
    "            logsigma_mb = u_enc[:, :, 1, :]\n",
    "            assert logsigma_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "        elif ltnt_sigtnsfm == 'explin':\n",
    "            logsigtlde_mb = u_enc[:, :, 1, :]\n",
    "            assert logsigtlde_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "            logsigma_mb = torch.where(logsigtlde_mb < 0, logsigtlde_mb, \n",
    "                (logsigtlde_mb + 1).log())\n",
    "            assert logsigma_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "        else:\n",
    "            raise ValueError(f'undefined ltnt_sigtnsfm={ltnt_sigtnsfm}')\n",
    "\n",
    "        #######################################\n",
    "        ##### Computing the IID Z/Y Loss ######\n",
    "        #######################################\n",
    "        loss_indzy, loss_indzyunit = 0, 0\n",
    "        if has_indzyloss:\n",
    "            loss_indzyunit = iid_lossfinder(z_mbs={'mu': mu_mb}, \n",
    "                y_mbs=y_mbs, n_seeds=n_seeds, n_mb=n_mb)\n",
    "            assert loss_indzyunit.shape == (n_seeds,)\n",
    "            loss_indzy = w_indzy * loss_indzyunit\n",
    "            assert loss_indzy.shape == (n_seeds,)\n",
    "\n",
    "        #######################################\n",
    "        #     Computing the KL-Divergences    #\n",
    "        #######################################\n",
    "        # Computing the KL-Divergences to the Normal prior in the latent space\n",
    "        logvar_mb = (2 * logsigma_mb)\n",
    "        assert logvar_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "        sigmasq_mb = logvar_mb.exp()\n",
    "        assert sigmasq_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "        musq_mb = mu_mb.square()\n",
    "        assert musq_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "        kl_mumb = musq_mb.sum(dim=-1) / 2.0\n",
    "        assert kl_mumb.shape == (n_seeds, n_mb)\n",
    "\n",
    "        kl_sigmb = (sigmasq_mb - 1.0 - logvar_mb).sum(dim=-1) / 2.0\n",
    "        assert kl_sigmb.shape == (n_seeds, n_mb)\n",
    "\n",
    "        loss_klmu = kl_mumb.mean(dim=-1)\n",
    "        assert loss_klmu.shape == (n_seeds,)\n",
    "\n",
    "        loss_klsig = kl_sigmb.mean(dim=-1)\n",
    "        assert loss_klsig.shape == (n_seeds,)\n",
    "\n",
    "        loss_kl = loss_klmu * w_klmu + loss_klsig * w_klsig\n",
    "        assert loss_kl.shape == (n_seeds,)\n",
    "\n",
    "        #######################################\n",
    "        #         Applying the Decoder        #\n",
    "        #######################################\n",
    "        eps_mb = rng.randn(n_seeds, n_mb, ltnt_dim)\n",
    "        assert eps_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "        z_mb = mu_mb + eps_mb * logsigma_mb.exp()\n",
    "        assert z_mb.shape == (n_seeds, n_mb, ltnt_dim)\n",
    "\n",
    "        unhat_mbs = decoder(z=z_mb, **y_mbs)\n",
    "        for u_node, (n_uchnls, n_ulen) in un_dims.items():\n",
    "            uhat_mb = unhat_mbs[u_node]\n",
    "            assert uhat_mb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "        xnhat_mbs = None\n",
    "        if ppcri_inpspc == 'x':\n",
    "            xnhat_mbs = nn_pp.inverse(unhat_mbs, u_shaper, strict=True, full=False)\n",
    "            for x_node, (n_xchnls, n_xlen) in xn_dims.items():\n",
    "                x_mb = xnhat_mbs[x_node]\n",
    "                assert x_mb.shape == (n_seeds, n_mb, n_xchnls, n_xlen)\n",
    "\n",
    "        #######################################\n",
    "        #   Finding the Reconstruction Loss   #\n",
    "        #######################################\n",
    "        # Finding the right input to the criterion pre-processor\n",
    "        if (ppcri_inpspc == 'x') and (ppcri_rndtrp == True):\n",
    "            # In principle, `x_tildembs` should be identical to `xn_mbs`.\n",
    "            # However, since some I/O transformations may not be truly\n",
    "            # one-to-one, or due to numerical errors, they can be different.\n",
    "            # Since this is not the VAE's fault, this should not exhibit\n",
    "            # itself in the reconstruction loss, which is why we use the\n",
    "            # `x_tildembs` tensor instead of `x_pln` when computing the\n",
    "            # reconstruction loss.\n",
    "            x_tildembs = nn_pp.inverse(un_mbs, u_shaper, strict=True, full=False)\n",
    "            for x_node, (n_xchnls, n_xlen) in xn_dims.items():\n",
    "                x_mb = x_tildembs[x_node]\n",
    "                assert x_mb.shape == (n_seeds, n_mb, n_xchnls, n_xlen)\n",
    "            cri_inpmbs, cri_inphatmbs = x_tildembs, xnhat_mbs\n",
    "        elif (ppcri_inpspc == 'x') and (ppcri_rndtrp == False):\n",
    "            cri_inpmbs, cri_inphatmbs, x_tildembs = xn_mbs, xnhat_mbs, xn_mbs\n",
    "        elif (ppcri_inpspc == 'u'):\n",
    "            cri_inpmbs, cri_inphatmbs, x_tildembs = un_mbs, unhat_mbs, None\n",
    "        else:\n",
    "            raise ValueError(f'undefined ppcri_inpspc={ppcri_inpspc}')\n",
    "\n",
    "        # Applying the criterion pre-processor to the original\n",
    "        u_cmbs, u_cshaper = cri_pp.forward(cri_inpmbs, full=False)\n",
    "        for u_node, (n_uchnls, n_ulen) in uc_dims.items():\n",
    "            u_mb = u_cmbs[u_node]\n",
    "            assert u_mb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "        # Applying the criterion pre-processor to the reconstruction\n",
    "        uhat_cmbs, uhat_cshaper = cri_pp.forward(cri_inphatmbs, full=False)\n",
    "        for u_node, (n_uchnls, n_ulen) in uc_dims.items():\n",
    "            uhat_cmb = uhat_cmbs[u_node]\n",
    "            assert uhat_cmb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "        # Finding the reconstruction loss\n",
    "        u_lossrcnsts = dict()\n",
    "        for u_node, (n_uchnls, n_ulen) in uc_dims.items():\n",
    "            u_cmb = u_cmbs[u_node]\n",
    "            assert u_cmb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "            uhat_cmb = uhat_cmbs[u_node]\n",
    "            assert uhat_cmb.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "            uhat_err = u_cmb - uhat_cmb\n",
    "            assert uhat_err.shape == (n_seeds, n_mb, n_uchnls, n_ulen)\n",
    "\n",
    "            if rcnst_cri == 'mse':\n",
    "                u_lossrcnst = 0.5 * uhat_err.square().mean(dim=[-1, -2, -3])\n",
    "                assert u_lossrcnst.shape == (n_seeds,)\n",
    "            elif rcnst_cri == 'mae':\n",
    "                u_lossrcnst = uhat_err.abs().mean(dim=[-1, -2, -3])\n",
    "                assert u_lossrcnst.shape == (n_seeds,)\n",
    "            else:\n",
    "                raise ValueError(f'undefined rcnst_cri={rcnst_cri}')\n",
    "\n",
    "            u_lossrcnsts[u_node] = u_lossrcnst\n",
    "\n",
    "        loss_rcnst = torch.stack(list(u_lossrcnsts.values()), dim=1).sum(dim=1)\n",
    "        assert loss_rcnst.shape == (n_seeds,)\n",
    "\n",
    "        loss = loss_kl + loss_rcnst + loss_indzy\n",
    "        assert loss.shape == (n_seeds,)\n",
    "\n",
    "        loss_sum = loss.sum()\n",
    "        assert loss_sum.shape == tuple()\n",
    "        assert not loss_sum.isnan()\n",
    "\n",
    "        loss_sum.backward()\n",
    "\n",
    "        # We will not update in the first epoch so that we will \n",
    "        # record the initialization statistics as well. Instead, \n",
    "        # we will update an extra epoch at the end.\n",
    "        if epoch > 0:\n",
    "            if pgrouper is not None:\n",
    "                pgrouper.attach_grads()\n",
    "            opt.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(metrics=loss, epoch=epoch)\n",
    "        \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        timer.toc()\n",
    "\n",
    "        # Logging the loss statistics into a dictionary\n",
    "        loss_dict = dict()\n",
    "        with torch.no_grad():\n",
    "            loss_dict['loss/net'] = loss\n",
    "            loss_dict['loss/rcnst/net'] = loss_rcnst\n",
    "            for u_node, u_lossrcnst in u_lossrcnsts.items():\n",
    "                loss_dict[f'loss/rcnst/{u_node}'] = u_lossrcnst\n",
    "            loss_dict['loss/kl/net'] = loss_kl\n",
    "            loss_dict['loss/kl/mu'] = loss_klmu\n",
    "            loss_dict['loss/kl/sig'] = loss_klsig\n",
    "            loss_dict['ltnt/sig/l2sq'] = sigmasq_mb.mean(dim=[-1, -2])\n",
    "            musq_mean = musq_mb.mean(dim=[-1, -2])\n",
    "            loss_dict['ltnt/mu/l2sq'] = musq_mean\n",
    "            loss_dict['ltnt/sig/l2'] = sigmasq_mb.mean(dim=-1).sqrt().mean(dim=-1)\n",
    "            loss_dict['ltnt/mu/l2'] = musq_mb.mean(dim=-1).sqrt().mean(dim=-1)\n",
    "            if has_indzyloss:\n",
    "                loss_dict['loss/indzy/net'] = loss_indzy\n",
    "\n",
    "            loss_dict['ulss/rcnst/net'] = loss_rcnst\n",
    "            for u_node, u_lossrcnst in u_lossrcnsts.items():\n",
    "                loss_dict[f'ulss/rcnst/{u_node}'] = u_lossrcnst\n",
    "            loss_dict['ulss/kl/mu'] = loss_klmu\n",
    "            loss_dict['ulss/kl/sig'] = loss_klsig\n",
    "            if has_indzyloss:\n",
    "                loss_dict['ulss/indzy/net'] = loss_indzyunit\n",
    "                \n",
    "        for key, tnsr in loss_dict.items():\n",
    "            assert tnsr.shape == (n_seeds,)\n",
    "\n",
    "        #######################################\n",
    "        #   Logging the training statistics   #\n",
    "        #######################################\n",
    "        lnet_ema, _ = ema('loss/net', loss)\n",
    "        lkl_ema, _ = ema('loss/kl/net', loss_kl)\n",
    "        lmusq_ema, _ = ema('ltnt/mu/l2sq', musq_mean)\n",
    "        lrcnst_ema, _ = ema('loss/rcnst/net', loss_rcnst)\n",
    "        lrcnsts_ema = {u_node: ema(f'loss/rcnst/{u_node}', u_lossrcnst)[0]\n",
    "            for u_node, u_lossrcnst in u_lossrcnsts.items()}\n",
    "        if has_indzyloss:\n",
    "            lindzy_ema, _ = ema('loss/indzy/net', loss_indzy)\n",
    "\n",
    "        if (epoch % 100 == 0) and (results_dir is not None):\n",
    "            print_str = f'Epoch {epoch:06d}: EMA loss={lnet_ema:.4f}, kl={lkl_ema:.4f}, musq={lmusq_ema:.4f}'\n",
    "            if has_indzyloss:\n",
    "                print_str += f', indzy: {lindzy_ema:.4f}'\n",
    "            for u_node, u_lrcnstema in lrcnsts_ema.items():\n",
    "                print_str += f', rcnst/{u_node}: {u_lrcnstema:.4f}'\n",
    "            if scheduler is not None:\n",
    "                lr_median = np.median(scheduler.get_last_lr())\n",
    "                print_str += f', median(lr)={lr_median:.4f}'\n",
    "            print(print_str, flush=True)\n",
    "\n",
    "        if do_logtb:\n",
    "            with torch.no_grad():\n",
    "                for loss_name, loss_tnsr in loss_dict.items():\n",
    "                    tbwriter.add_scalar(loss_name, loss_tnsr.mean(), epoch)\n",
    "\n",
    "        #######################################\n",
    "        #      Performing the Evaluations     #\n",
    "        #######################################\n",
    "        e_vizinputs, e_mtrcinputs = dict(), dict()\n",
    "        for eid, eopts in eparams.items():\n",
    "            # The evaluation frequency\n",
    "            e_frq = eopts['frq']\n",
    "            if (epoch % e_frq) > 0:\n",
    "                continue\n",
    "            # The evaluation pre-processing\n",
    "            e_pp = eopts['pp']\n",
    "            # The evaluation pre-processor's input type (either 'x' or 'u')\n",
    "            e_ppinpspc = eopts['inpspc']\n",
    "            # The nodes and their dimensions after applying the `e_pp` pre-processor\n",
    "            ue_dims = eopts['udims']\n",
    "            # The evaluation split configs\n",
    "            e_spltcfgs = eopts['splits']\n",
    "\n",
    "            for vid, vcfg in vizcfgs.items():\n",
    "                if vcfg['eval'] == eid:\n",
    "                    assert epoch % io_avgfrq == 0, dedent(f'''\n",
    "                        eval/{eid} requires storage, thus \n",
    "                        \"eval/{eid}/frq\" % \"io/avg/frq\" == 0 \n",
    "                        should hold, but {e_frq} % {io_avgfrq} != 0.''')\n",
    "\n",
    "            ######### Phase I: Applying the encoder to all splits ##########\n",
    "            # Whether the original x inputs should go through the nn pre-prcessor's forward/inverse round trip.\n",
    "            e_pprndtrp = eopts['rndtrp']\n",
    "            # The randomization of evaluation indices ('none', 'shuffle', or 'iid')\n",
    "            e_rndmztn = eopts['rndmztn']\n",
    "            # The nodes and their dimensions before applying the `e_pp` pre-processor\n",
    "            # This should practically be the same as `x_dimes` for VAEs.\n",
    "            xe_dims = eopts['xdims']\n",
    "\n",
    "            e_encpkgs = dict()\n",
    "            for e_split, e_spltcfg in e_spltcfgs.items():\n",
    "                # The split data indecis across all the data\n",
    "                e_splitidxs = e_spltcfg['idxs']\n",
    "                # The hyper-indices over `e_splitidxs` to keep track\n",
    "                # of how much we made progress through them.\n",
    "                eii = e_spltcfg['iidxs']\n",
    "                # The number of samples in the split for evaluation\n",
    "                n_evlpnts = e_spltcfg['n']\n",
    "                # The vae label networks's noise injection sigma scale\n",
    "                ey_sigscale = e_spltcfg['lbl']['sigscale']\n",
    "\n",
    "                # Applying the Encoder\n",
    "                timer.tic(f'sample/{eid}/{e_split}/encode')\n",
    "                *pkg_encode, renew_eii, eii = Evaluation.encode(\n",
    "                    eii, e_split, e_splitidxs, e_pp, e_frq, e_rndmztn,\n",
    "                    ltnt_sigtnsfm, data_dict, nn_pp, encoder,\n",
    "                    e_ppinpspc, e_pprndtrp, xe_dims, ue_dims,\n",
    "                    is_condvae, lblnn_enctype, ey_sigscale, \n",
    "                    lbl_pp, lblnn, xl_dims, ul_dims, xnl_dims, y_dims,\n",
    "                    n_seeds, n_evlpnts, ltnt_dim, eval_bsnn, epoch,\n",
    "                    xn_dims, un_dims, erng, tch_device, tch_dtype)\n",
    "                timer.toc()\n",
    "\n",
    "                # Saving the same ordering for continuing in the next epochs\n",
    "                if renew_eii:\n",
    "                    e_spltcfg['iidxs'] = eii\n",
    "\n",
    "                e_encpkgs[e_split] = pkg_encode\n",
    "\n",
    "                # Making sure these variables would not leak elsewhere\n",
    "                del n_evlpnts, eii, e_splitidxs\n",
    "\n",
    "            for e_split, e_spltcfg in e_spltcfgs.items():\n",
    "                # Getting the split encoded data\n",
    "                (emu_mb, esigma_mb, u_eppmbs, ey_mbs, ei_mb) = e_encpkgs[e_split]\n",
    "                # The variant configs\n",
    "                e_vrntcfgs = e_spltcfg['vrnts']\n",
    "                # The number of samples in the split for evaluation\n",
    "                n_evlpnts = e_spltcfg['n']\n",
    "\n",
    "                eis_vrntdatas = dict()\n",
    "                for e_vrnt, e_vrntcfg in e_vrntcfgs.items():\n",
    "                    ######### Phase II: Compiling the latent variables of all variants #########\n",
    "                    timer.tic(f'sample/{eid}/{e_split}/{e_vrnt}/zsample')\n",
    "                    ltnt_pkg = Evaluation.sample_zy(\n",
    "                        emu_mb, esigma_mb, ey_mbs, ei_mb, y_dims,\n",
    "                        n_seeds, n_evlpnts, ltnt_dim, is_condvae, erng, e_encpkgs,\n",
    "                        eid, e_split, e_vrnt, e_vrntcfg, tch_device, tch_dtype)\n",
    "                    emu_mbv, esigma_mbv, ez_mbv, ey_mbvs, ei_mbv, n_rcns, vrnt_type = ltnt_pkg\n",
    "                    timer.toc()\n",
    "\n",
    "                    ############## Phase III: Applying the decoder to all splits ###############\n",
    "                    # Applying the decoder to the evaluation samples\n",
    "                    timer.tic(f'sample/{eid}/{e_split}/{e_vrnt}/decode')\n",
    "                    if vrnt_type == 'orig':\n",
    "                        assert n_rcns == 1\n",
    "                        uhat_eppmbvs = {u_node: u_tnsr.unsqueeze(2)\n",
    "                            for u_node, u_tnsr in u_eppmbs.items()}\n",
    "                        for u_node, (n_uchnls, n_ulen) in ue_dims.items():\n",
    "                            eu_mbv = uhat_eppmbvs[u_node]\n",
    "                            assert eu_mbv.shape == (n_seeds, n_evlpnts, n_rcns, n_uchnls, n_ulen)\n",
    "                        ey_mbvs2 = ey_mbvs\n",
    "                    else:\n",
    "                        assert ez_mbv is not None\n",
    "                        uhat_eppmbvs, eyhat_mbvs = Evaluation.decode(\n",
    "                            ez_mbv, ey_mbvs, nn_pp, decoder, e_pp, e_ppinpspc, lbl_pp, lblnn,\n",
    "                            is_condvae, lblnn_enctype, n_seeds, n_evlpnts, n_rcns, ltnt_dim, eval_bsnn, \n",
    "                            xn_dims, un_dims, ue_dims, xl_dims, ul_dims, y_dims)\n",
    "                        ey_mbvs2 = eyhat_mbvs\n",
    "                    timer.toc()\n",
    "\n",
    "                    eis_vrntdatas[e_vrnt] = (emu_mbv, esigma_mbv, ez_mbv, ey_mbvs2, uhat_eppmbvs, ei_mbv, n_rcns)\n",
    "\n",
    "                    if vrnt_type in ('genr', 'gaus'):\n",
    "                        assert e_split == 'normal'\n",
    "                        eis_vrntdatas[f'{e_vrnt}.inp'] = (None, None, None, ey_mbvs, None, None, n_rcns)\n",
    "\n",
    "                ######## Data Assembly for Metrics and Visualization #########\n",
    "                eis_mtrcinputs, eis_vizinputs = Evaluation.assemble(\n",
    "                    eid, e_split, eis_vrntdatas, n_seeds, n_evlpnts,\n",
    "                    ltnt_dim, ue_dims, y_dims)\n",
    "                e_mtrcinputs.update(eis_mtrcinputs)\n",
    "                e_vizinputs.update(eis_vizinputs)\n",
    "\n",
    "        #################### Metric Calculations ###################\n",
    "        e_perfstats, e_perfhists = Evaluation.performance(perfcfgs, e_mtrcinputs, aero_cstinfo, \n",
    "            asgn_mdlscfg, metric_mdlscfg, eparams, split_vars, n_seeds, n_t, mrng, timer, \n",
    "            tch_device, tch_dtype, tch_cdtype)\n",
    "\n",
    "        ############# Visualization: MDS Calculation ###############       \n",
    "        e_vizoutputs = Visualization.mds(e_vizinputs, eparams, vizcfgs, vrng, n_seeds,\n",
    "            ltnt_dim, y_dims, epoch, timer, tch_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if do_logtb:\n",
    "                for key, val in e_perfstats.items():\n",
    "                    tbwriter.add_scalar(f'perf/{key}', val.mean(), epoch)\n",
    "                \n",
    "            ############ Visualizations: Part 3 (Plotting) ##############\n",
    "            Visualization.draw(e_vizoutputs, vizcfgs, epoch, n_seeds,\n",
    "                chem_species, d_histbins, fig_dir, timer, tbwriter)\n",
    "\n",
    "            if do_logtb:\n",
    "                tbwriter.flush()\n",
    "\n",
    "        timer.tic('save/pump')\n",
    "        if epoch == 0:\n",
    "            # Just making sure there is an entry for 'save/pump' before \n",
    "            # writing the report in the `dprepper.prep` call.\n",
    "            timer.toctic('save/pump')\n",
    "\n",
    "        # `stat_dict` will become hierarchical in the `.prep()` call\n",
    "        stat_dict = {'perf': e_perfstats, **loss_dict}\n",
    "        nn_dict = {'mdl/enc': encoder, 'mdl/dec': decoder}\n",
    "        if is_condvae and (lblnn_enctype == 'vae'): \n",
    "            nn_dict['mdl/lbl'] = lblnn.encoder\n",
    "        elif is_condvae and (lblnn_enctype == 'plain'): \n",
    "            nn_dict['mdl/lbl'] = lblnn\n",
    "        elif is_condvae:\n",
    "            raise ValueError('undefined case')\n",
    "        strg_dict = deep2hie({'var/eval/raw': e_vizinputs, \n",
    "            'var/eval/mds': e_vizoutputs})\n",
    "\n",
    "        # Filtering out the data that should not be stored\n",
    "        DataPrepper.filter_vizstrg(strg_dict, eparams, vizcfgs)\n",
    "        strg_dict['var/perf/hists'] = e_perfhists\n",
    "\n",
    "        # Prepping the data tuples to be stored to the disk\n",
    "        dtups = dprepper.prep(strg_dict, stat_dict, nn_dict, timer, epoch)\n",
    "\n",
    "        dwriter.add(data_tups=dtups, file_path=hdfpth)\n",
    "        timer.toc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c721c68",
   "metadata": {},
   "source": [
    "# Storing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011262d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:18:52.219417Z",
     "iopub.status.busy": "2025-05-19T22:18:52.219290Z",
     "iopub.status.idle": "2025-05-19T22:22:17.410395Z",
     "shell.execute_reply": "2025-05-19T22:22:17.409320Z",
     "shell.execute_reply.started": "2025-05-19T22:18:52.219405Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    if results_dir is not None:\n",
    "        print(f'Training finished in {time.time() - trn_sttime:.1f} seconds.')\n",
    "\n",
    "    # Closing up all the data/profile/log-writers\n",
    "    timer.tic('save/close')\n",
    "    dwriter.close()\n",
    "    timer.toc()\n",
    "\n",
    "    if do_logtb:\n",
    "        timer.tic('save/tbflush')\n",
    "        tbwriter.flush()\n",
    "        timer.toc()\n",
    "\n",
    "    if do_profile:\n",
    "        timer.tic('save/profile')\n",
    "        profiler.stop()\n",
    "        html = profiler.output_html()\n",
    "        htmlpath = f'{cfgstrg_dir}/profiler.html'\n",
    "        with open(htmlpath, 'w') as fp:\n",
    "            fp.write(html.encode('ascii', errors='ignore').decode('ascii'))\n",
    "        timer.toc()\n",
    "\n",
    "    if results_dir is not None:\n",
    "        timer.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5425f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:22:17.415236Z",
     "iopub.status.busy": "2025-05-19T22:22:17.415083Z",
     "iopub.status.idle": "2025-05-19T22:22:19.208255Z",
     "shell.execute_reply": "2025-05-19T22:22:19.207653Z",
     "shell.execute_reply.started": "2025-05-19T22:22:17.415223Z"
    }
   },
   "outputs": [],
   "source": [
    "    # Preparing the memory usage\n",
    "    outdict = dict()\n",
    "    tchmemusage = profmem()\n",
    "    assert str(tch_device) in tchmemusage\n",
    "    if 'cuda' in device_name:\n",
    "        tch_dvcmem = torch.cuda.get_device_properties(tch_device).total_memory\n",
    "    else:\n",
    "        tch_dvcmem = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "    outdict['dvc/mem/alloc'] = tchmemusage[str(tch_device)]\n",
    "    outdict['dvc/mem/total'] = tch_dvcmem"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d063f202",
   "metadata": {
    "tags": [
     "active-py"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    return outdict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    use_argparse = True\n",
    "    if use_argparse:\n",
    "        import argparse\n",
    "        my_parser = argparse.ArgumentParser()\n",
    "        my_parser.add_argument('-c', '--configid', action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-d', '--device',   action='store', type=str, required=True)\n",
    "        my_parser.add_argument('-s', '--nodesize', action='store', type=int, default=1)\n",
    "        my_parser.add_argument('-r', '--noderank', action='store', type=int, default=0)\n",
    "        my_parser.add_argument('-i', '--rsmindex', action='store', type=str, default=\"0.0\")\n",
    "        my_parser.add_argument('-n', '--nconfigs', action='store', type=int, default=-1)\n",
    "        my_parser.add_argument('-g', '--nseedmax', action='store', type=int, default=0)\n",
    "        my_parser.add_argument('--dry-run', action='store_true')\n",
    "        args = my_parser.parse_args()\n",
    "        args_configid = args.configid\n",
    "        args_device_name = args.device\n",
    "        args_nodesize = args.nodesize\n",
    "        args_noderank = args.noderank\n",
    "        arsg_rsmindex = args.rsmindex\n",
    "        args_nconfigs = args.nconfigs\n",
    "        args_dryrun = args.dry_run\n",
    "        args_nseedmax = args.nseedmax\n",
    "    else:\n",
    "        args_configid = 'lvl1/lvl2/poiss2d'\n",
    "        args_device_name = 'cuda:0'\n",
    "        args_nodesize = 1\n",
    "        args_noderank = 0\n",
    "        arsg_rsmindex = 0\n",
    "        args_nseedmax = 0\n",
    "        args_dryrun = True\n",
    "\n",
    "    assert args_noderank < args_nodesize\n",
    "    cfgidsplit = args_configid.split('/')\n",
    "    # Example: args_configid == 'lvl1/lvl2/poiss2d'\n",
    "    config_id = cfgidsplit[-1]\n",
    "    # Example: config_id == 'poiss2d'\n",
    "    config_tree = '/'.join(cfgidsplit[:-1])\n",
    "    # Example: config_tree == 'lvl1/lvl2'\n",
    "\n",
    "    os.makedirs(configs_dir, exist_ok=True)\n",
    "    # Example: configs_dir == '.../code_bspinn/config'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Example: results_dir == '.../code_bspinn/result'\n",
    "    # os.makedirs(storage_dir, exist_ok=True)\n",
    "    # Example: storage_dir == '.../code_bspinn/storage'\n",
    "    \n",
    "    if args_dryrun:\n",
    "        print('>> Running in dry-run mode', flush=True)\n",
    "\n",
    "    cfg_path_ = f'{configs_dir}/{config_tree}/{config_id}'\n",
    "    cfg_exts = [cfg_ext for cfg_ext in ['json', 'yml', 'yaml'] if exists(f'{cfg_path_}.{cfg_ext}')]\n",
    "    assert len(cfg_exts) < 2, f'found multiple {cfg_exts} extensions for {cfg_path_}'\n",
    "    assert len(cfg_exts) > 0, f'found no json or yaml config at {cfg_path_}'\n",
    "    cfg_ext = cfg_exts[0]\n",
    "    cfg_path = f'{cfg_path_}.{cfg_ext}'\n",
    "    print(f'>> Reading configuration from {cfg_path}', flush=True)\n",
    "    \n",
    "    if cfg_ext.lower() == 'json':\n",
    "        with open(cfg_path, 'r') as fp:\n",
    "            json_cfgdict = json.load(fp, object_pairs_hook=dict)\n",
    "    elif cfg_ext.lower() in ('yml', 'yaml'):\n",
    "        with open(cfg_path, 'r') as fp:\n",
    "            json_cfgdict = dict(ruyaml.safe_load(fp))\n",
    "    else:\n",
    "        raise RuntimeError(f'unknown config extension: {cfg_ext}')\n",
    "    \n",
    "    if args_dryrun:\n",
    "        import tempfile\n",
    "        temp_resdir = tempfile.TemporaryDirectory(dir=f'{PROJPATH}/trash')\n",
    "        temp_strdir = tempfile.TemporaryDirectory(dir=f'{PROJPATH}/trash')\n",
    "        print(f'>> [dry-run] Temporary results dir placed at {temp_resdir.name}')\n",
    "        print(f'>> [dry-run] Temporary storage dir placed at {temp_strdir.name}')\n",
    "        results_dir = temp_resdir.name\n",
    "        storage_dir = temp_strdir.name\n",
    "        \n",
    "        dr_maxfrq = 10\n",
    "        dr_opts = {'opt/epoch': dr_maxfrq, 'io/cmprssn_lvl': 0, 'io/strg/logtb': False,\n",
    "            'io/strg/profile': False, 'io/strg/savefigs': False, 'io/eval/enbl': False}\n",
    "        for opt in dr_opts:\n",
    "            assert opt in json_cfgdict\n",
    "            json_cfgdict[opt] = dr_opts[opt]\n",
    "        for opt in fnmatch.filter(json_cfgdict.keys(), '*/frq'):\n",
    "            if json_cfgdict[opt] > dr_maxfrq:\n",
    "                json_cfgdict[opt] = dr_opts[opt] = dr_maxfrq\n",
    "        for opt in fnmatch.filter(json_cfgdict.keys(), '*/viz/*/store'):\n",
    "            json_cfgdict[opt] = dr_opts[opt] = False\n",
    "\n",
    "        print(f'>> [dry-run] The following options were made overriden:', flush=True)\n",
    "        for opt, val in dr_opts.items():\n",
    "            print(f'>>           {opt}: {val}', flush=True)\n",
    "            \n",
    "    nodepstfx = f'_{args_noderank:02d}'\n",
    "    # Example: nodepstfx in ('_00', '_01', ...)\n",
    "    json_cfgdict['io/config_id'] = f'{config_id}{nodepstfx}'\n",
    "    # Example: ans in ('poiss2d', 'poiss2d_01')\n",
    "    json_cfgdict['io/results_dir'] = f'{results_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/result/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/storage_dir'] = f'{storage_dir}/{config_tree}'\n",
    "    # Example: ans == '.../code_bspinn/storage/lvl1/lv2/poiss2d'\n",
    "    json_cfgdict['io/tch/device'] = args_device_name\n",
    "    # Example: args_device_name == 'cuda:0'\n",
    "\n",
    "    # Pre-processing and applying the looping processes\n",
    "    all_cfgdicts = preproc_cfgdict(json_cfgdict)\n",
    "        \n",
    "    # Selecting this node's config dict subset\n",
    "    node_cfgdicts = [config_dict for cfgidx, config_dict in enumerate(all_cfgdicts) \n",
    "        if (cfgidx % args_nodesize == args_noderank)]\n",
    "    n_nodecfgs = len(node_cfgdicts)\n",
    "    \n",
    "    # Going over the config dicts one-by-one\n",
    "    rsmidx, rsmprt = tuple(int(x) for x in arsg_rsmindex.split('.'))\n",
    "    for cfgidx, config_dict1 in enumerate(node_cfgdicts):\n",
    "        # if a positive `args_nconfigs` is provided, we should only run \n",
    "        # `args_nconfigs` configs starting at `rsmidx`.\n",
    "        is_toonew = ((args_nconfigs >= 0) and (cfgidx >= rsmidx + args_nconfigs))\n",
    "        if (cfgidx < rsmidx) or is_toonew:\n",
    "            continue\n",
    "        \n",
    "        # Parsing all the \"/ref\"-, \"/def\"-, and \"/pop\"-ending keys\n",
    "        config_dict = parse_refs(config_dict1, trnsfrmtn='hie', pathsep=' -> ', \n",
    "            cfg_path=f'{config_tree}/{config_id}.{cfg_ext}')\n",
    "        # Dropping the trailing references section\n",
    "        ref_dict = get_subdict(config_dict, prefix='refs', pop=True)\n",
    "        \n",
    "        # Getting a single seed run to estimate the memory usage\n",
    "        tempcfg = config_dict.copy()\n",
    "        tempcfg['io/results_dir'] = None\n",
    "        tempcfg['io/storage_dir'] = None\n",
    "        tempcfg['opt/epoch'] = 0\n",
    "\n",
    "        if args_nseedmax <= 0:\n",
    "            # Taking a single-seed run\n",
    "            args_device = torch.device(args_device_name)\n",
    "            if 'cuda' in args_device_name:\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                torch.cuda.reset_peak_memory_stats(device=args_device)\n",
    "            tempcfg1 = dict(tempcfg)\n",
    "            tempcfg1['rng_seed/list'] = [0]\n",
    "            tod1 = main(tempcfg1)\n",
    "            allocmem1, totmem = tod1['dvc/mem/alloc'], tod1['dvc/mem/total']\n",
    "\n",
    "            # Taking a double-seed run\n",
    "            if 'cuda' in args_device_name:\n",
    "                torch.cuda.reset_peak_memory_stats(device=args_device)\n",
    "            tempcfg2 = dict(tempcfg)\n",
    "            tempcfg2['rng_seed/list'] = [0, 1000]\n",
    "            tod2 = main(tempcfg2)\n",
    "            allocmem2, totmem = tod2['dvc/mem/alloc'], tod2['dvc/mem/total']\n",
    "\n",
    "            # Doing a linear interpolation to see how many seeds can fit\n",
    "            allocmem2 = max(allocmem2, int(np.ceil(allocmem1 * 1.001)))\n",
    "            nsd_max = 1 + int(0.75 * (totmem - allocmem1) / (allocmem2 - allocmem1))\n",
    "        else:\n",
    "            nsd_max = args_nseedmax\n",
    "        \n",
    "        # Computing how many parts we must split the original config into\n",
    "        cfg_seeds = config_dict['rng_seed/list']\n",
    "        nprts = int(np.ceil(len(cfg_seeds) / nsd_max))\n",
    "        if args_nseedmax <= 0:\n",
    "            mem_bias = 2 * allocmem1 - allocmem2\n",
    "            mem_slope = allocmem2 - allocmem1\n",
    "            print(f'>> Config index {cfgidx} takes {mem_bias/1e6:.1f} MB + {mem_slope/1e6:.1f} ' + \n",
    "                f'MB/seed (out of {totmem/1e9:.1f} GB)', flush=True)\n",
    "            print(f'>> Config index {cfgidx} must be ' + \n",
    "                f'devided into {nprts} parts.', flush=True)\n",
    "        \n",
    "        # Looping over each part of the config\n",
    "        for iprt in range(nprts):\n",
    "            if (cfgidx == rsmidx) and (iprt < rsmprt):\n",
    "                continue\n",
    "            print(f'>>> Started Working on config index {cfgidx}.{iprt}' + \n",
    "                  f' (out of {nprts} parts and {n_nodecfgs} configs).', flush=True)\n",
    "            iprtcfgseeds = cfg_seeds[(iprt*nsd_max):((iprt+1)*nsd_max)]\n",
    "            iprtcfgdict = config_dict.copy()\n",
    "            iprtcfgdict['rng_seed/list'] = iprtcfgseeds\n",
    "            main(iprtcfgdict)\n",
    "            print('-' * 40, flush=True)\n",
    "        print(f'>> Finished working on config index {cfgidx} ' + \n",
    "              f'(out of {n_nodecfgs} configs).', flush=True)\n",
    "        print('='*80, flush=True)\n",
    "        \n",
    "    if args_dryrun:\n",
    "        print(f'>> [dry-run] Cleaning up {temp_resdir.name}')\n",
    "        temp_resdir.cleanup()\n",
    "        print(f'>> [dry-run] Cleaning up {temp_strdir.name}')\n",
    "        temp_strdir.cleanup()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86ba8221",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
