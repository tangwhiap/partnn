{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "from textwrap import dedent\n",
    "import ruamel.yaml as ruyaml\n",
    "from scipy.stats import norm\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import defaultdict\n",
    "from bokeh.io import output_file, save\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from partnn.io_cfg import data_dir\n",
    "from partnn.tch_utils import torch_cdf\n",
    "from partnn.aerometrics import get_relerr\n",
    "from n08_utils import squeeze_colnames, get_aggdf, get_dfidxs, draw_matplotlib\n",
    "from n08_utils import build_dashboard, get_dashdata, adjust_mtrcnames, BStrapAgg\n",
    "from n20_utils import plot_mpl, load_histdata, tag_axis, print_axheader, mark_rect\n",
    "from partnn.io_utils import parse_refs, deep2hie, hie2deep, get_subdict, get_subdictrnmd, get_ovatgrps\n",
    "from partnn.io_utils import hie2deep, downcast_df, decomp_df, parse_refs, resio, sort_keys, load_h5data\n",
    "from partnn.io_utils import save_h5data, load_h5data, get_h5du, drop_unqcols, eval_formula, results_dir, save_h5datav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './08_plotting'\n",
    "! mkdir -p {workdir}\n",
    "\n",
    "suppdir = f'{workdir}/supplement'\n",
    "! mkdir -p {suppdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerosol Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aerocsts():\n",
    "    n_epoch, n_chem, n_bins, n_wave = 2, 15, 19, 8\n",
    "\n",
    "    chem_species = ['SO4', 'NO3', 'Cl', 'NH4', 'ARO1', 'ARO2', 'ALK1', 'OLE1', \n",
    "        'API1', 'Na', 'OIN', 'OC', 'BC', 'MOC', 'H2O']\n",
    "\n",
    "    diam_low, diam_high = 10**(-8.75), 1e-4\n",
    "    logdiam_low, logdiam_high = np.log(diam_low), np.log(diam_high)\n",
    "    logdiams = np.linspace(logdiam_low, logdiam_high, n_bins + 1, endpoint=True)\n",
    "    d_histbins = np.exp(logdiams)\n",
    "    assert d_histbins.shape == (n_bins + 1,)\n",
    "\n",
    "    # The log-epsilon interval\n",
    "    eps_histmin, eps_histmax, n_epshist = 1e-3, 1e-1, 100\n",
    "    eps_histbins = np.exp(np.linspace(math.log(eps_histmin), math.log(eps_histmax), n_epshist + 1))\n",
    "    assert eps_histbins.shape == (n_epshist + 1,)\n",
    "\n",
    "    tmprtr_inpmin, tmprtr_inpmax, n_tmprtr = -40, 0, 100\n",
    "    temprtr_bins = np.linspace(tmprtr_inpmin, tmprtr_inpmax, n_tmprtr + 1)\n",
    "    assert temprtr_bins.shape == (n_tmprtr + 1,)\n",
    "\n",
    "    len_wvmin, len_wvmax = 300e-9, 1000e-9\n",
    "    len_wv = np.linspace(len_wvmin, len_wvmax, n_wave)\n",
    "    assert len_wv.shape == (n_wave,)\n",
    "\n",
    "    ycol2dims = {'m_chmprthst': (n_chem, n_bins), 'n_prthst': (1, n_bins), \n",
    "        'ccn_cdf': (1, n_epshist), 'qs_prt': (n_wave, n_bins), \n",
    "        'qscs_prt': (n_wave, n_bins), 'qs_pop': (n_wave, 1), \n",
    "        'qa_prt': (n_wave, n_bins),  'qacs_prt': (n_wave, n_bins), \n",
    "        'qa_pop': (n_wave, 1),  'frznfrac_tmp': (1, n_tmprtr), \n",
    "        'logfrznfrac_tmp': (1, n_tmprtr)}\n",
    "\n",
    "    # The dictionary of aerosol constants\n",
    "    aero_csts = dict(n_chem=n_chem, n_bins=n_bins, n_wave=n_wave, \n",
    "        eps_histmin=eps_histmin, eps_histmax=eps_histmax, n_epshist=n_epshist, \n",
    "        tmprtr_inpmin=tmprtr_inpmin, tmprtr_inpmax=tmprtr_inpmax, n_tmprtr=n_tmprtr, \n",
    "        len_wvmin=len_wvmin, len_wvmax=len_wvmax, len_wv=len_wv, diam_low=diam_low, \n",
    "        diam_high=diam_high, d_histbins=d_histbins, eps_histbins=eps_histbins, \n",
    "        temprtr_bins=temprtr_bins, chem_species=chem_species, ycol2dims=ycol2dims)\n",
    "    \n",
    "    return aero_csts, ycol2dims\n",
    "\n",
    "aero_csts, ycol2dims = get_aerocsts()\n",
    "\n",
    "# The sample index used for the intro figures\n",
    "i_sampintro = 5391\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binidx(x, x_min, x_max, n_x, tnsfrm='none'):\n",
    "    assert (x_min <= x <= x_max), x\n",
    "    assert tnsfrm in ('log', 'none')\n",
    "    x2 = math.log(x) if tnsfrm == 'log' else x\n",
    "    x_min2 = math.log(x_min) if tnsfrm == 'log' else x_min\n",
    "    x_max2 = math.log(x_max) if tnsfrm == 'log' else x_max\n",
    "    i_x = round((x2 - x_min2) * n_x / (x_max2 - x_min2))\n",
    "    assert (0 <= i_x < n_x), f'x={x}, i_x={i_x}'\n",
    "\n",
    "    alpha = (i_x / n_x)\n",
    "    assert (0 <= alpha <= 1), alpha\n",
    "    x3 = alpha * x_max2 + (1 - alpha) * x_min2\n",
    "    x4 = math.exp(x3) if tnsfrm == 'log' else x3\n",
    "    return x4, i_x\n",
    "\n",
    "@torch.no_grad()\n",
    "def np_flatten(input, start_dim, end_dim):\n",
    "    return torch.from_numpy(input).flatten(start_dim, end_dim).cpu().numpy()\n",
    "\n",
    "def cnvrt_physunits(v_ydataraw: dict, aero_csts: dict):\n",
    "    \"\"\"\n",
    "    Takes the freshly loaded data out of the storage HDF files, and converts the \n",
    "    physical units of the data (e.g., from kg to ug).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v_ydataraw: (dict) a dictionary of the loaded data arrays.\n",
    "\n",
    "    aero_csts: (dict) a dictionary of the physical aerosol constants.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_ydata: (dict) The same data with the physical units converted.\n",
    "\n",
    "    aero_csts: (dict) The same aerosol constants with the units converted.\n",
    "    \"\"\"\n",
    "\n",
    "    n_bins = aero_csts['n_bins']\n",
    "    n_wave = aero_csts['n_wave']\n",
    "    chem_species = aero_csts['chem_species']\n",
    "\n",
    "    d_histbins = aero_csts['d_histbins']\n",
    "    assert d_histbins.shape == (n_bins + 1,)\n",
    "    len_wv = aero_csts['len_wv']\n",
    "    assert len_wv.shape == (n_wave,)\n",
    "\n",
    "    # Finding `log10len_bin`\n",
    "    logdiams = np.log(d_histbins)\n",
    "    logdiam_low, logdiam_high, n_bins = logdiams[0], logdiams[-1], len(d_histbins) - 1\n",
    "    log10len_bin = (logdiam_high - logdiam_low) / (n_bins * np.log(10))\n",
    "\n",
    "    # Making sure the `d_histbins` is log-uniformly distributed.\n",
    "    logdiams2 = np.linspace(logdiam_low, logdiam_high, n_bins + 1, endpoint=True)\n",
    "    d_histbins2 = np.exp(logdiams2)\n",
    "    assert d_histbins2.shape == (n_bins + 1,)\n",
    "    assert np.allclose(d_histbins, d_histbins2)\n",
    "\n",
    "    # Converting the diameter data unit from `m` to `um`\n",
    "    d_histbinsum = d_histbins * 1e6\n",
    "    len_wvum = len_wv * 1e6\n",
    "\n",
    "    ########### Data Cleaning and Unit Conversions ############\n",
    "    v_ydata = dict(v_ydataraw)\n",
    "    vrnts = list({key.split('/', 1)[1]: None for key in v_ydata})\n",
    "    \n",
    "    ycols = ['m_chmprthst', 'n_prthst', 'qs_pop', 'qa_pop', \n",
    "        'qs_prt', 'qa_prt', 'qscs_prt', 'qacs_prt']\n",
    "    ycol2vrnts = {ycol: list(get_subdict(v_ydata, ycol)) for ycol in ycols}\n",
    "    ycol2vrnts['m_prthst'] = ycol2vrnts['m_chmprthst']\n",
    "    ycol2vrnts['m_chmprt'] = ycol2vrnts['m_chmprthst']\n",
    "\n",
    "    # Zeroing out negative particle mass or count value\n",
    "    for vrnt in ycol2vrnts['m_chmprthst']:\n",
    "        v_ydata[f'm_chmprthst/{vrnt}'] = np.clip(v_ydata[f'm_chmprthst/{vrnt}'], 0, None)\n",
    "    for vrnt in ycol2vrnts['n_prthst']:\n",
    "        v_ydata[f'n_prthst/{vrnt}'] = np.clip(v_ydata[f'n_prthst/{vrnt}'], 0, None)\n",
    "\n",
    "    # The ACSM mass measurements\n",
    "    #   1. The `1e9` rate is for conversion from `kg` to `ug`\n",
    "    inorg_species = ['SO4', 'NO3', 'NH4']\n",
    "    orgnc_species = ['OC', 'MOC', 'ARO1', 'ARO2', 'ALK1', 'OLE1', 'API1']\n",
    "    i_ychmsinorg = [chem_species.index(chm) for chm in inorg_species]\n",
    "    i_ychmsorgnc = [chem_species.index(chm) for chm in orgnc_species]\n",
    "    acsm_species = inorg_species + ['OA']\n",
    "    for vrnt in ycol2vrnts['m_chmprthst']:\n",
    "        m_chmprtvrnt = v_ydata[f'm_chmprthst/{vrnt}'].sum(axis=-1)\n",
    "        m_inorgvrnt = m_chmprtvrnt[..., i_ychmsinorg]\n",
    "        m_orgncvrnt = m_chmprtvrnt[..., i_ychmsorgnc].sum(axis=-1, keepdims=True)\n",
    "        m_acsmvrnt = np.concatenate([m_inorgvrnt, m_orgncvrnt], axis=-1)\n",
    "        v_ydata[f'm_acsm/{vrnt}'] = m_acsmvrnt[..., None] * 1e9\n",
    "\n",
    "    # The SMPS particle concentration measurements\n",
    "    # The `1e-6` rate is for conversion from `#/m^3` to `#/cm^3`\n",
    "    # Restricting the smps diameter bins to 10nm-560nm.\n",
    "    i1_smpsdiam = np.abs(d_histbinsum[:-1] - 0.010).argmin()\n",
    "    i2_smpsdiam = np.abs(d_histbinsum[:-1] - 0.560).argmin()\n",
    "    assert abs(d_histbinsum[i1_smpsdiam] - 0.010) < 0.001, dedent(f'''\n",
    "        Could not find a 10nm diameter: \n",
    "            d_histbinsum: {d_histbinsum}''')\n",
    "    assert abs(d_histbinsum[i2_smpsdiam] - 0.560) < 0.010, dedent(f'''\n",
    "        Could not find a 560nm diameter: \n",
    "            d_histbinsum: {d_histbinsum}''')\n",
    "    n_binssmps = i2_smpsdiam - i1_smpsdiam\n",
    "    d_binssmpsum = d_histbinsum[i1_smpsdiam: i2_smpsdiam + 1]\n",
    "    for vrnt in ycol2vrnts['n_prthst']:\n",
    "        v_ydata[f'n_smps/{vrnt}'] = v_ydata[f'n_prthst/{vrnt}'][..., i1_smpsdiam: i2_smpsdiam] * 1e-6\n",
    "\n",
    "    # Converting the mass data unit:\n",
    "    #   1. The `1e9` rate is for conversion from `kg` to `ug`\n",
    "    #   2. The `1/log10len_bin` rate is for diameter histogram density scaling.\n",
    "    for vrnt in ycol2vrnts['m_chmprthst']:\n",
    "        v_ydata[f'm_chmprthst/{vrnt}'] *= (1e9 / log10len_bin)\n",
    "\n",
    "    # Converting the mass data unit:\n",
    "    #   1. The `1e-6` rate is for conversion from `#/m^3` to `#/cm^3`\n",
    "    #   2. The `1/log10len_bin` rate is for diameter histogram density scaling.\n",
    "    for vrnt in ycol2vrnts['n_prthst']:\n",
    "        v_ydata[f'n_prthst/{vrnt}'] *= (1e-6 / log10len_bin)\n",
    "\n",
    "    # Converting the cross-section unit from `1/m` to `1/Mm`\n",
    "    for ycol in ['qs_pop', 'qscs_prt', 'qa_pop', 'qacs_prt']:\n",
    "        for vrnt in ycol2vrnts[ycol]:\n",
    "            v_ydata[f'{ycol}/{vrnt}'] = v_ydata[f'{ycol}/{vrnt}'] * 1e6\n",
    "\n",
    "    # Computing the total mass data for each particle\n",
    "    for vrnt in ycol2vrnts['m_prthst']:\n",
    "        v_ydata[f'm_prthst/{vrnt}'] = v_ydata[f'm_chmprthst/{vrnt}'].sum(axis=-2, keepdims=True)\n",
    "    \n",
    "    # Computing the species mass data for each particle\n",
    "    for vrnt in ycol2vrnts['m_chmprt']:\n",
    "        v_ydata[f'm_chmprt/{vrnt}'] = v_ydata[f'm_chmprthst/{vrnt}'].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    # Restricting the diameter bins to a max of 10um.\n",
    "    i1_diam, i2_diam = 0, np.abs(d_histbinsum[:-1] - 10).argmin()\n",
    "    assert abs(d_histbinsum[i2_diam] - 10) < 0.1, dedent(f'''\n",
    "        Could not find a 10um diameter: \n",
    "            d_histbins: {d_histbins}\n",
    "            d_histbinsum: {d_histbinsum}''')\n",
    "\n",
    "    for ycol in ['m_chmprthst', 'm_prthst', 'n_prthst', 'qs_prt', 'qa_prt', 'qscs_prt', 'qacs_prt']:\n",
    "        for vrnt in ycol2vrnts[ycol]:\n",
    "            v_ydata[f'{ycol}/{vrnt}'] = v_ydata[f'{ycol}/{vrnt}'][..., i1_diam : i2_diam]\n",
    "    for vrnt in ycol2vrnts['qs_pop']:\n",
    "        v_ydata[f'qs_pop/{vrnt}'] = v_ydata[f'qscs_prt/{vrnt}'].sum(axis=-1, keepdims=True)\n",
    "    for vrnt in ycol2vrnts['qa_pop']:\n",
    "        v_ydata[f'qa_pop/{vrnt}'] = v_ydata[f'qacs_prt/{vrnt}'].sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    d_histbinsum2 = d_histbinsum[i1_diam : i2_diam + 1] \n",
    "    n_binsum2 = len(d_histbinsum2) - 1\n",
    "    chmrnm = {'OIN': 'Dust', 'OC': 'POA'}\n",
    "    chem_species2 = [chmrnm.get(chm, chm) for chm in chem_species]\n",
    "\n",
    "    aero_cstscnv = dict(aero_csts)\n",
    "    aero_cstscnv['n_bins'] = n_binsum2\n",
    "    aero_cstscnv['d_histbins'] = d_histbinsum2\n",
    "    aero_cstscnv['len_wv'] = len_wvum\n",
    "    aero_cstscnv['i1_diam'] = i1_diam\n",
    "    aero_cstscnv['i2_diam'] = i2_diam\n",
    "    aero_cstscnv['chem_species'] = chem_species2\n",
    "    aero_cstscnv['n_binssmps'] = n_binssmps\n",
    "    aero_cstscnv['d_binssmps'] = d_binssmpsum\n",
    "    aero_cstscnv['acsm_species'] = acsm_species\n",
    "    aero_cstscnv['n_chemacsm'] = len(acsm_species)\n",
    "\n",
    "    return v_ydata, aero_cstscnv\n",
    "\n",
    "def calc_aerometrics(v_ydata: dict, aero_cstscnv: dict, avg_errs: bool, vrnt_trg: str = 'rcnst'):\n",
    "    \"\"\"\n",
    "    Calculates the aerosol error metrics from the loaded and converted data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v_ydata: (dict) a dictionary of the loaded data arrays with the \n",
    "        physical units converted.\n",
    "\n",
    "    aero_cstscnv: (dict) a dictionary of the physical aerosol constants \n",
    "        with the physical units converted\n",
    "\n",
    "    \n",
    "    vrnt_trg: (str) The target variant. Either 'rcnst' or 'znrm'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v_mtrcdata: (dict) The aerosol error metrics, with the underlying data.\n",
    "    \"\"\"\n",
    "    \n",
    "    eps_histmin = aero_cstscnv['eps_histmin']\n",
    "    eps_histmax = aero_cstscnv['eps_histmax']\n",
    "    n_epshist = aero_cstscnv['n_epshist']\n",
    "    tmprtr_inpmin = aero_cstscnv['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = aero_cstscnv['tmprtr_inpmax']\n",
    "    n_tmprtr = aero_cstscnv['n_tmprtr']\n",
    "    len_wvmin = aero_cstscnv['len_wvmin']\n",
    "    len_wvmax = aero_cstscnv['len_wvmax']\n",
    "    n_wave = aero_cstscnv['n_wave']\n",
    "    n_binsum = aero_cstscnv['n_bins']\n",
    "    n_chem = aero_cstscnv['n_chem']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    \n",
    "\n",
    "    # Defining the specific super-saturation epsilons\n",
    "    eps1, i_eps1 = get_binidx(0.001, eps_histmin, eps_histmax, n_epshist, 'log')\n",
    "    eps2, i_eps2 = get_binidx(0.003, eps_histmin, eps_histmax, n_epshist, 'log')\n",
    "    eps3, i_eps3 = get_binidx(0.006, eps_histmin, eps_histmax, n_epshist, 'log')\n",
    "\n",
    "    # Defining the specific freezing temperatures\n",
    "    tmp1, i_tmp1 = get_binidx(-25, tmprtr_inpmin, tmprtr_inpmax, n_tmprtr)\n",
    "    tmp2, i_tmp2 = get_binidx(-17, tmprtr_inpmin, tmprtr_inpmax, n_tmprtr)\n",
    "    tmp3, i_tmp3 = get_binidx(-10, tmprtr_inpmin, tmprtr_inpmax, n_tmprtr)\n",
    "\n",
    "    # Defining the specific wave-length\n",
    "    wvl1, i_wvl1 = get_binidx(500e-9, len_wvmin, len_wvmax, n_wave)\n",
    "\n",
    "    # Computing the plotting data\n",
    "    v_mtrcdata = dict()\n",
    "\n",
    "    mtrcs_spec = [\n",
    "        #             ycol          ycol2,     n_chnls,         n_len,   e_type,   i_elow,      i_ehigh, rel_rdcdims, y_tnsstr\n",
    "        (        'ccn_cdf',          None,           1,     n_epshist,    'rel',   i_eps1,   i_eps3 + 1,    [-1, -2],      'y'),\n",
    "        (         'qs_pop',          None,      n_wave,             1,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (         'qa_pop',          None,      n_wave,             1,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (         'qs_pop',   'logqs_pop',      n_wave,             1, 'logrel',        0,         None,    [-1, -2],      'y'),\n",
    "        (         'qa_pop',   'logqa_pop',      n_wave,             1, 'logrel',        0,         None,    [-1, -2],      'y'),\n",
    "        (         'qs_prt',          None,      n_wave,      n_binsum,     None,        0,         None,        None,      'y'),\n",
    "        (         'qa_prt',          None,      n_wave,      n_binsum,     None,        0,         None,        None,      'y'),\n",
    "        (       'qscs_prt',          None,      n_wave,      n_binsum,     None,        0,         None,        None,      'y'),\n",
    "        (       'qacs_prt',          None,      n_wave,      n_binsum,     None,        0,         None,        None,      'y'),\n",
    "        ('logfrznfrac_tmp',          None,           1,      n_tmprtr,    'rel',   i_tmp1,   i_tmp3 + 1,    [-1, -2],      'y'),\n",
    "        (   'frznfrac_tmp',          None,           1,      n_tmprtr,    'rel',   i_tmp1,   i_tmp3 + 1,    [-1, -2],      'y'),\n",
    "        (    'm_chmprthst',          None,      n_chem,      n_binsum,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (       'n_prthst',          None,           1,      n_binsum,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (       'm_prthst',          None,           1,      n_binsum,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (    'm_chmprthst', 'm_perchmhst',      n_chem,      n_binsum,    'rel',        0,         None,        [-1],      'y'),\n",
    "        (         'm_acsm',          None,  n_chemacsm,             1,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (         'n_smps',          None,           1,    n_binssmps,    'rel',        0,         None,    [-1, -2],      'y'),\n",
    "        (       'm_chmprt',          None,      n_chem,             1,    'rel',        0,         None,    [-1, -2],      'y')]\n",
    "\n",
    "    for ycol, ycol2, n_chnls, n_len, e_type, i_elow, i_ehigh, rel_rdcdims, y_tnsstr in mtrcs_spec:\n",
    "        ycol2 = ycol if ycol2 is None else ycol2\n",
    "\n",
    "        y_rcnst = torch.from_numpy(v_ydata[f'{ycol}/{vrnt_trg}'])\n",
    "        n_seeds, n_snrt, n_rcns = y_rcnst.shape[:-2]\n",
    "        assert y_rcnst.shape == (n_seeds, n_snrt, n_rcns, n_chnls, n_len)\n",
    "\n",
    "        y_rcnst2 = y_rcnst.flatten(1, 2)\n",
    "        assert y_rcnst2.shape == (n_seeds, n_snrt * n_rcns, n_chnls, n_len)\n",
    "\n",
    "        tnsfm_y = eval(f'lambda y: {y_tnsstr}')\n",
    "        v_mtrcdata[f'{ycol2}/{vrnt_trg}'] = tnsfm_y(y_rcnst2).detach().cpu().numpy()\n",
    "\n",
    "        if f'{ycol}/orig' not in v_ydata:\n",
    "            v_mtrcdata[f'{ycol2}/err'] = None    \n",
    "            v_mtrcdata[f'{ycol2}/orig'] = None\n",
    "            v_mtrcdata[f'{ycol2}/origraw'] = None\n",
    "            v_mtrcdata[f'{ycol2}/origcdf'] = None\n",
    "            continue\n",
    "\n",
    "        y_orig1 = torch.from_numpy(v_ydata[f'{ycol}/orig'])\n",
    "        n_rcns1 = y_orig1.shape[2]\n",
    "        assert y_orig1.shape == (n_seeds, n_snrt, n_rcns1, n_chnls, n_len)\n",
    "\n",
    "        assert n_rcns1 == 1\n",
    "        y_orig2 = y_orig1.expand(n_seeds, n_snrt, n_rcns, n_chnls, n_len)\n",
    "        assert y_orig2.shape == (n_seeds, n_snrt, n_rcns, n_chnls, n_len)\n",
    "        \n",
    "        y_orig3 = y_orig2.flatten(1, 2)\n",
    "        assert y_orig3.shape == (n_seeds, n_snrt * n_rcns, n_chnls, n_len)\n",
    "\n",
    "        y_orig4 = y_orig3[..., i_elow: i_ehigh]\n",
    "        y_rcnst3 = y_rcnst2[..., i_elow: i_ehigh]\n",
    "        \n",
    "        if e_type == 'mae':\n",
    "            y_err = (y_orig4 - y_rcnst3).abs().mean(dim=[-1, -2])\n",
    "            assert y_err.shape == (n_seeds, n_snrt * n_rcns)\n",
    "\n",
    "            if avg_errs:\n",
    "                y_err2 = y_err.mean(dim=-1).detach().cpu().numpy()\n",
    "                assert y_err2.shape == (n_seeds,)\n",
    "            else:\n",
    "                y_err2 = y_err.detach().cpu().numpy()\n",
    "                assert y_err2.shape == (n_seeds, n_snrt * n_rcns)\n",
    "        elif e_type in ('rel', 'logrel'):\n",
    "            if e_type == 'rel':\n",
    "                y_orig5, y_rcnst4 = y_orig4, y_rcnst3\n",
    "            elif e_type == 'logrel':\n",
    "                y_orig5 = y_orig4.clamp(min=1e-1).log()\n",
    "                y_rcnst4 = y_rcnst3.clamp(min=1e-1).log()\n",
    "            else:\n",
    "                raise ValueError(f'e_type={e_type} undefined')\n",
    "            \n",
    "            y_err = get_relerr(y_orig5, y_rcnst4, rdcdims=rel_rdcdims) / 2.0\n",
    "            y_err = y_err.unsqueeze(-1).flatten(2, -1).mean(-1)\n",
    "            assert y_err.shape == (n_seeds, n_snrt * n_rcns)\n",
    "\n",
    "            if avg_errs:\n",
    "                y_err2 = y_err.mean(dim=-1).detach().cpu().numpy()\n",
    "                assert y_err2.shape == (n_seeds,)\n",
    "            else:\n",
    "                y_err2 = y_err.detach().cpu().numpy()\n",
    "                assert y_err2.shape == (n_seeds, n_snrt * n_rcns)\n",
    "        else:\n",
    "            assert e_type is None, f'undefined e_type = {e_type}'\n",
    "            y_err, y_err2 = None, None\n",
    "        \n",
    "        # Computing the cdf if necessary\n",
    "        need_yorigcdf = (vrnt_trg == 'znrm')\n",
    "        if need_yorigcdf:\n",
    "            y_rcnstsrtd = y_rcnst.sort(dim=2).values\n",
    "            assert y_rcnstsrtd.shape == (n_seeds, n_snrt, n_rcns, n_chnls, n_len)\n",
    "\n",
    "            y_origcdf1 = torch_cdf(y_orig1, y_rcnstsrtd, dim=2, \n",
    "                frame='data', domain=[-float('inf'), float('inf')])\n",
    "            assert y_origcdf1.shape == (n_seeds, n_snrt, n_rcns1, n_chnls, n_len)\n",
    "\n",
    "            y_origraw = y_orig1.squeeze(dim=2)\n",
    "            assert y_origraw.shape == (n_seeds, n_snrt, n_chnls, n_len)\n",
    "\n",
    "            y_origcdf2 = y_origcdf1.squeeze(dim=2)\n",
    "            assert y_origcdf2.shape == (n_seeds, n_snrt, n_chnls, n_len)\n",
    "        else:\n",
    "            y_origcdf2 = None\n",
    "        \n",
    "        v_mtrcdata[f'{ycol2}/err'] = y_err2 if (y_err2 is not None) else None    \n",
    "        v_mtrcdata[f'{ycol2}/orig'] = tnsfm_y(y_orig3).detach().cpu().numpy()\n",
    "        v_mtrcdata[f'{ycol2}/origraw'] = tnsfm_y(y_origraw).detach().cpu().numpy() if need_yorigcdf else None\n",
    "        v_mtrcdata[f'{ycol2}/origcdf'] = tnsfm_y(y_origcdf2).detach().cpu().numpy() if need_yorigcdf else None\n",
    "\n",
    "    v_mtrcdata.update(dict(\n",
    "        eps1=eps1, i_eps1=i_eps1, eps2=eps2, i_eps2=i_eps2, eps3=eps3, i_eps3=i_eps3,\n",
    "        tmp1=tmp1, i_tmp1=i_tmp1, tmp2=tmp2, i_tmp2=i_tmp2, tmp3=tmp3, i_tmp3=i_tmp3,\n",
    "        wvl1=wvl1, i_wvl1=i_wvl1))\n",
    "    \n",
    "    return v_mtrcdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Data For Aerosol Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "exprm_infos = dict()\n",
    "expfam_cfgs = v_mplcfgs['expspec']\n",
    "for expfam, expfam_cfg in hie2deep(expfam_cfgs, maxdepth=1).items():\n",
    "    exp_id2fpidx = get_subdict(expfam_cfg, 'fpidx', pop=True)\n",
    "    splt2vrnts = get_subdict(expfam_cfg, 'vrnts', pop=True)\n",
    "    resdir = expfam_cfg.pop('resdir').format(results_dir=results_dir)\n",
    "    figdir = expfam_cfg.pop('fig/dir').format(workdir=workdir, suppdir=suppdir)\n",
    "    n_seeds = expfam_cfg.pop('n_seeds')\n",
    "    nicknmfrmla = expfam_cfg.pop('nicknm/frmla')\n",
    "    nicknmfrmla = 'None' if nicknmfrmla is None else nicknmfrmla\n",
    "    i_figcfg = get_subdict(expfam_cfg, 'fig/idx', pop=True)\n",
    "    n_rcnsspec = get_subdict(expfam_cfg, 'n_rcns', pop=True)\n",
    "    optns_piped = deep2hie(expfam_cfg)\n",
    "    n_epoch = expfam_cfg.pop('n_epoch', 2)\n",
    "    n_snrt = expfam_cfg.pop('n_snrt', 1000)\n",
    "    if isinstance(n_rcnsspec, int):\n",
    "        n_rcnsspec = {f'{splt}/{vrnt}': n_rcnsspec \n",
    "            for splt, vrnts in splt2vrnts.items() for vrnt in vrnts}\n",
    "\n",
    "    exprm_faminfos = dict()\n",
    "    for exp_idarch, fpidx in exp_id2fpidx.items():\n",
    "        expid, arch = exp_idarch.split('/')\n",
    "        exprmnt = f'{expfam}.{expid}'\n",
    "        namevars = {'expid': expid, 'expfam': expfam, 'arch': arch}\n",
    "        nicknm = eval_formula(nicknmfrmla, namevars)\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/fpidx'] = fpidx\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/splt2vrnts'] = splt2vrnts\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/resdir'] = resdir\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/n_seeds'] = n_seeds\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/n_epoch'] = n_epoch\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/n_snrt'] = n_snrt\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/nicknm'] = nicknm\n",
    "        exprm_faminfos[f'{exprmnt}:{arch}/figdir'] = figdir\n",
    "        for split, vrnts in splt2vrnts.items():\n",
    "            for vrnt in vrnts:\n",
    "                n_rcns = n_rcnsspec.get(f'{split}/{vrnt}', 1)\n",
    "                exprm_faminfos[f'{exprmnt}:{arch}/n_rcns/{split}/{vrnt}'] = n_rcns\n",
    "        for key, val in optns_piped.items():\n",
    "            exprm_faminfos[f'{exprmnt}:{arch}/{key}'] = val\n",
    "\n",
    "    i_fig = None\n",
    "    for fig_type, i_fig0 in i_figcfg.items():\n",
    "        i_fig = i_fig0 if (i_fig0 is not None) else i_fig\n",
    "        assert i_fig is not None, 'first one must be specified'\n",
    "        for exprm_id in list(hie2deep(exprm_faminfos, sep='/', maxdepth=1)):\n",
    "            (exprmnt, arch) = exprm_id.split(':')\n",
    "            exprm_faminfos[f'{exprmnt}:{arch}/i_fig/{fig_type}'] = i_fig\n",
    "            i_fig += 1\n",
    "        \n",
    "    exprm_infos.update(exprm_faminfos)\n",
    "\n",
    "exprm_infos = {key: val for key, val in exprm_infos.items() \n",
    "    if any(fnmatch.fnmatch(key, pat) for pat in \n",
    "        ['trad.*', 'cond.cont.acsmsmps.*', 'cond.cont.trilbl.*', \n",
    "        'cond.mqnt.indzy.nrmtrg.himb.frac.null*',\n",
    "        'cond.mqnt.indzy.nrmtrg.himb.frac.pow1*'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_datas = dict()\n",
    "\n",
    "for exprm_id, exprm_info in hie2deep(exprm_infos, maxdepth=1).items():\n",
    "    (exprmnt, arch) = exprm_id.split(':')\n",
    "    fpidx = exprm_info['fpidx']\n",
    "    splt2vrnts = exprm_info['splt2vrnts']\n",
    "    resdir = exprm_info['resdir']\n",
    "    n_epoch = exprm_info['n_epoch']\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_epoch = -1\n",
    "    \n",
    "    try:\n",
    "        rio = resio(fpidx=fpidx, resdir=resdir)\n",
    "    except Exception as exc:\n",
    "        print(f'There was an error opening {fpidx}. I will move on.')\n",
    "        continue\n",
    "    \n",
    "    for split, vrnts in splt2vrnts.items():\n",
    "        print(f'Loading the data for {exprmnt}:{arch}:{split}...')\n",
    "\n",
    "        ############## Collecting the plotting data ###############\n",
    "        ycols = ['m_chmprthst', 'n_prthst', 'ccn_cdf', 'qs_prt', 'qscs_prt', 'qs_pop',\n",
    "            'qa_prt', 'qacs_prt', 'qa_pop', 'frznfrac_tmp', 'logfrznfrac_tmp']\n",
    "\n",
    "        v_ydataraw = dict()\n",
    "        for ycol, vrnt in product(ycols, vrnts):\n",
    "            n_ychnls, n_ylen = ycol2dims[ycol]\n",
    "            n_rcns = exprm_info[f'n_rcns/{split}/{vrnt}']   \n",
    "            y_nparr = rio(f'var/eval/raw/yaero:x:{ycol}/{split}/{vrnt}/pnts/data')\n",
    "            assert y_nparr.shape == (n_epoch * n_seeds, n_snrt, n_rcns, n_ychnls, n_ylen)\n",
    "            y_nparr2 = y_nparr.reshape(n_epoch, n_seeds, n_snrt, n_rcns, n_ychnls, n_ylen)\n",
    "            assert y_nparr2.shape == (n_epoch, n_seeds, n_snrt, n_rcns, n_ychnls, n_ylen)\n",
    "            v_ydataraw[f'{ycol}/{vrnt}'] = y_nparr2[i_epoch]\n",
    "\n",
    "        ########### Data Cleaning and Unit Conversions ############\n",
    "        v_ydata, aero_cstscnv = cnvrt_physunits(v_ydataraw, aero_csts)\n",
    "        n_chem = aero_cstscnv['n_chem']\n",
    "        n_bins = aero_cstscnv['n_bins']\n",
    "        chem_species = aero_cstscnv['chem_species']\n",
    "        \n",
    "        ############### Selecting Examples to Show ################\n",
    "        if set(vrnts) == {'orig', 'rcnst'}:\n",
    "            n_clctn, n_prtrt = 1, 5\n",
    "            with torch.no_grad():\n",
    "                m_orig = torch.from_numpy(v_ydata['m_chmprthst/orig'])\n",
    "                assert m_orig.shape == (n_seeds, n_snrt, 1, n_chem, n_bins)\n",
    "                m_rcnst = torch.from_numpy(v_ydata['m_chmprthst/rcnst'])\n",
    "                assert m_rcnst.shape == (n_seeds, n_snrt, 1, n_chem, n_bins)\n",
    "                e_samps = get_relerr(m_orig.squeeze(1), m_rcnst.squeeze(1)\n",
    "                    ).ravel().detach().cpu().numpy()\n",
    "                assert e_samps.shape == (n_seeds * n_snrt,)\n",
    "                fltr_hih2o, m_fltrh2o = True, m_orig\n",
    "        elif set(vrnts) == {'orig', 'rcnst', 'znrm', 'yknn'}:\n",
    "            n_clctn, n_prtrt = 1, 20\n",
    "            with torch.no_grad():\n",
    "                m_orig = torch.from_numpy(v_ydata['m_chmprthst/orig'])\n",
    "                assert m_orig.shape == (n_seeds, n_snrt, 1, n_chem, n_bins)\n",
    "                e_samps = m_orig.sum(dim=[-1, -2, -3]).ravel().detach().cpu().numpy()\n",
    "                assert e_samps.shape == (n_seeds * n_snrt,)\n",
    "                fltr_hih2o, m_fltrh2o = True, m_orig\n",
    "        elif set(vrnts) == {'genr', 'genr0', 'genr1', 'genr2', 'genr3', \n",
    "            'gaus', 'gaus0', 'gaus1', 'gaus2', 'gaus3'}:\n",
    "            n_clctn, n_prtrt = 20, 5\n",
    "            with torch.no_grad():\n",
    "                m_gauss1 = np.stack([v_ydata[f'm_chmprthst/{vrnt}'] \n",
    "                    for vrnt in ['gaus0', 'gaus1', 'gaus2', 'gaus3']], axis=-1)\n",
    "                assert m_gauss1.shape == (n_seeds, n_snrt, 1, n_chem, n_bins, 4)\n",
    "                m_gauss2 = m_gauss1 / (1e-30 + m_gauss1.sum(axis=(-3, -2), keepdims=True))\n",
    "                assert m_gauss2.shape == (n_seeds, n_snrt, 1, n_chem, n_bins, 4)\n",
    "                m_gauss3 = m_gauss2.max(axis=-1)\n",
    "                assert m_gauss3.shape == (n_seeds, n_snrt, 1, n_chem, n_bins)\n",
    "                np_random = np.random.RandomState(12345)\n",
    "                e_samps = np_random.rand(n_seeds * n_snrt)\n",
    "                assert e_samps.shape == (n_seeds * n_snrt,)\n",
    "                fltr_hih2o, m_fltrh2o = True, m_gauss3\n",
    "        elif set(vrnts) == {'genr'}:\n",
    "            n_clctn, n_prtrt = 5, 4\n",
    "            with torch.no_grad():\n",
    "                m_genr = v_ydata['m_chmprthst/genr']\n",
    "                assert m_genr.shape == (n_seeds, n_snrt, 1, n_chem, n_bins)\n",
    "                i_oin = chem_species.index('Dust')\n",
    "                e_samps1 = m_genr[:, :, 0, i_oin, :].sum(axis=-1) / m_genr.sum(axis=(-3, -2, -1))\n",
    "                assert e_samps1.shape == (n_seeds, n_snrt)\n",
    "                e_samps = e_samps1.ravel()\n",
    "                assert e_samps.shape == (n_seeds * n_snrt,)\n",
    "                fltr_hih2o, m_fltrh2o = True, m_genr\n",
    "        else:\n",
    "            raise ValueError(f'case undefined: {vrnts}')\n",
    "\n",
    "        # Removing the samples with high water content\n",
    "        if fltr_hih2o:\n",
    "            i_water = chem_species.index('H2O')\n",
    "            assert m_fltrh2o.shape == (n_seeds, n_snrt, 1, n_chem, n_bins)\n",
    "            m_allchm = m_fltrh2o[:, :, 0, :, :].sum(axis=(-1, -2)).ravel()\n",
    "            assert m_allchm.shape == (n_seeds * n_snrt,)\n",
    "            m_water = m_fltrh2o[:, :, 0, i_water, :].sum(axis=-1).ravel()\n",
    "            assert m_water.shape == (n_seeds * n_snrt,)\n",
    "            m_waterfrac = m_water / m_allchm\n",
    "            assert m_waterfrac.shape == (n_seeds * n_snrt,)\n",
    "            e_samps[m_waterfrac > 0.1] = np.nan\n",
    "        \n",
    "        e_sampargsrt1 = np.argsort(e_samps)\n",
    "        assert e_sampargsrt1.shape == (n_seeds * n_snrt,)\n",
    "\n",
    "        # Filtering out the nan values in `e_samps`\n",
    "        i_nanesamp = np.where(np.isnan(e_samps))[0]\n",
    "        i_nanesampset = set(i_nanesamp)\n",
    "        e_sampargsrt2 = np.array([ii for ii in e_sampargsrt1 if ii not in i_nanesampset])\n",
    "        n_samps2 = n_seeds * n_snrt - len(i_nanesampset)\n",
    "        assert e_sampargsrt2.shape == (n_samps2,)\n",
    "\n",
    "        q_sel1 = np.linspace(0.1, 0.9, n_prtrt)\n",
    "        assert q_sel1.shape == (n_prtrt,)\n",
    "        q_sel = q_sel1[None, :] + 100 * np.arange(n_clctn)[:, None] / (n_samps2)\n",
    "        assert q_sel.shape == (n_clctn, n_prtrt)\n",
    "        iq_sel = (q_sel.ravel() * n_samps2).astype(int)\n",
    "        iq_sel = np.clip(iq_sel, 0, n_samps2 - 1)\n",
    "        assert iq_sel.shape == (n_clctn * n_prtrt,)\n",
    "        i_sel = e_sampargsrt2[iq_sel].reshape(n_clctn, n_prtrt)\n",
    "        assert i_sel.shape == (n_clctn, n_prtrt)\n",
    "\n",
    "        viz_datas[f'{exprmnt}:{arch}:{split}/v_ydata'] = v_ydata\n",
    "        viz_datas[f'{exprmnt}:{arch}:{split}/v_ydataraw'] = v_ydataraw\n",
    "        viz_datas[f'{exprmnt}:{arch}:{split}/q_sel'] = q_sel\n",
    "        viz_datas[f'{exprmnt}:{arch}:{split}/i_sel'] = i_sel\n",
    "        viz_datas[f'{exprmnt}:{arch}:{split}/e_samps'] = e_samps\n",
    "        viz_datas[f'{exprmnt}:{arch}:{split}/aero_csts'] = aero_cstscnv\n",
    "        for key, val in aero_cstscnv.items():\n",
    "            viz_datas[f'{exprmnt}:{arch}:{split}/{key}'] = val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Tri-Label Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Conditional Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_datasdeep = hie2deep(viz_datas, maxdepth=1)\n",
    "\n",
    "df_errslst = []\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    n_rcns = exprm_info['n_rcns/test/znrm']\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False, vrnt_trg='znrm')\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    ######## Framing the Error Metrics as a DataFrame #########\n",
    "    dfdict = dict(viz_id=viz_id, exprmnt=exprmnt, arch=arch, \n",
    "        split=split, epoch=-1, rng_seed=np.arange(n_seeds))\n",
    "\n",
    "    for ycol, y_samps1 in v_mtrcdata1.items():\n",
    "        if not ycol.endswith('/err'):\n",
    "            continue\n",
    "        \n",
    "        if y_samps1 is None:\n",
    "            y_samps2 = None\n",
    "        else:\n",
    "            assert y_samps1.shape == (n_seeds, n_snrt * n_rcns)\n",
    "            y_samps2 = y_samps1.mean(axis=-1)\n",
    "            assert y_samps2.shape == (n_seeds,)\n",
    "\n",
    "        dfdict[ycol] = y_samps2\n",
    "\n",
    "    df_vizid = pd.DataFrame(dfdict)\n",
    "    df_errslst.append(df_vizid)\n",
    "\n",
    "df_errs = pd.concat(df_errslst, axis=0, ignore_index=True)\n",
    "df_errs = df_errs.dropna(axis=1, how='all')\n",
    "ycols = [col for col in df_errs.columns if col.endswith('/err')]\n",
    "\n",
    "######## Bootstrap Aggregating the Results #########\n",
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[2.5, 97.5], stat='mean', device='cpu')\n",
    "hpcols = ['viz_id', 'exprmnt', 'arch', 'split']\n",
    "stcols = [col for col in df_errs.columns if col not in hpcols]\n",
    "agg_data = get_aggdf(df_errs[hpcols], df_errs[stcols], xcol='epoch', \n",
    "    huecol='viz_id', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "hpdf_agg, stdf_agg = agg_data['hpdf'], agg_data['stdf']\n",
    "df_agg = pd.concat([hpdf_agg, stdf_agg], axis=1)\n",
    "df_agg = df_agg.drop(columns=['exprmnt', 'arch', 'split', 'epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Creating a Formatted Table for Latex #########\n",
    "df_agglst2 = []\n",
    "for i_row, row_dict in df_agg.iterrows():\n",
    "    row_dict2 = dict(viz_id=row_dict['viz_id'])\n",
    "    for ycol in ycols:\n",
    "        y_mean = row_dict[f'{ycol}/mean'] * 100\n",
    "        y_low = row_dict[f'{ycol}/low'] * 100\n",
    "        y_high = row_dict[f'{ycol}/high'] * 100\n",
    "        y_meanstr = f'{y_mean:.2g}' if y_mean < 1 else f'{y_mean:.3g}'\n",
    "        y_lowstr = f'{y_low:.2g}' if y_low < 1 else f'{y_low:.3g}'\n",
    "        y_highstr = f'{y_high:.2g}' if y_high < 1 else f'{y_high:.3g}'\n",
    "        row_dict2[ycol] = f'${y_meanstr}\\% [{y_lowstr}\\%,{y_highstr}\\%]$'\n",
    "    df_agglst2.append(row_dict2)\n",
    "df_agg2 = pd.DataFrame(df_agglst2)\n",
    "\n",
    "# Transposing the table\n",
    "df_agg3 = df_agg2.set_index('viz_id').T\n",
    "df_agg3.columns.name = None\n",
    "\n",
    "err_rnmngs = {\n",
    "    'ccn_cdf/err': 'CCN Spectrum',\n",
    "    'logqa_pop/err': 'Vol Scat Coef',\n",
    "    'logqs_pop/err': 'Vol Abs Coef',\n",
    "    'logfrznfrac_tmp/err': 'Frozen Fraction',\n",
    "    'm_acsm/err': 'ACSM Readings',\n",
    "    'n_smps/err': 'SMPS Readings',\n",
    "    'n_prthst/err': 'Number Dist',\n",
    "    'm_prthst/err': 'Total Mass',\n",
    "    'm_chmprt/err': 'Species Mass'}\n",
    "\n",
    "col_rnmngs0 = {\n",
    "    'index': 'Gen Ambiguity',\n",
    "    'cond.cont.trilbl.depzy:mlp:test': 'HDM (Trad)',\n",
    "    'cond.cont.trilbl.indzy:mlp:test': 'HDM (Ours)',\n",
    "    'cond.cont.acsmsmps.duo.depzy:mlp:test': 'LDM (Trad)',\n",
    "    'cond.cont.acsmsmps.duo.indzy:mlp:test': 'LDM (Ours)'}\n",
    "\n",
    "col_rnmngs1 = {\n",
    "    'index': r'\\makecell[c]{Generative\\\\Ambiguity}',\n",
    "    'cond.cont.trilbl.indzy:mlp:test': r'\\makecell[c]{High-Dim Measurements\\\\Mean [95\\% CI]}',\n",
    "    'cond.cont.acsmsmps.duo.indzy:mlp:test': r'\\makecell[c]{Low-Dim Measurements\\\\Mean [95\\% CI]}'}\n",
    "\n",
    "col_rnmngs2 = {\n",
    "    'index': '\\makecell[c]{Generative\\\\Ambiguity}',\n",
    "    'cond.cont.trilbl.depzy:mlp:test': r'\\makecell[c]{Traditional CVAE\\\\Mean [95\\% CI]}',\n",
    "    'cond.cont.trilbl.indzy:mlp:test': r'\\makecell[c]{Wasserstein-Regularized CVAE (Ours)\\\\Mean [95\\% CI]}'}\n",
    "\n",
    "col_rnmngs3 = {\n",
    "    'index': '\\makecell[c]{Generative\\\\Ambiguity}',\n",
    "    'cond.cont.acsmsmps.duo.depzy:mlp:test': r'\\makecell[c]{Traditional CVAE\\\\Mean [95\\% CI]}',\n",
    "    'cond.cont.acsmsmps.duo.indzy:mlp:test': r'\\makecell[c]{Wasserstein-Regularized CVAE (Ours)\\\\Mean [95\\% CI]}'}\n",
    "\n",
    "\n",
    "df_agg4 = df_agg3.copy(deep=True)\n",
    "# Selecting a subset of the errors and Reordering them\n",
    "df_agg5 = df_agg4.loc[list(err_rnmngs)].reset_index()\n",
    "# Renaming the errors\n",
    "df_agg6 = df_agg5.replace(err_rnmngs)\n",
    "# Selecting a subset of the columns and Reordering them and renaming the columns\n",
    "df_agg7 = df_agg6.loc[:, list(col_rnmngs0)].rename(columns=col_rnmngs0)\n",
    "df_agg8 = df_agg6.loc[:, list(col_rnmngs1)].rename(columns=col_rnmngs1)\n",
    "df_agg9 = df_agg6.loc[:, list(col_rnmngs2)].rename(columns=col_rnmngs2)\n",
    "df_agg10 = df_agg6.loc[:, list(col_rnmngs3)].rename(columns=col_rnmngs3)\n",
    "\n",
    "df_agg10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_tex = df_agg10.to_latex(index=False)\n",
    "tbl_tex = tbl_tex.replace(r'\\begin{tabular}{lllll}', \n",
    "    r'\\begin{tabular}{' + r'|p{0.17\\textwidth}' * 5 + r'|}')\n",
    "tbl_tex = tbl_tex.replace(r'\\toprule', r'\\hline')\n",
    "tbl_tex = tbl_tex.replace(r'\\midrule', r'\\hline')\n",
    "tbl_tex = tbl_tex.replace(r'\\bottomrule', r'\\hline')\n",
    "tbl_tex = tbl_tex.replace(r'$ \\\\', r'$ \\\\\\hline')\n",
    "print(tbl_tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Mass and Number Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.trilbl')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/xorig']\n",
    "\n",
    "    n_figrows, n_figcols = 1, 2\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "\n",
    "            # The speciated mass data\n",
    "            i_row, i_col = 0, 0\n",
    "            n_rcns, i_rcns = 1, 0\n",
    "            v_ynparr = v_ydata['m_chmprthst/orig']\n",
    "            assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "            v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "            assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "            ax_idx = i_row * n_figcols + i_col\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'm_chmprthst.cndinp/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                v_mpldatas[f'm_chmprthst.cndinp/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([1, d_histbinsum, 'n_prthst'])\n",
    "            \n",
    "            for i_row in range(n_figrows):\n",
    "                for i_col, xvals, ycol in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    n_rcns, i_rcns = 1, 0\n",
    "                    v_ynparr = v_ydata[f'{ycol}/orig']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndinp/{ax_idx}:orig:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndinp/{ax_idx}:orig:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        tag_text = f'({\"ab\"[i_figcol]})'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.9))\n",
    "                # for i_figcol, ax in enumerate(axes[0]):\n",
    "                #     print_axheader(ax, f'Input Label {i_figcol+1}', 'top', fontsize=14, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/label'] = fig\n",
    "\n",
    "        os.makedirs(figdir, exist_ok=True)\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_xanec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Input Label Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    n_chem = viz_data['n_chem']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.trilbl.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/label']\n",
    "\n",
    "    n_figrows, n_figcols = 1, 3\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            i_chems = np.arange(n_chem + 1)\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([0, d_histbinsum, 'n_prthst'])\n",
    "            ax_ixyspec.append([1, d_histbinsum, 'm_prthst'])\n",
    "            ax_ixyspec.append([2, i_chems,      'm_chmprt'])\n",
    "            \n",
    "            for i_row in range(n_figrows):\n",
    "                for i_col, xvals, ycol in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    n_rcns, i_rcns = 1, 0\n",
    "                    v_ynparr = v_ydata[f'{ycol}/orig']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndlbl/{ax_idx}:orig:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndlbl/{ax_idx}:orig:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        tag_text = f'({\"cde\"[i_figcol]})'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.9))\n",
    "                for i_figcol, ax in enumerate(axes[0]):\n",
    "                    print_axheader(ax, f'Measurement {i_figcol+1}', 'top', fontsize=14, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/label'] = fig\n",
    "\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_yanec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=f, pig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Generated Sample and Label Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    d_binssmpsum = aero_cstscnv['d_binssmps']\n",
    "    acsm_species = aero_cstscnv['acsm_species']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.trilbl.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/gendiag']\n",
    "\n",
    "    # vrnt_specs = exprm_info.get('vrnt_specs', None)\n",
    "    vrnt_specs = [\n",
    "        ('orig', 0, 'Original'), ('znrm', 0, 'Norm Lat'), \n",
    "        ('znrm', 1, 'Norm Lat'), ('znrm', 2, 'Norm Lat'), \n",
    "        ('znrm', 3, 'Norm Lat'), ('znrm', 4, 'Norm Lat')][1:]\n",
    "    n_figrows, n_figcols = 4, len(vrnt_specs)\n",
    "    n_figrows = v_mplcfgs['m_chmprthst.cndgen']['plt.subplots/nrows']\n",
    "    v_mplcfgs['m_chmprthst.cndgen']['plt.subplots/ncols'] = n_figcols\n",
    "    assert len(vrnt_specs) == n_figcols\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.cndgen']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.cndgen']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "            \n",
    "            # The speciated mass data\n",
    "            i_row = 0\n",
    "            for i_col in range(n_figcols):\n",
    "                vrnt, i_rcns, ttl = vrnt_specs[i_col]\n",
    "                n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                ax_idx = i_row * n_figcols + i_col\n",
    "                for i_chem, chem in enumerate(chem_species):\n",
    "                    v_mpldatas[f'm_chmprthst.cndgen/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                    v_mpldatas[f'm_chmprthst.cndgen/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            i_chems = np.arange(n_chem + 1)\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([1, d_histbinsum, 'n_prthst'     , True ])\n",
    "            ax_ixyspec.append([2, d_histbinsum, 'm_prthst'     , True ])\n",
    "            ax_ixyspec.append([3, i_chems,      'm_chmprt'     , True ])\n",
    "            # ax_ixyspec.append([4, eps_histbins, 'ccn_cdf'     , False])\n",
    "            # ax_ixyspec.append([5, temprtr_bins, 'frznfrac_tmp', False])\n",
    "            # ax_ixyspec.append([6, len_wvum,     'qs_pop'      , False])\n",
    "            # ax_ixyspec.append([7, len_wvum,     'qa_pop'      , False])\n",
    "            \n",
    "            for i_col in range(n_figcols):\n",
    "                for i_row, xvals, ycol, plot_orig in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    vrnt, i_rcns, ttl = vrnt_specs[i_col]\n",
    "                    n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                    v_ynparr = v_ydata[f'{ycol}/{vrnt}']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:rcnst:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:rcnst:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "\n",
    "                    if not plot_orig: continue\n",
    "                    n_rcns, i_rcns = 1, 0\n",
    "                    v_ynparr = v_ydata[f'{ycol}/orig']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:orig:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:orig:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        tag_text = f'({\"fghi\"[i_figrow]}$_{{{i_figcol + 1}}}$)'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "                for i_figcol, ax in enumerate(axes[0]):\n",
    "                    txthdr = f'Sample {i_figcol + 1}'\n",
    "                    print_axheader(ax, txthdr, 'top', fontsize=14, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/diag'] = fig\n",
    "\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Aerosol Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "show_mchmprt = False\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    d_binssmpsum = aero_cstscnv['d_binssmps']\n",
    "    acsm_species = aero_cstscnv['acsm_species']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.trilbl.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/vardiag']\n",
    "    \n",
    "    n_figrows, n_figcols = 1 + show_mchmprt, 4\n",
    "    for ycol in ['m_chmprthst', 'n_prthst', 'ccn_cdf', 'qs_pop', 'qa_pop', 'frznfrac_tmp']:\n",
    "        v_mplcfgs[f'{ycol}.cndvar']['plt.subplots/nrows'] = n_figrows\n",
    "        v_mplcfgs[f'{ycol}.cndvar']['plt.subplots/ncols'] = n_figcols\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.cndvar']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.cndvar']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "\n",
    "            if show_mchmprt:\n",
    "                # The speciated mass data\n",
    "                i_row, vrnt = 0, 'znrm'\n",
    "                n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                for i_col in range(n_figcols):\n",
    "                    v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                    assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    ax_idx, i_rcns = i_row * n_figcols + i_col, i_col\n",
    "                    for i_chem, chem in enumerate(chem_species):\n",
    "                        v_mpldatas[f'm_chmprthst.cndvar/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                        v_mpldatas[f'm_chmprthst.cndvar/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            i_acsmchems = np.arange(n_chemacsm + 1)\n",
    "            ax_ixyspec = []\n",
    "            # ax_ixyspec.append([0, d_histbinsum, 'n_prthst'    , False])\n",
    "            ax_ixyspec.append([0, eps_histbins, 'ccn_cdf'     , False])\n",
    "            ax_ixyspec.append([1, len_wvum,     'qs_pop'      , False])\n",
    "            ax_ixyspec.append([2, len_wvum,     'qa_pop'      , False])\n",
    "            ax_ixyspec.append([3, temprtr_bins, 'frznfrac_tmp', False])\n",
    "            \n",
    "            i_row = 1 if show_mchmprt else 0\n",
    "            for i_col, xvals, ycol, plot_orig in ax_ixyspec:\n",
    "                ax_idx = i_row * n_figcols + i_col\n",
    "                for vrnt, plot_ci in [('znrm', True), ('orig', False)]:\n",
    "                    v_ynparr = v_ydata[f'{ycol}/{vrnt}']\n",
    "                    n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    d_ydata = math.prod(v_ynparr2.shape[2:])\n",
    "                    v_ynparr3 = v_ynparr2.reshape(n_seeds * n_snrt, n_rcns, d_ydata)\n",
    "                    assert v_ynparr3.shape == (n_seeds * n_snrt, n_rcns, d_ydata)\n",
    "                    v_ynparr4 = v_ynparr3[i_samp]\n",
    "                    assert v_ynparr4.shape == (n_rcns, d_ydata)\n",
    "                    v_ymean = np.median(v_ynparr4, axis=0)\n",
    "                    assert v_ymean.shape == (d_ydata,)\n",
    "                    v_ylow = np.quantile(v_ynparr4, q=0.0, axis=0)\n",
    "                    assert v_ylow.shape == (d_ydata,)\n",
    "                    v_yhigh = np.quantile(v_ynparr4, q=1.0, axis=0)\n",
    "                    assert v_yhigh.shape == (d_ydata,)\n",
    "                    v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:x'] = xvals\n",
    "                    if plot_ci:\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y/mean'] = v_ymean\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y/low'] = v_ylow\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y/high'] = v_yhigh\n",
    "                    else:\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y'] = v_ymean\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        if show_mchmprt and (i_figrow == 0):\n",
    "                            tag_text = f'(a$_{{{i_figcol + 1}}}$)'\n",
    "                        elif show_mchmprt and (i_figrow > 0):\n",
    "                            tag_text = f'({\"bcdef\"[i_figcol]})'\n",
    "                        else:\n",
    "                            tag_text = f'({\"jklmn\"[i_figcol]})'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "                if show_mchmprt:\n",
    "                    for i_figrow, ax in enumerate(axes[:, 0]):\n",
    "                        txthdr = ('Speciated Mass', 'Diagnostics')[i_figrow] \n",
    "                        print_axheader(ax, txthdr, 'left', fontsize=14, pad=10, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/vardiag'] = fig\n",
    "    \n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Collective Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "colorspec = v_mplcfgs['colorspec']\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.trilbl.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/diagsmry']\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False, vrnt_trg='znrm')\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    # Assembling the plotting data\n",
    "    v_mpldatas1 = dict()\n",
    "    v_mpldatas1['nprthst_errhist/0:n_smpserr:y'] = v_mtrcdata1['n_prthst/err'].ravel()\n",
    "    v_mpldatas1['mprthst_errhist/1:m_acsmerr:y'] = v_mtrcdata1['m_prthst/err'].ravel()\n",
    "    v_mpldatas1['mchmprt_errhist/2:m_acsmerr:y'] = v_mtrcdata1['m_chmprt/err'].ravel()\n",
    "    for ax_idx, ax_id, i_eps in [(3, 'ccn_sctcnd1', i_eps1), (6, 'ccn_sctcnd3', i_eps3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:x'] = v_mtrcdata1['ccn_cdf/orig'][..., i_eps].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:y'] = v_mtrcdata1['ccn_cdf/znrm'][..., i_eps].ravel()\n",
    "    v_mpldatas1['qs_popsctcnd/4:qs_pop:x'] = v_mtrcdata1['qs_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qs_popsctcnd/4:qs_pop:y'] = v_mtrcdata1['qs_pop/znrm'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcnd/7:qa_pop:x'] = v_mtrcdata1['qa_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcnd/7:qa_pop:y'] = v_mtrcdata1['qa_pop/znrm'][:, :, i_wvl1, :].ravel()\n",
    "    for ax_idx, ax_id, i_tmp in [(5, 'frznfrac_sctcnd1', i_tmp1), (8, 'frznfrac_sctcnd3', i_tmp3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:x'] = v_mtrcdata1['frznfrac_tmp/orig'][..., i_tmp].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:y'] = v_mtrcdata1['frznfrac_tmp/znrm'][..., i_tmp].ravel()\n",
    "\n",
    "    # Making the matplotlib calls\n",
    "    v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatas2):\n",
    "        fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, axes=axes, \n",
    "            mplopts=v_mplcfgs[ax_id])\n",
    "    axes1d = axes.ravel()\n",
    "\n",
    "    # Adding the text boxes\n",
    "    for ax_idx, textstr in [\n",
    "        (3,  f'$s={100*eps1:0.2g}\\%$'), \n",
    "        (6,  f'$s={100*eps3:0.2g}\\%$'), \n",
    "        (4,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (7,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (5,  f'$T={{\\\\rm {tmp1:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (8,  f'$T={{\\\\rm {tmp3:.0f}^{{\\circ}}\\\\ C}}$')]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', usetex=True, bbox={'facecolor': 'none', \n",
    "            'edgecolor': 'none'})\n",
    "\n",
    "    # Adding the x=y line to scatter plots\n",
    "    for ax_idx in range(3, 9):\n",
    "        ax = axes1d[ax_idx]\n",
    "        x_lo = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "        x_hi = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "        x_id = np.linspace(x_lo, x_hi, 100)\n",
    "        ax.plot(x_id, x_id, lw=1, ls='--', color=colorspec['blue'])\n",
    "    \n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                ax_idx = i_figrow * axes.shape[1] + i_figcol\n",
    "                tag_text = f'({\"abcdefghi\"[ax_idx]})'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "    \n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_smry.pdf'\n",
    "    pngpath = pdfpath[:-4] + '.png'\n",
    "    fig.savefig(pngpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pngpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Diagnostic Calibration Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "colorspec = v_mplcfgs['colorspec']\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.trilbl.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/diagsmryqnt']\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False, vrnt_trg='znrm')\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    # Assembling the plotting data\n",
    "    v_mpldatas1 = dict()\n",
    "    # v_mpldatas1['nsmps_errhist/0:n_smpserr:y'] = v_mtrcdata1['n_smps/err'].ravel()\n",
    "    # v_mpldatas1['macsm_errhist/1:m_acsmerr:y'] = v_mtrcdata1['m_acsm/err'].ravel()\n",
    "    for ax_idx, ax_id, i_eps in [(0, 'ccn_sctcndqnt1', i_eps1), (3, 'ccn_sctcndqnt3', i_eps3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:x'] = v_mtrcdata1['ccn_cdf/origraw'][..., i_eps].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:y'] = v_mtrcdata1['ccn_cdf/origcdf'][..., i_eps].ravel()\n",
    "    v_mpldatas1['qs_popsctcndqnt/1:qs_pop:x'] = v_mtrcdata1['qs_pop/origraw'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qs_popsctcndqnt/1:qs_pop:y'] = v_mtrcdata1['qs_pop/origcdf'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcndqnt/4:qa_pop:x'] = v_mtrcdata1['qa_pop/origraw'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcndqnt/4:qa_pop:y'] = v_mtrcdata1['qa_pop/origcdf'][:, :, i_wvl1, :].ravel()\n",
    "    for ax_idx, ax_id, i_tmp in [(2, 'frznfrac_sctcndqnt1', i_tmp1), (5, 'frznfrac_sctcndqnt3', i_tmp3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:x'] = v_mtrcdata1['frznfrac_tmp/origraw'][..., i_tmp].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:y'] = v_mtrcdata1['frznfrac_tmp/origcdf'][..., i_tmp].ravel()\n",
    "\n",
    "    # Making the matplotlib calls\n",
    "    v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatas2):\n",
    "        fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, axes=axes, \n",
    "            mplopts=v_mplcfgs[ax_id])\n",
    "    axes1d = axes.ravel()\n",
    "\n",
    "    # Adding the text boxes\n",
    "    for ax_idx, textstr in [\n",
    "        (0,  f'$s={100*eps1:0.2g}\\%$'), \n",
    "        (3,  f'$s={100*eps3:0.2g}\\%$'), \n",
    "        (1,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (4,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (2,  f'$T={{\\\\rm {tmp1:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (5,  f'$T={{\\\\rm {tmp3:.0f}^{{\\circ}}\\\\ C}}$')]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', usetex=True, bbox={'facecolor': 'none', \n",
    "            'edgecolor': 'none'})\n",
    "    \n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                ax_idx = i_figrow * axes.shape[1] + i_figcol\n",
    "                tag_text = f'({\"abc\"[i_figcol]}$_{{{i_figrow+1}}}$)'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "        \n",
    "        for i_figcol, col_ttl in enumerate(['Cloud Condensation', 'Optical Properties', 'Ice Nucleation']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', pad=8, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_smry.pdf'\n",
    "    pngpath = pdfpath[:-4] + '.png'\n",
    "    fig.savefig(pngpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pngpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Conditional and Conditional Average Error Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_datasdeep = hie2deep(viz_datas, maxdepth=1)\n",
    "\n",
    "df_errslst = []\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if not((fnmatch.fnmatch(exprmnt, 'trad.*') or \n",
    "            fnmatch.fnmatch(exprmnt, 'cond.cont.*')) and (split in ('test',))):\n",
    "        continue\n",
    "\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    if exprmnt.startswith('trad.'):\n",
    "        vrnt_trg = 'rcnst'\n",
    "    elif exprmnt.startswith('cond.cont.'):\n",
    "        vrnt_trg = 'znrm'\n",
    "    else:\n",
    "        vrnt_trg = None\n",
    "    n_rcns = exprm_info[f'n_rcns/{split}/{vrnt_trg}']\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=True, vrnt_trg=vrnt_trg)\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    ######## Framing the Error Metrics as a DataFrame #########\n",
    "    dfdict = dict(viz_id=viz_id, exprmnt=exprmnt, arch=arch, \n",
    "        split=split, epoch=-1, rng_seed=np.arange(n_seeds))\n",
    "    dfdict.update({ycol: y_samps for ycol, y_samps in v_mtrcdata1.items() \n",
    "        if ycol.endswith('/err')})\n",
    "    df_vizid = pd.DataFrame(dfdict)\n",
    "    df_errslst.append(df_vizid)\n",
    "\n",
    "df_errs = pd.concat(df_errslst, axis=0, ignore_index=True)\n",
    "df_errs = df_errs.dropna(axis=1, how='all')\n",
    "ycols = [col for col in df_errs.columns if col.endswith('/err')]\n",
    "\n",
    "######## Bootstrap Aggregating the Results #########\n",
    "df_agglst = []\n",
    "for viz_id, df_errsvid in df_errs.groupby('viz_id', sort=False, observed=True):\n",
    "    aggcfg = dict(type='bootstrap', n_boot=40, q=[2.5, 97.5], stat='mean', device='cpu')\n",
    "    hpcols = ['viz_id', 'exprmnt', 'arch', 'split']\n",
    "    stcols = [col for col in df_errsvid.columns if col not in hpcols]\n",
    "    agg_data = get_aggdf(df_errsvid[hpcols], df_errsvid[stcols], xcol='epoch', \n",
    "        huecol='viz_id', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_agg, stdf_agg = agg_data['hpdf'], agg_data['stdf']\n",
    "    df_aggvid = pd.concat([hpdf_agg, stdf_agg], axis=1)\n",
    "    df_aggvid = df_aggvid.drop(columns=['exprmnt', 'arch', 'split', 'epoch'])\n",
    "    df_agglst.append(df_aggvid)\n",
    "\n",
    "df_agg = pd.concat(df_agglst, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Creating a Formatted Table for Latex #########\n",
    "df_agglst2 = []\n",
    "for i_row, row_dict in df_agg.iterrows():\n",
    "    row_dict2 = dict(viz_id=row_dict['viz_id'])\n",
    "    for ycol in ycols:\n",
    "        y_mean = row_dict[f'{ycol}/mean'] * 100\n",
    "        y_low = row_dict[f'{ycol}/low'] * 100\n",
    "        y_high = row_dict[f'{ycol}/high'] * 100\n",
    "        y_meanstr = f'{y_mean:.2g}' if y_mean < 1 else f'{y_mean:.3g}'\n",
    "        y_lowstr = f'{y_low:.2g}' if y_low < 1 else f'{y_low:.3g}'\n",
    "        y_highstr = f'{y_high:.2g}' if y_high < 1 else f'{y_high:.3g}'\n",
    "        row_dict2[ycol] = f'${y_meanstr}\\% [{y_lowstr}\\%,{y_highstr}\\%]$'\n",
    "    df_agglst2.append(row_dict2)\n",
    "df_agg2 = pd.DataFrame(df_agglst2)\n",
    "\n",
    "# Transposing the table\n",
    "df_agg3 = df_agg2.set_index('viz_id').T\n",
    "df_agg3.columns.name = None\n",
    "\n",
    "err_rnmngs = {\n",
    "    'ccn_cdf/err': 'CCN Spectrum',\n",
    "    'logqs_pop/err': 'Vol Scat Coef',\n",
    "    'logqa_pop/err': 'Vol Abs Coef',\n",
    "    'logfrznfrac_tmp/err': 'Frozen Fraction',\n",
    "    'm_acsm/err': 'ACSM Readings',\n",
    "    'n_smps/err': 'SMPS Readings',\n",
    "    'n_prthst/err': 'Number Dist',\n",
    "    'm_prthst/err': 'Total Mass',\n",
    "    'm_chmprt/err': 'Species Mass',\n",
    "    'm_chmprthst/err': 'Speciated Mass'}\n",
    "\n",
    "col_rnmngs1 = {\n",
    "    'index': 'Reconst Error',\n",
    "    'trad.gen:mlp:test': 'MLP',\n",
    "    'trad.gen:cnn:test': 'CNN'}\n",
    "\n",
    "col_rnmngs2 = {\n",
    "    'index': 'Gen Ambiguity',\n",
    "    'cond.cont.trilbl.depzy:mlp:test': 'HDM (Trad)',\n",
    "    'cond.cont.trilbl.indzy:mlp:test': 'HDM (Ours)',\n",
    "    'cond.cont.acsmsmps.duo.depzy:mlp:test': 'LDM (Trad)',\n",
    "    'cond.cont.acsmsmps.duo.indzy:mlp:test': 'LDM (Ours)'}\n",
    "\n",
    "col_rnmngs3 = {\n",
    "    'index': r'\\makecell[c]{Generative\\\\Ambiguity}',\n",
    "    'cond.cont.trilbl.indzy:mlp:test': r'\\makecell[c]{High-Dim Measurements\\\\Mean [95\\% CI]}',\n",
    "    'cond.cont.acsmsmps.duo.indzy:mlp:test': r'\\makecell[c]{Low-Dim Measurements\\\\Mean [95\\% CI]}'}\n",
    "\n",
    "col_rnmngs4 = {\n",
    "    'index': '\\makecell[c]{Generative\\\\Ambiguity}',\n",
    "    'cond.cont.trilbl.depzy:mlp:test': r'\\makecell[c]{Traditional CVAE\\\\Mean [95\\% CI]}',\n",
    "    'cond.cont.trilbl.indzy:mlp:test': r'\\makecell[c]{Wasserstein-Regularized CVAE (Ours)\\\\Mean [95\\% CI]}'}\n",
    "\n",
    "col_rnmngs5 = {\n",
    "    'index': '\\makecell[c]{Generative\\\\Ambiguity}',\n",
    "    'cond.cont.acsmsmps.duo.depzy:mlp:test': r'\\makecell[c]{Traditional CVAE\\\\Mean [95\\% CI]}',\n",
    "    'cond.cont.acsmsmps.duo.indzy:mlp:test': r'\\makecell[c]{Wasserstein-Regularized CVAE (Ours)\\\\Mean [95\\% CI]}'}\n",
    "\n",
    "\n",
    "df_agg4 = df_agg3.copy(deep=True)\n",
    "# Selecting a subset of the errors and Reordering them\n",
    "df_agg5 = df_agg4.loc[list(err_rnmngs)].reset_index()\n",
    "# Renaming the errors\n",
    "df_agg6 = df_agg5.replace(err_rnmngs)\n",
    "# Selecting a subset of the columns and Reordering them and renaming the columns\n",
    "df_agg7 = df_agg6.loc[:, list(col_rnmngs1)].rename(columns=col_rnmngs1)\n",
    "df_agg8 = df_agg6.loc[:, list(col_rnmngs2)].rename(columns=col_rnmngs2)\n",
    "df_agg9 = df_agg6.loc[:, list(col_rnmngs3)].rename(columns=col_rnmngs3)\n",
    "df_agg10 = df_agg6.loc[:, list(col_rnmngs4)].rename(columns=col_rnmngs4)\n",
    "df_agg11 = df_agg6.loc[:, list(col_rnmngs5)].rename(columns=col_rnmngs5)\n",
    "\n",
    "df_agg7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_tex = df_agg7.to_latex(index=False)\n",
    "tbl_tex = tbl_tex.replace(r'\\begin{tabular}{lllll}', \n",
    "    r'\\begin{tabular}{' + r'|p{0.17\\textwidth}' * 5 + r'|}')\n",
    "tbl_tex = tbl_tex.replace(r'\\toprule', r'\\hline')\n",
    "tbl_tex = tbl_tex.replace(r'\\midrule', r'\\hline')\n",
    "tbl_tex = tbl_tex.replace(r'\\bottomrule', r'\\hline')\n",
    "tbl_tex = tbl_tex.replace(r'$ \\\\', r'$ \\\\\\hline')\n",
    "print(tbl_tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional ACSM/SMPS Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Mass and Number Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    d_binssmpsum = aero_cstscnv['d_binssmps']\n",
    "    acsm_species = aero_cstscnv['acsm_species']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.acsmsmps.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/xorig']\n",
    "\n",
    "    n_figrows, n_figcols = 1, 2\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "\n",
    "            # The speciated mass data\n",
    "            i_row, i_col = 0, 0\n",
    "            n_rcns, i_rcns = 1, 0\n",
    "            v_ynparr = v_ydata['m_chmprthst/orig']\n",
    "            assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "            v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "            assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "            ax_idx = i_row * n_figcols + i_col\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'm_chmprthst.cndinp/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                v_mpldatas[f'm_chmprthst.cndinp/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([1, d_histbinsum, 'n_prthst'])\n",
    "            \n",
    "            for i_row in range(n_figrows):\n",
    "                for i_col, xvals, ycol in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    n_rcns, i_rcns = 1, 0\n",
    "                    v_ynparr = v_ydata[f'{ycol}/orig']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndinp/{ax_idx}:orig:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndinp/{ax_idx}:orig:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        tag_text = f'({\"ab\"[i_figcol]})'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.9))\n",
    "                # for i_figcol, ax in enumerate(axes[0]):\n",
    "                #     print_axheader(ax, f'Input Label {i_figcol+1}', 'top', fontsize=14, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/label'] = fig\n",
    "\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_xanec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Input Label Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    d_binssmpsum = aero_cstscnv['d_binssmps']\n",
    "    acsm_species = aero_cstscnv['acsm_species']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.acsmsmps.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/label']\n",
    "\n",
    "    n_figrows, n_figcols = 1, 2\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            i_acsmchems = np.arange(n_chemacsm + 1)\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([0, d_binssmpsum, 'n_smps'])\n",
    "            ax_ixyspec.append([1, i_acsmchems,  'm_acsm'])\n",
    "            \n",
    "            for i_row in range(n_figrows):\n",
    "                for i_col, xvals, ycol in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    n_rcns, i_rcns = 1, 0\n",
    "                    v_ynparr = v_ydata[f'{ycol}/orig']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndlbl/{ax_idx}:orig:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndlbl/{ax_idx}:orig:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        tag_text = f'({\"cd\"[i_figcol]})'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.9))\n",
    "                for i_figcol, ax in enumerate(axes[0]):\n",
    "                    print_axheader(ax, f'Measurement {i_figcol+1}', 'top', fontsize=14, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/label'] = fig\n",
    "\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_yanec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Generated Sample and Label Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    d_binssmpsum = aero_cstscnv['d_binssmps']\n",
    "    acsm_species = aero_cstscnv['acsm_species']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.acsmsmps.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/gendiag']\n",
    "\n",
    "    # vrnt_specs = exprm_info.get('vrnt_specs', None)\n",
    "    vrnt_specs = [\n",
    "        ('orig', 0, 'Original'), ('znrm', 0, 'Norm Lat'), \n",
    "        ('znrm', 1, 'Norm Lat'), ('znrm', 2, 'Norm Lat'), \n",
    "        ('znrm', 3, 'Norm Lat'), ('znrm', 4, 'Norm Lat')][1:]\n",
    "    n_figrows, n_figcols = 4, len(vrnt_specs)\n",
    "    n_figrows = v_mplcfgs['m_chmprthst.cndgen']['plt.subplots/nrows']\n",
    "    v_mplcfgs['m_chmprthst.cndgen']['plt.subplots/ncols'] = n_figcols\n",
    "    assert len(vrnt_specs) == n_figcols\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.cndgen']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.cndgen']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "            \n",
    "            # The speciated mass data\n",
    "            i_row = 0\n",
    "            for i_col in range(n_figcols):\n",
    "                vrnt, i_rcns, ttl = vrnt_specs[i_col]\n",
    "                n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                ax_idx = i_row * n_figcols + i_col\n",
    "                for i_chem, chem in enumerate(chem_species):\n",
    "                    v_mpldatas[f'm_chmprthst.cndgen/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                    v_mpldatas[f'm_chmprthst.cndgen/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            i_acsmchems = np.arange(n_chemacsm + 1)\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([1, d_histbinsum, 'n_prthst'    , False])\n",
    "            ax_ixyspec.append([2, d_binssmpsum, 'n_smps'      , True ])\n",
    "            ax_ixyspec.append([3, i_acsmchems,  'm_acsm'      , True ])\n",
    "            # ax_ixyspec.append([4, eps_histbins, 'ccn_cdf'     , False])\n",
    "            # ax_ixyspec.append([5, temprtr_bins, 'frznfrac_tmp', False])\n",
    "            # ax_ixyspec.append([6, len_wvum,     'qs_pop'      , False])\n",
    "            # ax_ixyspec.append([7, len_wvum,     'qa_pop'      , False])\n",
    "            \n",
    "            for i_col in range(n_figcols):\n",
    "                for i_row, xvals, ycol, plot_orig in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    vrnt, i_rcns, ttl = vrnt_specs[i_col]\n",
    "                    n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                    v_ynparr = v_ydata[f'{ycol}/{vrnt}']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:rcnst:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:rcnst:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "\n",
    "                    if not plot_orig: continue\n",
    "                    n_rcns, i_rcns = 1, 0\n",
    "                    v_ynparr = v_ydata[f'{ycol}/orig']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:orig:x'] = xvals\n",
    "                    v_mpldatas[f'{ycol}.cndgen/{ax_idx}:orig:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        tag_text = f'({\"fghi\"[i_figrow]}$_{{{i_figcol + 1}}}$)'\n",
    "                        # tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "                for i_figcol, ax in enumerate(axes[0]):\n",
    "                    txthdr = f'Sample {i_figcol + 1}'\n",
    "                    print_axheader(ax, txthdr, 'top', fontsize=14, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/diag'] = fig\n",
    "    \n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotal Aerosol Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "show_mchmprt = False\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "\n",
    "    n_binssmps = aero_cstscnv['n_binssmps']\n",
    "    d_binssmpsum = aero_cstscnv['d_binssmps']\n",
    "    acsm_species = aero_cstscnv['acsm_species']\n",
    "    n_chemacsm = aero_cstscnv['n_chemacsm']\n",
    "\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.acsmsmps.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/vardiag']\n",
    "    \n",
    "    n_figrows, n_figcols = 1 + show_mchmprt, 4\n",
    "    for ycol in ['m_chmprthst', 'n_prthst', 'ccn_cdf', 'qs_pop', 'qa_pop', 'frznfrac_tmp']:\n",
    "        v_mplcfgs[f'{ycol}.cndvar']['plt.subplots/nrows'] = n_figrows\n",
    "        v_mplcfgs[f'{ycol}.cndvar']['plt.subplots/ncols'] = n_figcols\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.cndvar']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.cndvar']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "\n",
    "            if show_mchmprt:\n",
    "                # The speciated mass data\n",
    "                i_row, vrnt = 0, 'znrm'\n",
    "                n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                for i_col in range(n_figcols):\n",
    "                    v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                    assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    ax_idx, i_rcns = i_row * n_figcols + i_col, i_col\n",
    "                    for i_chem, chem in enumerate(chem_species):\n",
    "                        v_mpldatas[f'm_chmprthst.cndvar/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                        v_mpldatas[f'm_chmprthst.cndvar/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "\n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            i_acsmchems = np.arange(n_chemacsm + 1)\n",
    "            ax_ixyspec = []\n",
    "            # ax_ixyspec.append([0, d_histbinsum, 'n_prthst'    , False])\n",
    "            ax_ixyspec.append([0, eps_histbins, 'ccn_cdf'     , False])\n",
    "            ax_ixyspec.append([1, len_wvum,     'qs_pop'      , False])\n",
    "            ax_ixyspec.append([2, len_wvum,     'qa_pop'      , False])\n",
    "            ax_ixyspec.append([3, temprtr_bins, 'frznfrac_tmp', False])\n",
    "            \n",
    "            i_row = 1 if show_mchmprt else 0\n",
    "            for i_col, xvals, ycol, plot_orig in ax_ixyspec:\n",
    "                ax_idx = i_row * n_figcols + i_col\n",
    "                for vrnt, plot_ci in [('znrm', True), ('orig', False)]:\n",
    "                    v_ynparr = v_ydata[f'{ycol}/{vrnt}']\n",
    "                    n_rcns = exprm_info[f'n_rcns/test/{vrnt}']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    d_ydata = math.prod(v_ynparr2.shape[2:])\n",
    "                    v_ynparr3 = v_ynparr2.reshape(n_seeds * n_snrt, n_rcns, d_ydata)\n",
    "                    assert v_ynparr3.shape == (n_seeds * n_snrt, n_rcns, d_ydata)\n",
    "                    v_ynparr4 = v_ynparr3[i_samp]\n",
    "                    assert v_ynparr4.shape == (n_rcns, d_ydata)\n",
    "                    v_ymean = np.median(v_ynparr4, axis=0)\n",
    "                    assert v_ymean.shape == (d_ydata,)\n",
    "                    v_ylow = np.quantile(v_ynparr4, q=0.0, axis=0)\n",
    "                    assert v_ylow.shape == (d_ydata,)\n",
    "                    v_yhigh = np.quantile(v_ynparr4, q=1.0, axis=0)\n",
    "                    assert v_yhigh.shape == (d_ydata,)\n",
    "                    v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:x'] = xvals\n",
    "                    if plot_ci:\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y/mean'] = v_ymean\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y/low'] = v_ylow\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y/high'] = v_yhigh\n",
    "                    else:\n",
    "                        v_mpldatas[f'{ycol}.cndvar/{ax_idx}:{vrnt}:y'] = v_ymean\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for i_figrow in range(axes.shape[0]):\n",
    "                    for i_figcol in range(axes.shape[1]):\n",
    "                        ax = axes[i_figrow, i_figcol]\n",
    "                        if show_mchmprt and (i_figrow == 0):\n",
    "                            tag_text = f'(a$_{{{i_figcol + 1}}}$)'\n",
    "                        elif show_mchmprt and (i_figrow > 0):\n",
    "                            tag_text = f'({\"bcdef\"[i_figcol]})'\n",
    "                        else:\n",
    "                            tag_text = f'({\"jklm\"[i_figcol]})'\n",
    "                        tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "                if show_mchmprt:\n",
    "                    for i_figrow, ax in enumerate(axes[:, 0]):\n",
    "                        txthdr = ('Speciated Mass', 'Diagnostics')[i_figrow] \n",
    "                        print_axheader(ax, txthdr, 'left', fontsize=14, pad=10, fontweight='bold')\n",
    "\n",
    "            figs_dict[f'{i_page}/vardiag'] = fig\n",
    "\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight', pad_inches=1/72)\n",
    "        print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Collective Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "colorspec = v_mplcfgs['colorspec']\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.acsmsmps.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/diagsmry']\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False, vrnt_trg='znrm')\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    # Assembling the plotting data\n",
    "    v_mpldatas1 = dict()\n",
    "    v_mpldatas1['nsmps_errhist/0:n_smpserr:y'] = v_mtrcdata1['n_smps/err'].ravel()\n",
    "    v_mpldatas1['macsm_errhist/1:m_acsmerr:y'] = v_mtrcdata1['m_acsm/err'].ravel()\n",
    "    for ax_idx, ax_id, i_eps in [(3, 'ccn_sctcnd1', i_eps1), (6, 'ccn_sctcnd3', i_eps3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:x'] = v_mtrcdata1['ccn_cdf/orig'][..., i_eps].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:y'] = v_mtrcdata1['ccn_cdf/znrm'][..., i_eps].ravel()\n",
    "    v_mpldatas1['qs_popsctcnd/4:qs_pop:x'] = v_mtrcdata1['qs_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qs_popsctcnd/4:qs_pop:y'] = v_mtrcdata1['qs_pop/znrm'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcnd/7:qa_pop:x'] = v_mtrcdata1['qa_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcnd/7:qa_pop:y'] = v_mtrcdata1['qa_pop/znrm'][:, :, i_wvl1, :].ravel()\n",
    "    for ax_idx, ax_id, i_tmp in [(5, 'frznfrac_sctcnd1', i_tmp1), (8, 'frznfrac_sctcnd3', i_tmp3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:x'] = v_mtrcdata1['frznfrac_tmp/orig'][..., i_tmp].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:y'] = v_mtrcdata1['frznfrac_tmp/znrm'][..., i_tmp].ravel()\n",
    "\n",
    "    # Making the matplotlib calls\n",
    "    v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatas2):\n",
    "        fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, axes=axes, \n",
    "            mplopts=v_mplcfgs[ax_id])\n",
    "    axes1d = axes.ravel()\n",
    "\n",
    "    # Adding the text boxes\n",
    "    for ax_idx, textstr in [\n",
    "        (3,  f'$s={100*eps1:0.2g}\\%$'), \n",
    "        (6,  f'$s={100*eps3:0.2g}\\%$'), \n",
    "        (4,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (7,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (5,  f'$T={{\\\\rm {tmp1:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (8,  f'$T={{\\\\rm {tmp3:.0f}^{{\\circ}}\\\\ C}}$')]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', usetex=True, bbox={'facecolor': 'none', \n",
    "            'edgecolor': 'none'})\n",
    "\n",
    "    # Adding the x=y line to scatter plots\n",
    "    for ax_idx in range(3, 9):\n",
    "        ax = axes1d[ax_idx]\n",
    "        x_lo = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "        x_hi = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "        x_id = np.linspace(x_lo, x_hi, 100)\n",
    "        ax.plot(x_id, x_id, lw=1, ls='--', color=colorspec['blue'])\n",
    "    \n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                ax_idx = i_figrow * axes.shape[1] + i_figcol\n",
    "                tag_text = f'({\"abcdefghi\"[ax_idx]})'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "    \n",
    "    # Removing the empty axis at the end of the first row\n",
    "    axes[0, 2].remove()\n",
    "    \n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_smry.pdf'\n",
    "    pngpath = pdfpath[:-4] + '.png'\n",
    "    fig.savefig(pngpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pngpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Diagnostic Calibration Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "colorspec = v_mplcfgs['colorspec']\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.acsmsmps.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/diagsmryqnt']\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False, vrnt_trg='znrm')\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    # Assembling the plotting data\n",
    "    v_mpldatas1 = dict()\n",
    "    # v_mpldatas1['nsmps_errhist/0:n_smpserr:y'] = v_mtrcdata1['n_smps/err'].ravel()\n",
    "    # v_mpldatas1['macsm_errhist/1:m_acsmerr:y'] = v_mtrcdata1['m_acsm/err'].ravel()\n",
    "    for ax_idx, ax_id, i_eps in [(0, 'ccn_sctcndqnt1', i_eps1), (3, 'ccn_sctcndqnt3', i_eps3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:x'] = v_mtrcdata1['ccn_cdf/origraw'][..., i_eps].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:ccn_cdf:y'] = v_mtrcdata1['ccn_cdf/origcdf'][..., i_eps].ravel()\n",
    "    v_mpldatas1['qs_popsctcndqnt/1:qs_pop:x'] = v_mtrcdata1['qs_pop/origraw'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qs_popsctcndqnt/1:qs_pop:y'] = v_mtrcdata1['qs_pop/origcdf'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcndqnt/4:qa_pop:x'] = v_mtrcdata1['qa_pop/origraw'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsctcndqnt/4:qa_pop:y'] = v_mtrcdata1['qa_pop/origcdf'][:, :, i_wvl1, :].ravel()\n",
    "    for ax_idx, ax_id, i_tmp in [(2, 'frznfrac_sctcndqnt1', i_tmp1), (5, 'frznfrac_sctcndqnt3', i_tmp3)]:\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:x'] = v_mtrcdata1['frznfrac_tmp/origraw'][..., i_tmp].ravel()\n",
    "        v_mpldatas1[f'{ax_id}/{ax_idx}:frznfrac_tmp:y'] = v_mtrcdata1['frznfrac_tmp/origcdf'][..., i_tmp].ravel()\n",
    "\n",
    "    # Making the matplotlib calls\n",
    "    v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatas2):\n",
    "        fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, axes=axes, \n",
    "            mplopts=v_mplcfgs[ax_id])\n",
    "    axes1d = axes.ravel()\n",
    "\n",
    "    # Adding the text boxes\n",
    "    for ax_idx, textstr in [\n",
    "        (0,  f'$s={100*eps1:0.2g}\\%$'), \n",
    "        (3,  f'$s={100*eps3:0.2g}\\%$'), \n",
    "        (1,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (4,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (2,  f'$T={{\\\\rm {tmp1:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (5,  f'$T={{\\\\rm {tmp3:.0f}^{{\\circ}}\\\\ C}}$')]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', usetex=True, bbox={'facecolor': 'none', \n",
    "            'edgecolor': 'none'})\n",
    "    \n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                ax_idx = i_figrow * axes.shape[1] + i_figcol\n",
    "                tag_text = f'({\"abc\"[i_figcol]}$_{{{i_figrow+1}}}$)'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "        \n",
    "        for i_figcol, col_ttl in enumerate(['Cloud Condensation', 'Optical Properties', 'Ice Nucleation']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', pad=8, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_smry.pdf'\n",
    "    pngpath = pdfpath[:-4] + '.png'\n",
    "    fig.savefig(pngpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pngpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Conditional Trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Plot: Data Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 84\n",
    "plt.ioff()\n",
    "\n",
    "# Loading the original data\n",
    "data_path = f'{data_dir}/02_masshist/03_bwchisamp.nc'\n",
    "data_hist = load_histdata(data_path)\n",
    "m_chmprthst2 = data_hist['m_chmprthst']\n",
    "v_ydata2, aero_cstscnv2 = cnvrt_physunits({'m_chmprthst/orig': m_chmprthst2}, aero_csts)\n",
    "n_snr, n_t = data_hist['n_snr'], data_hist['n_t']\n",
    "n_chem, n_bins = aero_cstscnv2['n_chem'], aero_cstscnv2['n_bins']\n",
    "m_chmprthst3 = v_ydata2['m_chmprthst/orig']\n",
    "assert m_chmprthst3.shape == (n_snr, n_t, n_chem, n_bins)\n",
    "m_chmprthst4 = m_chmprthst3.mean(axis=(0, 1))\n",
    "assert m_chmprthst4.shape == (n_chem, n_bins)\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "for ax_id in ['paper.intro.data.hmap.specific', 'paper.intro.data.hmap.global']:\n",
    "    v_mplcfgs[ax_id]['yticks/ticks'] = (np.arange(n_chem) + 0.5).tolist()\n",
    "    v_mplcfgs[ax_id]['yticks/labels'] = chem_species\n",
    "    v_mplcfgs[ax_id]['ylim'] = [0, n_chem]\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    i_sel = viz_data['i_sel']\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    n_chem = viz_data['n_chem']\n",
    "    n_bins = viz_data['n_bins']\n",
    "    d_histbins2 = viz_data['d_histbins']\n",
    "\n",
    "    if (not exprmnt.startswith('trad.')) or (split not in ('train',)) or (arch not in ('mlp',)):\n",
    "        continue\n",
    "\n",
    "    i_samp = i_sel[0, 2] if i_sampintro is None else i_sampintro\n",
    "    v_ydata2 = {key: np_flatten(val, 0, 2)[i_samp] for key, val in v_ydata.items()}\n",
    "\n",
    "    m_chmprtsamp = v_ydata2[f'm_chmprthst/orig']\n",
    "\n",
    "    assert d_histbins2.shape == (n_bins + 1,)\n",
    "    assert m_chmprtsamp.shape == (n_chem, n_bins)\n",
    "    assert m_chmprthst4.shape == (n_chem, n_bins)\n",
    "\n",
    "    v_mpldatas = dict()\n",
    "    # The speciated mass stack bar plot data\n",
    "    ax_idx = 0\n",
    "    for i_chem, chem in enumerate(chem_species):\n",
    "        v_mpldatas[f'paper.intro.data.bar/{ax_idx}:{chem}:x'] = d_histbins2\n",
    "        v_mpldatas[f'paper.intro.data.bar/{ax_idx}:{chem}:y'] = m_chmprtsamp[i_chem]\n",
    "\n",
    "    # The speciated mass heatmap data\n",
    "    ax_idx, ax_id = 1, 'paper.intro.data.hmap.specific'\n",
    "    v_mpldatas[f'{ax_id}/{ax_idx}:mass:x'] = d_histbins2\n",
    "    v_mpldatas[f'{ax_id}/{ax_idx}:mass:y'] = np.arange(n_chem + 1)\n",
    "    v_mpldatas[f'{ax_id}/{ax_idx}:mass:c'] = m_chmprtsamp\n",
    "\n",
    "    # The speciated mass heatmap data\n",
    "    ax_idx, ax_id = 2, 'paper.intro.data.hmap.global'\n",
    "    v_mpldatas[f'{ax_id}/{ax_idx}:mass:x'] = d_histbins2\n",
    "    v_mpldatas[f'{ax_id}/{ax_idx}:mass:y'] = np.arange(n_chem + 1)\n",
    "    v_mpldatas[f'{ax_id}/{ax_idx}:mass:c'] = m_chmprthst4\n",
    "    \n",
    "    v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "\n",
    "    ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatashie):\n",
    "        fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "            axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "    \n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for ax, tag_txt in [(axes[0, 0], '(a)'), \n",
    "            (axes[0, 1], '(b)'), (axes[0, 2], '(c)')]:\n",
    "            tag_axis(ax, tag_txt, fontsize=11, pad=[0.3, 0.8])\n",
    "    \n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_intro_data.pdf'\n",
    "    fig.savefig(pdfpath, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "    \n",
    "    i_fig += 1\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Plot: Example Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 85\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbins = viz_data['d_histbins']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "    len_wv = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "\n",
    "    if (not exprmnt.startswith('trad.')) or (split not in ('train',)) or (arch not in ('mlp',)):\n",
    "        continue\n",
    "\n",
    "    v_mplcfgs['paper.intro.diag.ccn_cdf']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['paper.intro.diag.frznfrac_tmp']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    assert i_sel.shape[0] == 1\n",
    "    i_samplst = i_sel[0, :] if i_sampintro is None else [i_sampintro]\n",
    "\n",
    "    figs_dict = dict()\n",
    "    for ii_samp, i_samp in enumerate(i_samplst):\n",
    "        v_ydata2 = {key: np_flatten(val, 0, 2)[i_samp] for key, val in v_ydata.items()}\n",
    "\n",
    "        v_mpldatas = dict()\n",
    "        # The speciated mass data\n",
    "        ax_idx = 0\n",
    "        for i_chem, chem in enumerate(chem_species):\n",
    "            v_mpldatas[f'paper.intro.diag.m_chmprthst/{ax_idx}:{chem}:x'] = d_histbins\n",
    "            v_mpldatas[f'paper.intro.diag.m_chmprthst/{ax_idx}:{chem}:y'] = v_ydata2[f'm_chmprthst/orig'][i_chem]\n",
    "        \n",
    "        # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "        ax_ixyspec = []\n",
    "        ax_ixyspec.append([2, d_histbins,   'm_prthst'    ])\n",
    "        ax_ixyspec.append([3, d_histbins,   'n_prthst'    ])\n",
    "        ax_ixyspec.append([4, eps_histbins, 'ccn_cdf'     ])\n",
    "        ax_ixyspec.append([5, len_wv,       'qs_pop'      ])\n",
    "        ax_ixyspec.append([6, len_wv,       'qa_pop'      ])\n",
    "        ax_ixyspec.append([7, temprtr_bins, 'frznfrac_tmp'])\n",
    "        \n",
    "        for ax_idx, xvals, ycol in ax_ixyspec:\n",
    "            v_mpldatas[f'paper.intro.diag.{ycol}/{ax_idx}:orig:x'] = xvals\n",
    "            v_mpldatas[f'paper.intro.diag.{ycol}/{ax_idx}:orig:y'] = np.squeeze(v_ydata2[f'{ycol}/orig'])\n",
    "        \n",
    "        v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "\n",
    "        ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "        fig, axes = None, None\n",
    "        for ax_idx, ax_id in enumerate(v_mpldatashie):\n",
    "            fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "        \n",
    "        # Sharing the y-axis limits between the three axes\n",
    "        axes_yshrd = axes[0, :3]\n",
    "        ax_ylimlo = min(ax.get_ylim()[0] for ax in axes_yshrd)\n",
    "        ax_ylimhi = max(ax.get_ylim()[1] for ax in axes_yshrd)\n",
    "        for ax in axes_yshrd:\n",
    "            ax.set_ylim(ax_ylimlo, ax_ylimhi)\n",
    "\n",
    "        # Adding the top left april tag for axis identification\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for ax_idx, tag_char in enumerate('abcdefgh'):\n",
    "                ax = axes.ravel()[ax_idx]\n",
    "                tag_axis(ax, f'({tag_char})', fontsize=11, pad=(0.3, 0.6))\n",
    "\n",
    "        # Removing the reconstructed mass axis\n",
    "        axes[0, 1].remove()\n",
    "    \n",
    "        figs_dict[f'{ii_samp}/diag'] = fig\n",
    "\n",
    "        ########## The Per Wave-Length Optical Plots ########## \n",
    "        for ycol in ('qscs_prt', 'qacs_prt')[:0]:\n",
    "            v_mpldatas2 = dict()\n",
    "            for i_wvl, wvl in enumerate(len_wv):\n",
    "                v_mpldatas2[f'{i_wvl}:orig:x'] = d_histbins\n",
    "                v_mpldatas2[f'{i_wvl}:orig:y'] = v_ydata2[f'{ycol}/orig'][i_wvl]\n",
    "            mploptid = f'paper.intro.diag.{ycol}'\n",
    "            fig2, axes2 = plot_mpl(data=v_mpldatas2, fig=None, axes=None, \n",
    "                mplopts=v_mplcfgs[mploptid])\n",
    "            for i_wvl, (wvl, ax) in enumerate(zip(len_wv, axes2.ravel())):\n",
    "                props = dict(facecolor='none', edgecolor='none')\n",
    "                textstr = f'$\\\\lambda={len_wv[i_wvl]:.1f}\\\\ {{\\\\rm \\\\mu m}}$'\n",
    "                ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    usetex=True, verticalalignment='top', bbox=props)\n",
    "            figs_dict[f'{ii_samp}/{mploptid}'] = fig2\n",
    "\n",
    "        ########## The Per-Chemical Species Plots ########## \n",
    "        v_mpldatas3 = dict()\n",
    "        for i_chem, chem in enumerate(chem_species):\n",
    "            v_mpldatas3[f'{i_chem}:orig:x'] = d_histbins\n",
    "            v_mpldatas3[f'{i_chem}:orig:y'] = v_ydata2[f'm_chmprthst/orig'][i_chem]\n",
    "        for mploptid in ('paper.intro.diag.m_chmhst1', 'paper.intro.diag.m_chmhst2'):\n",
    "            fig3, axes3 = plot_mpl(data=v_mpldatas3, fig=None, axes=None, \n",
    "                mplopts=v_mplcfgs[mploptid])\n",
    "            for i_chem, (chem, ax) in enumerate(zip(chem_species, axes3.ravel())):\n",
    "                props = dict(facecolor='none', edgecolor='none')\n",
    "                ax.text(0.05, 0.95, chem, transform=ax.transAxes, fontsize=10,\n",
    "                    usetex=True, verticalalignment='top', bbox=props)\n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for ax_idx, ax in enumerate(axes3.ravel()):\n",
    "                    tag_txt = {'m_chmhst1': f'(i$_{{{ax_idx+1}}}$)', \n",
    "                        'm_chmhst2': f'(j$_{{{ax_idx+1}}}$)'}[mploptid.split('.')[-1]]\n",
    "                    tag_axis(ax, tag_txt, fontsize=11, pad=(0.3, 0.6))\n",
    "\n",
    "            figs_dict[f'{ii_samp}/{mploptid}'] = fig3\n",
    "    \n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_intro_diag.pdf'\n",
    "    with PdfPages(pdfpath) as pdf:\n",
    "        for ii_samp, fig3 in figs_dict.items():\n",
    "            pdf.savefig(figure=fig3, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "    \n",
    "    i_fig += 1\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Single Aerosol Population Original vs. Reconstruction Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 38\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "v_mplcfgs['ccn_cdf']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "v_mplcfgs['frznfrac_tmp']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbins = viz_data['d_histbins']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "    len_wv = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "\n",
    "    if (not exprmnt.startswith('trad.')) or (split not in ('train', 'test')):\n",
    "        continue\n",
    "    \n",
    "    assert i_sel.shape[0] == 1\n",
    "\n",
    "    figs_dict = dict()\n",
    "    for ii_samp, i_samp in enumerate(i_sel[0]):\n",
    "        v_ydata2 = {key: np_flatten(val, 0, 2)[i_samp] for key, val in v_ydata.items()}\n",
    "        \n",
    "        v_mpldatas = dict()\n",
    "        # The speciated mass data\n",
    "        for ax_idx, vrnt in enumerate(['orig', 'rcnst']):\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'm_chmprthst.{vrnt}/{ax_idx}:{chem}:x'] = d_histbins\n",
    "                v_mpldatas[f'm_chmprthst.{vrnt}/{ax_idx}:{chem}:y'] = v_ydata2[f'm_chmprthst/{vrnt}'][i_chem]\n",
    "        \n",
    "        # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "        ax_ixyspec = []\n",
    "        ax_ixyspec.append([2, d_histbins,   'm_prthst'    ])\n",
    "        ax_ixyspec.append([3, d_histbins,   'n_prthst'    ])\n",
    "        ax_ixyspec.append([4, eps_histbins, 'ccn_cdf'     ])\n",
    "        ax_ixyspec.append([5, len_wv,       'qs_pop'      ])\n",
    "        ax_ixyspec.append([6, len_wv,       'qa_pop'      ])\n",
    "        ax_ixyspec.append([7, temprtr_bins, 'frznfrac_tmp'])\n",
    "        \n",
    "        for ax_idx, xvals, ycol in ax_ixyspec:\n",
    "            for vrnt in ['orig', 'rcnst']:\n",
    "                v_mpldatas[f'{ycol}/{ax_idx}:{vrnt}:x'] = xvals\n",
    "                v_mpldatas[f'{ycol}/{ax_idx}:{vrnt}:y'] = np.squeeze(v_ydata2[f'{ycol}/{vrnt}'])\n",
    "        \n",
    "        v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "\n",
    "        ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "        fig, axes = None, None\n",
    "        for ax_idx, ax_id in enumerate(v_mpldatashie):\n",
    "            fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "        \n",
    "        # Sharing the y-axis limits between the three axes\n",
    "        axes_yshrd = axes[0, :3]\n",
    "        ax_ylimlo = min(ax.get_ylim()[0] for ax in axes_yshrd)\n",
    "        ax_ylimhi = max(ax.get_ylim()[1] for ax in axes_yshrd)\n",
    "        for ax in axes_yshrd:\n",
    "            ax.set_ylim(ax_ylimlo, ax_ylimhi)\n",
    "\n",
    "        # Adding the small identification text box\n",
    "        axes1d = axes.ravel()\n",
    "        axtexts = [(1, f'SMRE={e_samps[i_samp].item():.2f}')]        \n",
    "        for ax_idx, textstr in axtexts:\n",
    "            ax = axes1d[ax_idx]\n",
    "            props = dict(facecolor='none', edgecolor='none')\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                usetex=True, verticalalignment='top', bbox=props)\n",
    "\n",
    "        # Adding the orignal and reconstruction labels\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for ax, text_str in [(axes[0, 0], 'Original'), (axes[0, 1], 'Reconstruction')]:\n",
    "                print_axheader(ax, text_str, 'top', fontsize=10)\n",
    "\n",
    "        # Adding the top left april tag for axis identification\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for ax_idx, tag_char in enumerate('abcdefgh'):\n",
    "                ax = axes.ravel()[ax_idx]\n",
    "                tag_axis(ax, f'({tag_char})', fontsize=11, pad=(0.3, 0.6))\n",
    "    \n",
    "        figs_dict[f'{ii_samp}/diag'] = fig\n",
    "\n",
    "        ########## The Per Wave-Length Optical Plots ########## \n",
    "        for ycol in ('qscs_prt', 'qacs_prt')[:0]:\n",
    "            v_mpldatas2 = dict()\n",
    "            for i_wvl, wvl in enumerate(len_wv):\n",
    "                for vrnt in ['orig', 'rcnst']:\n",
    "                    v_mpldatas2[f'{i_wvl}:{vrnt}:x'] = d_histbins\n",
    "                    v_mpldatas2[f'{i_wvl}:{vrnt}:y'] = v_ydata2[f'{ycol}/{vrnt}'][i_wvl]\n",
    "            mploptid = ycol\n",
    "            fig2, axes2 = plot_mpl(data=v_mpldatas2, fig=None, axes=None, \n",
    "                mplopts=v_mplcfgs[mploptid])\n",
    "            for i_wvl, (wvl, ax) in enumerate(zip(len_wv, axes2.ravel())):\n",
    "                props = dict(facecolor='none', edgecolor='none')\n",
    "                textstr = f'$\\\\lambda={len_wv[i_wvl]:.1f}\\\\ {{\\\\rm \\\\mu m}}$'\n",
    "                ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    usetex=True, verticalalignment='top', bbox=props)\n",
    "            figs_dict[f'{ii_samp}/{mploptid}'] = fig2\n",
    "\n",
    "        ########## The Per-Chemical Species Plots ########## \n",
    "        v_mpldatas3 = dict()\n",
    "        for i_chem, chem in enumerate(chem_species):\n",
    "            for vrnt in ['orig', 'rcnst']:\n",
    "                v_mpldatas3[f'{i_chem}:{vrnt}:x'] = d_histbins\n",
    "                v_mpldatas3[f'{i_chem}:{vrnt}:y'] = v_ydata2[f'm_chmprthst/{vrnt}'][i_chem]\n",
    "        for mploptid in ('m_chmhst1', 'm_chmhst2'):\n",
    "            fig3, axes3 = plot_mpl(data=v_mpldatas3, fig=None, axes=None, \n",
    "                mplopts=v_mplcfgs[mploptid])\n",
    "            for i_chem, (chem, ax) in enumerate(zip(chem_species, axes3.ravel())):\n",
    "                props = dict(facecolor='none', edgecolor='none')\n",
    "                ax.text(0.05, 0.95, chem, transform=ax.transAxes, fontsize=10,\n",
    "                    usetex=True, verticalalignment='top', bbox=props)\n",
    "            \n",
    "            # Adding the top left april tag for axis identification\n",
    "            with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "                for ax_idx, ax in enumerate(axes3.ravel()):\n",
    "                    tag_txt = {'m_chmhst1': f'(i$_{{{ax_idx+1}}}$)', \n",
    "                        'm_chmhst2': f'(j$_{{{ax_idx+1}}}$)'}[mploptid]\n",
    "                    tag_axis(ax, tag_txt, fontsize=11, pad=(0.3, 0.6))\n",
    "            figs_dict[f'{ii_samp}/{mploptid}'] = fig3\n",
    "\n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_{arch}_{split}_anec.pdf'\n",
    "    with PdfPages(pdfpath) as pdf:\n",
    "        for ii_samp, fig3 in figs_dict.items():\n",
    "            pdf.savefig(figure=fig3, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "    \n",
    "    i_fig += 1\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generated Aerosol Population Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 42\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbins = viz_data['d_histbins']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "    len_wv = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    n_clctn, n_prtr = i_sel.shape\n",
    "    e_samps = viz_data['e_samps']\n",
    "\n",
    "    if (not exprmnt.startswith('trad.')) or (split not in ('normal',)):\n",
    "        continue\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.genr']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.genr']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "\n",
    "    figs_dict, samp_cntr = dict(), 1\n",
    "    for i_clctn in range(n_clctn):\n",
    "        v_mpldatas = dict()\n",
    "        i_selclctn = i_sel[i_clctn]\n",
    "        # The speciated mass data\n",
    "        for ii_samp, i_samp in enumerate(i_selclctn):\n",
    "            ax_idx = ii_samp\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'm_chmprthst.genr/{ax_idx}:{chem}:x'] = d_histbins\n",
    "                y_vals = np_flatten(v_ydata['m_chmprthst/genr'], 0, 2)[i_samp, i_chem]\n",
    "                v_mpldatas[f'm_chmprthst.genr/{ax_idx}:{chem}:y'] = y_vals\n",
    "        \n",
    "        # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "        ax_ixyspec = []\n",
    "        ax_ixyspec.append([1, d_histbins,   'n_prthst'    ])\n",
    "        ax_ixyspec.append([2, eps_histbins, 'ccn_cdf'     ])\n",
    "        ax_ixyspec.append([3, len_wv,       'qs_pop'      ])\n",
    "        ax_ixyspec.append([4, len_wv,       'qa_pop'      ])\n",
    "        ax_ixyspec.append([5, temprtr_bins, 'frznfrac_tmp'])\n",
    "        for ii_samp, i_samp in enumerate(i_selclctn):\n",
    "            for i_row, xvals, ycol in ax_ixyspec:\n",
    "                ax_idx = i_row * n_prtr + ii_samp\n",
    "                v_mpldatas[f'{ycol}.genr/{ax_idx}:genr:x'] = xvals\n",
    "                y_vals = np_flatten(v_ydata[f'{ycol}/genr'], 0, 2)[i_samp]\n",
    "                v_mpldatas[f'{ycol}.genr/{ax_idx}:genr:y'] = np.squeeze(y_vals)\n",
    "        \n",
    "        v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "        ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "        fig, axes = None, None\n",
    "        for ax_id in v_mpldatashie:\n",
    "            fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "    \n",
    "        figs_dict[f'{i_clctn}/diag'] = fig\n",
    "\n",
    "        # Adding the small identification text box\n",
    "        axes1d = axes.ravel()\n",
    "        for ii_samp, i_samp in enumerate(i_selclctn):\n",
    "            ax = axes[0, ii_samp]\n",
    "            textstr = f'{100 * e_samps[i_samp].item():.1f}' + '\\% Dust'\n",
    "            props = dict(facecolor='none', edgecolor='none')\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                usetex=True, verticalalignment='top', bbox=props)\n",
    "        \n",
    "        # Adding the top left april tag for axis identification\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for i_figrow in range(axes.shape[0]):\n",
    "                for i_figcol in range(axes.shape[1]):\n",
    "                    ax = axes[i_figrow, i_figcol]\n",
    "                    tag_text = f'({\"abcdefgh\"[i_figrow]}$_{{{i_figcol + 1}}}$)'\n",
    "                    tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "            for i_figcol in range(axes.shape[1]):\n",
    "                print_axheader(axes[0, i_figcol], f'Sample {samp_cntr}', 'top', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "                samp_cntr += 1\n",
    "\n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_{arch}_{split}_anec.pdf'\n",
    "    with PdfPages(pdfpath) as pdf:\n",
    "        for i_clctn, fig_clctn in figs_dict.items():\n",
    "            pdf.savefig(figure=fig_clctn, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "    \n",
    "    i_fig += 1\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sample Selection Criterion Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the plot configs\n",
    "plt.ioff()\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, sep=':', mindepth=2, maxdepth=2).items():\n",
    "    exprmnt, arch = viz_id.split(':')\n",
    "    if not exprmnt.startswith('trad.'):\n",
    "        continue\n",
    "    v_mpldata = {\n",
    "        '0:sm_relerr:y': viz_data['train/e_samps'],\n",
    "        '1:sm_relerr:y': viz_data['test/e_samps']}\n",
    "    fig, axes = plot_mpl(data=v_mpldata, fig=None, axes=None, \n",
    "        mplopts=v_mplcfgs['sm_relerrhist'])\n",
    "\n",
    "    for ax, split in [(axes[0,0], 'train'), (axes[0, 1], 'test')]:\n",
    "        i_sel = viz_data[f'{split}/i_sel']\n",
    "        e_samps = viz_data[f'{split}/e_samps']\n",
    "\n",
    "        for i_samp in i_sel[0]:\n",
    "            ax.axvline(e_samps[i_samp], lw=1, color='black', ls='--')\n",
    "        \n",
    "        props = dict(facecolor='none', edgecolor='none')\n",
    "        for i_samp in i_sel[0, -2:]:\n",
    "            ii_prcntl = (e_samps <= e_samps[i_samp]).mean()\n",
    "            textstr = f'$q_{{{ii_prcntl*100:.2g}}}$'\n",
    "            ax.text(e_samps[i_samp] + 0.01, 3.95, textstr, transform=ax.transData, \n",
    "                fontsize=12, usetex=True, verticalalignment='top', bbox=props)\n",
    "        \n",
    "        ax.text(0.71, 0.95, f'{arch.upper()} {split.capitalize()}', transform=ax.transAxes, \n",
    "            fontsize=10, usetex=True, verticalalignment='top', bbox=props)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collective Aerosol Diagnostics (Summary Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 44\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "colorspec = v_mplcfgs['colorspec']\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    aero_cstscnv = viz_data['aero_csts']\n",
    "\n",
    "    if (not exprmnt.startswith('trad.')) or (split not in ('train', 'test')):\n",
    "        continue\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata1 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False)\n",
    "\n",
    "    eps1, eps2, eps3 = v_mtrcdata1['eps1'], v_mtrcdata1['eps2'], v_mtrcdata1['eps3']\n",
    "    i_eps1, i_eps2, i_eps3 = v_mtrcdata1['i_eps1'], v_mtrcdata1['i_eps2'], v_mtrcdata1['i_eps3']\n",
    "    tmp1, tmp2, tmp3 = v_mtrcdata1['tmp1'], v_mtrcdata1['tmp2'], v_mtrcdata1['tmp3']\n",
    "    i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata1['i_tmp1'], v_mtrcdata1['i_tmp2'], v_mtrcdata1['i_tmp3']\n",
    "    wvl1, i_wvl1 = v_mtrcdata1['wvl1'], v_mtrcdata1['i_wvl1']\n",
    "\n",
    "    # Assembling the plotting data\n",
    "    v_mpldatas1 = dict()\n",
    "    v_mpldatas1['ccn_errhist/0:ccn_cdferr:y'] = v_mtrcdata1['ccn_cdf/err'].ravel()\n",
    "    for i, i_eps in [(3, i_eps1), (6, i_eps2), (9, i_eps3)]:\n",
    "        v_mpldatas1[f'ccn_sct{i//3}/{i}:ccn_cdf:x'] = v_mtrcdata1['ccn_cdf/orig'][..., i_eps].ravel()\n",
    "        v_mpldatas1[f'ccn_sct{i//3}/{i}:ccn_cdf:y'] = v_mtrcdata1['ccn_cdf/rcnst'][..., i_eps].ravel()\n",
    "    v_mpldatas1['qs_errhist/1:qs_pop:y'] = v_mtrcdata1['logqs_pop/err'].ravel()\n",
    "    v_mpldatas1['qa_errhist/4:qa_pop:y'] = v_mtrcdata1['logqa_pop/err'].ravel()\n",
    "    v_mpldatas1['qs_popsct/7:qs_pop:x'] = v_mtrcdata1['qs_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qs_popsct/7:qs_pop:y'] = v_mtrcdata1['qs_pop/rcnst'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsct/10:qa_pop:x'] = v_mtrcdata1['qa_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['qa_popsct/10:qa_pop:y'] = v_mtrcdata1['qa_pop/rcnst'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1['logfrznfrac_errhist/2:logfrznfrac_tmp:y'] = v_mtrcdata1['logfrznfrac_tmp/err'].ravel()\n",
    "    for i, i_tmp in [(5, i_tmp1), (8, i_tmp2), (11, i_tmp3)]:\n",
    "        v_mpldatas1[f'frznfrac_sct{i//3}/{i}:frznfrac_tmp:x'] = v_mtrcdata1['frznfrac_tmp/orig'][..., i_tmp].ravel()\n",
    "        v_mpldatas1[f'frznfrac_sct{i//3}/{i}:frznfrac_tmp:y'] = v_mtrcdata1['frznfrac_tmp/rcnst'][..., i_tmp].ravel()\n",
    "\n",
    "    # Making the matplotlib calls\n",
    "    v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatas2):\n",
    "        fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, axes=axes, \n",
    "            mplopts=v_mplcfgs[ax_id])\n",
    "    axes1d = axes.ravel()\n",
    "\n",
    "    # Adding the text boxes\n",
    "    for ax_idx, textstr in [\n",
    "        (3,  f'$s={100*eps1:0.2g}\\%$'), \n",
    "        (6,  f'$s={100*eps2:0.2g}\\%$'), \n",
    "        (9,  f'$s={100*eps3:0.2g}\\%$'),\n",
    "        (7,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (10, f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (5,  f'$T={{\\\\rm {tmp1:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (8,  f'$T={{\\\\rm {tmp2:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (11, f'$T={{\\\\rm {tmp3:.0f}^{{\\circ}}\\\\ C}}$')]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', usetex=True, bbox={'facecolor': 'none', \n",
    "            'edgecolor': 'none'})\n",
    "\n",
    "    # Adding the x=y line to scatter plots\n",
    "    for ax_idx in [3, 6, 9, 7, 10, 5, 8, 11]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        x_lo = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "        x_hi = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "        x_id = np.linspace(x_lo, x_hi, 100)\n",
    "        ax.plot(x_id, x_id, lw=1, ls='--', color=colorspec['blue'])\n",
    "\n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                tag_text = f'({\"abc\"[i_figcol]}$_{{{i_figrow + 1}}}$)'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "\n",
    "        for i_figcol, col_ttl in enumerate(['Cloud Condensation', 'Optical Properties', 'Ice Nucleation']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', pad=8, fontsize=14, fontweight='bold')\n",
    "\n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_{arch}_{split}_smry.pdf'\n",
    "    pngpath = pdfpath[:-4] + '.png'\n",
    "    fig.savefig(pngpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pngpath}')\n",
    "    i_fig += 1\n",
    " \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generative Diagnostic Calibration Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 48\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "colorspec = v_mplcfgs['colorspec']\n",
    "\n",
    "for viz_id1, viz_data1 in hie2deep(viz_datas, sep=':', mindepth=2).items():\n",
    "    exprmnt, arch = viz_id1.split(':')\n",
    "\n",
    "    if (not exprmnt.startswith('trad.')) or (split not in ('test', 'normal')):\n",
    "        continue\n",
    "    \n",
    "    v_mtrcdata1deep = dict()\n",
    "    for split, viz_data2 in hie2deep(viz_data1, sep='/', maxdepth=1).items():\n",
    "        if (split not in ('test', 'normal')):\n",
    "            continue\n",
    "        vrnt_trg = {'test': 'orig', 'normal': 'genr'}[split]\n",
    "\n",
    "        v_ydata = viz_data2['v_ydata']\n",
    "        aero_cstscnv = viz_data2['aero_csts']\n",
    "\n",
    "        ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "        v_mtrcdata2 = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=False, vrnt_trg=vrnt_trg)\n",
    "\n",
    "        eps1, eps2, eps3 = v_mtrcdata2['eps1'], v_mtrcdata2['eps2'], v_mtrcdata2['eps3']\n",
    "        i_eps1, i_eps2, i_eps3 = v_mtrcdata2['i_eps1'], v_mtrcdata2['i_eps2'], v_mtrcdata2['i_eps3']\n",
    "        tmp1, tmp2, tmp3 = v_mtrcdata2['tmp1'], v_mtrcdata2['tmp2'], v_mtrcdata2['tmp3']\n",
    "        i_tmp1, i_tmp2, i_tmp3 = v_mtrcdata2['i_tmp1'], v_mtrcdata2['i_tmp2'], v_mtrcdata2['i_tmp3']\n",
    "        wvl1, i_wvl1 = v_mtrcdata2['wvl1'], v_mtrcdata2['i_wvl1']\n",
    "\n",
    "        v_mtrcdata1deep[split] = v_mtrcdata2\n",
    "    v_mtrcdata1 = deep2hie(v_mtrcdata1deep)\n",
    "\n",
    "    # Assembling the plotting data\n",
    "    v_mpldatas1 = dict()\n",
    "\n",
    "    v_mpldatas1[f'ccn_hist1/0:test:y'] = v_mtrcdata1['test/ccn_cdf/orig'][..., i_eps1].ravel()\n",
    "    v_mpldatas1[f'ccn_hist1/0:normal:y'] = v_mtrcdata1['normal/ccn_cdf/genr'][..., i_eps1].ravel()\n",
    "    v_mpldatas1[f'ccn_hist2/3:test:y'] = v_mtrcdata1['test/ccn_cdf/orig'][..., i_eps3].ravel()\n",
    "    v_mpldatas1[f'ccn_hist2/3:normal:y'] = v_mtrcdata1['normal/ccn_cdf/genr'][..., i_eps3].ravel()\n",
    "\n",
    "    v_mpldatas1[f'qs_hist/1:test:y'] = v_mtrcdata1['test/qs_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1[f'qs_hist/1:normal:y'] = v_mtrcdata1['normal/qs_pop/genr'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1[f'qa_hist/4:test:y'] = v_mtrcdata1['test/qa_pop/orig'][:, :, i_wvl1, :].ravel()\n",
    "    v_mpldatas1[f'qa_hist/4:normal:y'] = v_mtrcdata1['normal/qa_pop/genr'][:, :, i_wvl1, :].ravel()\n",
    "\n",
    "    v_mpldatas1[f'frznfrac_hist1/2:test:y'] = v_mtrcdata1['test/frznfrac_tmp/orig'][..., i_tmp1].ravel()\n",
    "    v_mpldatas1[f'frznfrac_hist1/2:normal:y'] = v_mtrcdata1['normal/frznfrac_tmp/genr'][..., i_tmp1].ravel()\n",
    "    v_mpldatas1[f'frznfrac_hist2/5:test:y'] = v_mtrcdata1['test/frznfrac_tmp/orig'][..., i_tmp3].ravel()\n",
    "    v_mpldatas1[f'frznfrac_hist2/5:normal:y'] = v_mtrcdata1['normal/frznfrac_tmp/genr'][..., i_tmp3].ravel()\n",
    "\n",
    "    # Making the matplotlib calls\n",
    "    v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatas2):\n",
    "        fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, axes=axes, \n",
    "            mplopts=v_mplcfgs[ax_id])\n",
    "    axes1d = axes.ravel()\n",
    "\n",
    "    # Adding the text boxes\n",
    "    for ax_idx, textstr in [\n",
    "        (0,  f'$s={100*eps1:0.2g}\\%$'), \n",
    "        (3,  f'$s={100*eps3:0.2g}\\%$'),\n",
    "        (1,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (4,  f'${{\\\\rm \\lambda={wvl1*1e6:.1f}\\\\ \\mu m}}$'), \n",
    "        (2,  f'$T={{\\\\rm {tmp1:.0f}^{{\\circ}}\\\\ C}}$'), \n",
    "        (5,  f'$T={{\\\\rm {tmp3:.0f}^{{\\circ}}\\\\ C}}$')]:\n",
    "        ax = axes1d[ax_idx]\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', usetex=True, bbox={'facecolor': 'none', \n",
    "            'edgecolor': 'none'})\n",
    "\n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                tag_text = f'({\"abc\"[i_figcol]}$_{{{i_figrow + 1}}}$)'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "\n",
    "        for i_figcol, col_ttl in enumerate(['Cloud Condensation', 'Optical Properties', 'Ice Nucleation']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', pad=8, fontsize=14, fontweight='bold')\n",
    "\n",
    "    akws = {'xycoords': 'axes fraction', 'textcoords': 'axes fraction', \n",
    "        'fontsize': 9, 'bbox/pad': 90, 'bbox/facecolor': 'none', 'bbox/edgecolor': 'none', \n",
    "        'arrowprops/arrowstyle': '->', 'arrowprops/connectionstyle': 'arc3,rad=0.1'}\n",
    "    \n",
    "    annot_kws = dict()\n",
    "    annot_kws['test'] = {'text': 'Test Data', 'xytext': [0.35, 0.72], \n",
    "        'xy': [0.16, 0.72], 'arrowprops/relpos': [0.0, 0.5], **akws}\n",
    "    annot_kws['normal'] = {'text': 'Generated', 'xytext': [0.45, 0.53], \n",
    "        'xy': [0.20, 0.50], 'arrowprops/relpos': [0.0, 0.5], **akws}\n",
    "    for key, annot_kw in annot_kws.items():\n",
    "        axes[0, 0].annotate(**hie2deep(annot_kw))\n",
    "\n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_{arch}_gencal.pdf'\n",
    "    fig.savefig(pdfpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "    i_fig += 1\n",
    " \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Data For MDS Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizsct_datas = dict()\n",
    "for arch in ['mlp', 'cnn']:\n",
    "    ############## Collecting the plotting data ###############\n",
    "    fpidx = {'mlp': '11_mlphist.0.0', 'cnn': '12_cnnhist.0.0'}[arch]\n",
    "    rio = resio(fpidx=fpidx, resdir='./20_vaehist/results', driver='sec2')\n",
    "    rio_dtypes = rio.dtypes()\n",
    "    rio_keys = [key.split(':', 1)[-1] for key in rio_dtypes]\n",
    "    n_seeds = 12\n",
    "\n",
    "    v_mdsdataraw = dict()\n",
    "    for key in rio_keys:\n",
    "        if not any(key.startswith(f'var/eval/mds/{eid}:') for eid in ['identity', 'polyhstbal']):\n",
    "            continue\n",
    "        val = rio(key)\n",
    "        n_pnts, n_rcns, d_repr = val.shape[1:]\n",
    "        assert n_rcns == 1\n",
    "        assert val.shape == (n_epoch * n_seeds, n_pnts, n_rcns, d_repr)\n",
    "        val_lastepoch = val.reshape(n_epoch, n_seeds, n_pnts, d_repr)[-1]\n",
    "        assert val_lastepoch.shape == (n_seeds, n_pnts, d_repr)\n",
    "        v_mdsdataraw[key] = val_lastepoch\n",
    "\n",
    "    # Example:\n",
    "    #   v_mdsdataraw = {\n",
    "    #     'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/normal/genr/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/test/orig/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/test/rcnst/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/train/orig/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/train/rcnst/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/normal/genr/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/mu/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/phi/data': 'np.randn(12, 5000, 1)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/sig/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/test/rcnst/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/mu/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/phi/data': 'np.randn(12, 5000, 1)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/sig/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1zpcalr:z:z/train/rcnst/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1ztsne:z:z/normal/genr/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1ztsne:z:z/test/orig/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1ztsne:z:z/test/rcnst/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1ztsne:z:z/train/orig/pnts/data': 'np.randn(12, 5000, 2)',\n",
    "    #     'var/eval/mds/identity:1x1ztsne:z:z/train/rcnst/pnts/data': 'np.randn(12, 5000, 2)'\n",
    "    #   }\n",
    "\n",
    "    vizsct_datas[f'{arch}/v_mdsdataraw'] = v_mdsdataraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collective MDS Scatter Plots Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 50\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "    \n",
    "figs_dict = dict()\n",
    "for arch, viz_data in hie2deep(vizsct_datas, maxdepth=1).items():\n",
    "    v_mdsdataraw = viz_data['v_mdsdataraw']\n",
    "    \n",
    "    ########################### Collecting the plotting data ############################\n",
    "    v_mpldatas = dict()\n",
    "\n",
    "    ###### The Z TSNE individual-split data ######\n",
    "    for ax_idx, split, vrnt in [(0, 'train', 'orig'), (1, 'test', 'orig'), (2, 'normal', 'genr')]:\n",
    "        pats_rnm2 = {\n",
    "            f'var/eval/mds/identity:1x1ztsne:z:z/{split}/{vrnt}/pnts/data': \n",
    "            f'ztsne/{ax_idx}:{split}/{vrnt}:pnts'}\n",
    "        ax_data = get_subdictrnmd(v_mdsdataraw, pats_rnm2)\n",
    "        v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The Z TSNE combined-splits data ######\n",
    "    ax_idx = 3\n",
    "    pats_rnm1 = {\n",
    "        'ztsne/{ax_idx}:{split}/{vrnt}:{vrepr}': \n",
    "        f'ztsne/{ax_idx}:{{split}}/{{vrnt}}:{{vrepr}}'}\n",
    "    ax_data = get_subdictrnmd(v_mpldatas, pats_rnm1)\n",
    "    v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The Z PCA individual-split data ######\n",
    "    for ax_idx, split, vrnt in [(4, 'train', 'orig'), (5, 'test', 'orig'), (6, 'normal', 'genr')]:\n",
    "        pats_rnm4 = {\n",
    "            f'var/eval/mds/identity:1x1zpcalr:z:z/{split}/{vrnt}/{{vrepr}}/data':\n",
    "            f'zpca/{ax_idx}:{split}/{vrnt}:{{vrepr}}'}\n",
    "        ax_data = get_subdictrnmd(v_mdsdataraw, pats_rnm4)\n",
    "        v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The Z PCA combined-splits data ######\n",
    "    ax_idx = 7\n",
    "    pats_rnm3 = {\n",
    "        'zpca/{ax_idx}:{split}/{vrnt}:{vrepr}': \n",
    "        f'zpca/{ax_idx}:{{split}}/{{vrnt}}:{{vrepr}}'}\n",
    "    ax_data = get_subdictrnmd(v_mpldatas, pats_rnm3)\n",
    "    v_mpldatas.update(ax_data)\n",
    "\n",
    "    ##### The X TSNE individual-split data #####\n",
    "    for ax_idx, split in [(8, 'train'), (9, 'test'), (10, 'normal')]:\n",
    "        pats_rnm6 = {\n",
    "            f'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/{split}/{{vrnt}}/pnts/data': \n",
    "            f'xtsne/{ax_idx}:{split}/{{vrnt}}:pnts'}\n",
    "        ax_data = get_subdictrnmd(v_mdsdataraw, pats_rnm6)\n",
    "        v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The X TSNE combined-splits data ######\n",
    "    ax_idx = 11\n",
    "    pats_rnm5 = {\n",
    "        'xtsne/{ax_idx}:{split}/{vrnt}:{vrepr}': \n",
    "        f'xtsne/{ax_idx}:{{split}}/{{vrnt}}:{{vrepr}}'}\n",
    "    ax_data = get_subdictrnmd(v_mpldatas, pats_rnm5)\n",
    "    v_mpldatas.update(ax_data)\n",
    "\n",
    "    # Restricting the data to the first seed\n",
    "    v_mpldatas2 = {key: val[0] for key, val in v_mpldatas.items()}\n",
    "\n",
    "    # Splitting the various plotting configs\n",
    "    v_mpldatas3 = hie2deep(v_mpldatas2, maxdepth=1)\n",
    "\n",
    "    ################### Calling Matplotlib to Plot the Scatter Points ################### \n",
    "    fig, axes = None, None\n",
    "    for ax_id in v_mpldatas3:\n",
    "        fig, axes = plot_mpl(data=v_mpldatas3[ax_id], fig=fig, \n",
    "            axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "    # Adding the top left april tag for axis identification\n",
    "    n_figrows, n_figcols = axes.shape\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for ax_idx, ax in enumerate(axes.ravel()):\n",
    "            i_figrow, i_figcol = ax_idx // n_figcols, ax_idx % n_figcols\n",
    "            tag_text = f'({\"abcd\"[i_figcol]}$_{{{i_figrow + 1}}}$)'\n",
    "            tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "    \n",
    "        for i_figrow, row_ttl in enumerate(['Latent t-SNE', 'Latent PCA', 'Mass t-SNE']):\n",
    "            print_axheader(axes[i_figrow, 0], row_ttl, 'left', \n",
    "                fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for i_figcol, col_ttl in enumerate(['Train', 'Test', 'Generated', 'All']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', \n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Making sure the legend labels have full alpha!\n",
    "    leg, = fig.legends\n",
    "    for handle in leg.legend_handles:\n",
    "        handle.set_alpha(1.0)\n",
    "\n",
    "    figpath1 = f'{workdir}/{i_fig:02d}_{arch}_mds.pdf'\n",
    "    fig.savefig(figpath1, bbox_inches='tight')\n",
    "    print(f'Finished writing {figpath1}')\n",
    "\n",
    "    figpath2 = f'{workdir}/{i_fig:02d}_{arch}_mds.png'\n",
    "    fig.savefig(figpath2, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {figpath2}')\n",
    "    i_fig += 1\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVAT Parameter Sweep Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recmpl_data = False\n",
    "\n",
    "fpidx_infos1, resdir, rioall = dict(), None, None\n",
    "if recmpl_data:\n",
    "    resdir = f'{results_dir}/02_adhoc'\n",
    "    for arch, fpidxall, n_seeds in [('mlp', '11_mlphist.*.*', 16), ('cnn', '12_cnnhist.*.*', 10)]:\n",
    "        rioarch = resio(fpidx=fpidxall, resdir=resdir, driver='sec2')\n",
    "        for fpidx in rioarch.rslv_fpidx(fpidxall):\n",
    "            fpidx_infos1[fpidx] = {'arch': arch, 'n_seeds': n_seeds, 'n_snrt': 1000,\n",
    "                'n_epoch': 2, 'n_rcns': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split, vrnts = 'test', ['orig', 'rcnst']\n",
    "\n",
    "stdfs_dict = defaultdict(list)\n",
    "fpidx_infos2 = dict()\n",
    "for i_fpidx, (fpidx, fpidx_info) in enumerate(fpidx_infos1.items()):\n",
    "    print(f'Working on {fpidx}.   ', end='')\n",
    "    if (i_fpidx + 1) % 5 == 0:\n",
    "        print('', flush=True)\n",
    "    rio2 = resio(fpidx=fpidx, resdir=resdir, driver='sec2')\n",
    "\n",
    "    n_seeds, n_epoch, i_epoch = fpidx_info['n_seeds'], fpidx_info['n_epoch'], -1\n",
    "    n_snrt, n_rcns = fpidx_info['n_snrt'], fpidx_info['n_rcns']\n",
    "\n",
    "    ##########################################################################################\n",
    "    ############################# Loading the Data From the Disk #############################\n",
    "    ##########################################################################################\n",
    "    ycols = ['m_chmprthst', 'n_prthst', 'ccn_cdf', 'qs_prt', 'qscs_prt', 'qs_pop',\n",
    "        'qa_prt', 'qacs_prt', 'qa_pop', 'frznfrac_tmp', 'logfrznfrac_tmp']\n",
    "\n",
    "    v_ydataraw = dict()\n",
    "    for ycol, vrnt in product(ycols, vrnts):\n",
    "        n_ychnls, n_ylen = ycol2dims[ycol]        \n",
    "        y_nparr = rio2(f'var/eval/raw/yaero:x:{ycol}/{split}/{vrnt}/pnts/data')\n",
    "        assert y_nparr.shape == (n_epoch * n_seeds, n_snrt, n_rcns, n_ychnls, n_ylen)\n",
    "        y_nparr2 = y_nparr.reshape(n_epoch, n_seeds, n_snrt, n_rcns, n_ychnls, n_ylen)\n",
    "        assert y_nparr2.shape == (n_epoch, n_seeds, n_snrt, n_rcns, n_ychnls, n_ylen)\n",
    "        v_ydataraw[f'{ycol}/{vrnt}'] = y_nparr2[i_epoch]\n",
    "\n",
    "\n",
    "    ##########################################################################################\n",
    "    ######################## Computing the Aerosol Diagnostic Metrics ########################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    ########### Data Cleaning and Unit Conversions ############\n",
    "    v_ydata, aero_cstscnv = cnvrt_physunits(v_ydataraw, aero_csts)\n",
    "\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=True)\n",
    "\n",
    "    ##########################################################################################\n",
    "    ################################### Compiling the Data ###################################\n",
    "    ##########################################################################################\n",
    "    df_cols = {col: val for col, val in v_mtrcdata.items() \n",
    "        if col.endswith('/err') and (val is not None)}\n",
    "\n",
    "    stdf1 = rio2('stat')\n",
    "    epoch_max = stdf1['epoch'].max()\n",
    "    idx_lastep = stdf1['epoch'] == epoch_max\n",
    "    stdf1 = stdf1[idx_lastep].reset_index(drop=True)\n",
    "    assert stdf1.shape[0] == n_seeds\n",
    "\n",
    "    hpdf1 = pd.DataFrame(rio2.load_key('hp'))\n",
    "    hpdf1.insert(0, 'fpidx', fpidx)\n",
    "    assert hpdf1.shape[0] == n_seeds\n",
    "\n",
    "    stdf2 = pd.DataFrame(df_cols)\n",
    "    stdf2.insert(0, 'fpidx', fpidx)\n",
    "    assert stdf2.shape[0] == n_seeds\n",
    "\n",
    "    stdf3 = pd.concat([stdf1, stdf2], axis=1)\n",
    "    fpidx_info2 = fpidx_info.copy()\n",
    "    fpidx_info2['hpdf'] = hpdf1\n",
    "    fpidx_info2['stdf'] = stdf3\n",
    "    fpidx_infos2[fpidx] = fpidx_info2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f'{workdir}/91_pltdata.h5'\n",
    "\n",
    "if recmpl_data:\n",
    "    arch2dfs = dict()\n",
    "    for fpidx, fpidx_info in fpidx_infos2.items():\n",
    "        arch = fpidx_info['arch']\n",
    "        hpdf3 = fpidx_info['hpdf'].copy(deep=True)\n",
    "        if 'fpidx' not in hpdf3.columns:\n",
    "            hpdf3.insert(0, 'fpidx', fpidx)\n",
    "        arch2dfs[f'{arch}:hp/{fpidx}'] = hpdf3\n",
    "        arch2dfs[f'{arch}:stat/{fpidx}'] = fpidx_info['stdf']\n",
    "\n",
    "    save_data = dict()\n",
    "    for archkey, fpidx2dfs in hie2deep(arch2dfs).items():\n",
    "        arch, key = archkey.split(':')\n",
    "        key_dfslst = list(fpidx2dfs.values())\n",
    "        key_df = pd.concat(key_dfslst, axis=0, ignore_index=True)\n",
    "        save_data[f'{arch}/{key}'] = key_df\n",
    "\n",
    "    save_h5datav2(save_data, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = load_h5data(cache_path)\n",
    "hpdf_aggs, stdf_aggs, ovat_grpngs = dict(), dict(), dict()\n",
    "for arch in ['mlp', 'cnn']:\n",
    "    hpdf_arch1 = load_data[f'{arch}/hp']\n",
    "    hpdf_arch1.insert(2, 'arch', arch)\n",
    "    stdf_arch1 = load_data[f'{arch}/stat']\n",
    "\n",
    "    assert (hpdf_arch1['fpidx'] == stdf_arch1['fpidx']).all()\n",
    "\n",
    "    # hpdf_arch1.insert(0, 'fpidx', stdf_arch1['fpidx'])\n",
    "    # hpdf_arch2 = hpdf_arch1.set_index('fpidx').loc[stdf_arch1['fpidx']].reset_index()\n",
    "    hpdf_arch2 = drop_unqcols(hpdf_arch1)\n",
    "    hpdf_arch2.insert(1, 'fpidxgrp', hpdf_arch2['fpidx'])\n",
    "    stdf_arch2 = stdf_arch1.drop(columns='fpidx')\n",
    "\n",
    "    # Aggregating the data\n",
    "    aggcfg = dict(type='bootstrap', n_boot=1000, q=[5, 95], stat='mean', device='cpu')\n",
    "    agg_data = get_aggdf(hpdf_arch2, stdf_arch2, xcol='epoch', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_archagg, stdf_archagg = agg_data['hpdf'], agg_data['stdf']\n",
    "\n",
    "    ovat_grps = get_ovatgrps(hpdf_archagg)\n",
    "    ovat_grpng = dict()\n",
    "    for ovat_grp in ovat_grps:\n",
    "        hpdf_grp1 = hpdf_archagg[hpdf_archagg['fpidx'].isin(ovat_grp)]\n",
    "        hpdf_grp2 = drop_unqcols(hpdf_grp1)\n",
    "        ovat_cols = hpdf_grp2.drop(columns=['fpidx', 'fpidxgrp']).columns.tolist()\n",
    "        ovat_col = ovat_cols[0]\n",
    "        ovat_grpng[ovat_col] = ovat_grp\n",
    "\n",
    "    # Creating combined performance metrics\n",
    "    ycol_fmt = 'perf/reconstruction/polyhst/test/orig/polyhst/test/rcnst/same/0/x/{u_node}/sqerr/mean/{stat}'\n",
    "    u_nodes = ['m_chmprtnrm', 'm_prthstmag', 'n_prthstnrm']\n",
    "    for stat in ['mean', 'low', 'high']:\n",
    "        ycols_agg = [ycol_fmt.format(u_node=u_node, stat=stat) for u_node in u_nodes]\n",
    "        stdf_archagg[f'rcnst_net/err/{stat}'] = stdf_archagg[ycols_agg].values.sum(axis=1)\n",
    "\n",
    "    for stat in ['mean', 'low', 'high']:\n",
    "        opt_errs = stdf_archagg[[f'qs_pop/err/{stat}', f'qa_pop/err/{stat}']]\n",
    "        stdf_archagg[f'qsa_pop/err/{stat}'] = opt_errs.values.mean(axis=1)\n",
    "\n",
    "    ycol_fmt = 'perf/urealism/urealsim/normal/genr/urealsim/test/orig/same/0/x/{u_node}/uslcwass:2/mean/{stat}'\n",
    "    for stat in ['mean', 'low', 'high']:\n",
    "        ycols_agg = [ycol_fmt.format(u_node=u_node, stat=stat) for u_node in u_nodes]\n",
    "        stdf_archagg[f'realism/err/{stat}'] = stdf_archagg[ycols_agg].values.sum(axis=1)\n",
    "    \n",
    "    hpdf_aggs[arch] = hpdf_archagg\n",
    "    stdf_aggs[arch] = stdf_archagg\n",
    "    ovat_grpngs[arch] = ovat_grpng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 92\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "x_pgcols = [\n",
    "    ['nn/enc/d_ltnt', 'cri/kl/sig/w', 'cri/kl/mu/w'],\n",
    "    ['nn/pp/magexp', 'nn/pp/nrmexp', 'nn/pp/cntexp']]\n",
    "y_cols = ['rcnst_net/err', 'realism/err', 'ccn_cdf/err', \n",
    "    'qsa_pop/err', 'logfrznfrac_tmp/err']\n",
    "\n",
    "hp_dflts = hpdf_aggs['mlp'].iloc[0].to_dict()\n",
    "\n",
    "figs_dict = dict()\n",
    "for i_figpage, x_cols in enumerate(x_pgcols):\n",
    "    fig, axes = None, None\n",
    "    n_figrows, n_figcols = len(y_cols), len(x_cols)\n",
    "    for i_figrow, y_col in enumerate(y_cols):\n",
    "        for i_figcol, x_col in enumerate(x_cols):\n",
    "            ax_idx = i_figrow * n_figcols + i_figcol\n",
    "\n",
    "            v_mpldatas = dict()\n",
    "            for arch in hpdf_aggs:\n",
    "                hpdf_agg = hpdf_aggs[arch]\n",
    "                stdf_agg = stdf_aggs[arch]\n",
    "                ovat_grpng = ovat_grpngs[arch]\n",
    "\n",
    "                # Collecting the data\n",
    "                if x_col not in ovat_grpng:\n",
    "                    continue\n",
    "                fpidxs_swp = ovat_grpng[x_col]\n",
    "                idx_swp = hpdf_agg['fpidx'].isin(fpidxs_swp)\n",
    "                hpdf_swp = hpdf_agg[idx_swp]\n",
    "                statdf_swp = stdf_agg[idx_swp]\n",
    "                hpdf_swp = hpdf_swp.sort_values(by=x_col)\n",
    "                statdf_swp = statdf_swp.loc[hpdf_swp.index]\n",
    "                hpdf_swp = hpdf_swp.reset_index(drop=True)\n",
    "                statdf_swp = statdf_swp.reset_index(drop=True)\n",
    "                hpstdf_swp = pd.concat([hpdf_swp, statdf_swp], axis=1)\n",
    "\n",
    "                v_mpldatas[f'{ax_idx}:{arch}:x'] = hpstdf_swp[x_col].values\n",
    "                v_mpldatas[f'{ax_idx}:{arch}:y/mean'] = hpstdf_swp[f'{y_col}/mean'].values\n",
    "                v_mpldatas[f'{ax_idx}:{arch}:y/low'] = hpstdf_swp[f'{y_col}/low'].values\n",
    "                v_mpldatas[f'{ax_idx}:{arch}:y/high'] = hpstdf_swp[f'{y_col}/high'].values\n",
    "\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            v_mplcfg = v_mplcfgs['paper.hpstudy.figure'].copy()\n",
    "            v_mplcfg.update(v_mplcfgs[f'paper.hpstudy.xaxis.{x_col.replace(\"/\", \".\")}'])\n",
    "            v_mplcfg.update(v_mplcfgs[f'paper.hpstudy.yaxis.{y_col.replace(\"/\", \".\")}'])\n",
    "\n",
    "            fig, axes = plot_mpl(data=v_mpldatas, fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfg)\n",
    "\n",
    "    akws = {'xycoords': 'axes fraction', 'textcoords': 'axes fraction', \n",
    "        'fontsize': 9, 'bbox/pad': 90, 'bbox/facecolor': 'none', 'bbox/edgecolor': 'none', \n",
    "        'arrowprops/arrowstyle': '->', 'arrowprops/connectionstyle': 'arc3,rad=0.3'}\n",
    "    annot_kws = dict()\n",
    "    annot_kws['mlp'] = {'text': 'MLP', 'xytext': [0.55, 0.4], \n",
    "        'xy': [0.4, 0.25], 'arrowprops/relpos': [0.0, 0.5], **akws}\n",
    "    annot_kws['cnn'] = {'text': 'CNN', 'xytext': [0.03, 0.03], \n",
    "        'xy': [0.4, 0.16], 'arrowprops/relpos': [1.0, 0.5], **akws}\n",
    "    annot_kws = hie2deep(deep2hie(annot_kws))\n",
    "\n",
    "    if i_figpage == 0:\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for lbl, antkws in annot_kws.items():\n",
    "                axes[1, 0].annotate(**antkws)\n",
    "\n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(n_figrows):\n",
    "            for i_figcol, tag_char in enumerate('abc'):\n",
    "                ax = axes[i_figrow, i_figcol]\n",
    "                tag_axis(ax, f'({tag_char}$_{{{i_figrow+1}}}$)', \n",
    "                    fontsize=10, pad=(0.3, 0.6))\n",
    "    \n",
    "    # Adding the base hp identification rectangle boxes\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(axes.shape[0]):\n",
    "            for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "                x_col = x_cols[i_figcol]\n",
    "                mark_rect(ax, x_boxdata=hp_dflts[x_col], y_boxdata=None, \n",
    "                    w_boxfrac=0.1, h_boxfrac=0.15, label='cnn', linewidth=1)\n",
    "\n",
    "    figs_dict[f'{i_figpage}'] = fig\n",
    "\n",
    "pdfpath = f'{workdir}/{i_fig:02d}_hpstudy.pdf'\n",
    "with PdfPages(pdfpath) as pdf:\n",
    "    for i_fig, fig3 in figs_dict.items():\n",
    "        pdf.savefig(figure=fig3, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OIN Mass Quantile Conditional Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generated OIN Histograms - Polished for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "v_mpldatasfrac, v_mpldatasabs = dict(), dict()\n",
    "vrnt_specs = {'null': (0, 'Traditional CVAE'), 'pow1': (1, 'Wasserstein-Regularized CVAE (Ours)')}\n",
    "i_savefigs = []\n",
    "\n",
    "acc_vrnts = dict()\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    if (split != 'normal') or not exprmnt.startswith('cond.mqnt.'):\n",
    "        continue\n",
    "    \n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/histcmprd']\n",
    "    i_savefigs.append(i_savefig)\n",
    "\n",
    "    if nicknm not in vrnt_specs:\n",
    "        continue\n",
    "    \n",
    "    chem_species = viz_data['chem_species']\n",
    "    n_rcns, i_oin = 1, chem_species.index('Dust')\n",
    "\n",
    "    for vrnt, mfraclo, mfrachi in [\n",
    "        ('gaus0', 0.00, 0.04),\n",
    "        ('gaus1', 0.04, 0.55), \n",
    "        ('gaus2', 0.55, 0.84), \n",
    "        ('gaus3', 0.84, 1.00)]:\n",
    "        v_ydata = viz_data['v_ydata']\n",
    "        v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "        n_bins2 = v_ynparr.shape[-1]\n",
    "        assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_bins2)\n",
    "\n",
    "        # The Dust absolute mass of generated samples\n",
    "        m_oinabs1 = v_ynparr.sum(-1)[:, :, 0, i_oin]\n",
    "        assert m_oinabs1.shape == (n_seeds, n_snrt)\n",
    "\n",
    "        m_oinabs2 = m_oinabs1[0]\n",
    "        assert m_oinabs2.shape == (n_snrt,)\n",
    "\n",
    "        # The OIN mass fraction of generated samples\n",
    "        m_chmfracs = v_ynparr.sum(-1, keepdims=True) / v_ynparr.sum((-1, -2), keepdims=True)\n",
    "        assert m_chmfracs.shape == (n_seeds, n_snrt, n_rcns, n_chem, 1)\n",
    "        m_oinfrac1 = m_chmfracs[:, :, 0, i_oin, 0]\n",
    "        assert m_oinfrac1.shape == (n_seeds, n_snrt)\n",
    "        m_oinfrac2 = m_oinfrac1[0]\n",
    "        assert m_oinfrac2.shape == (n_snrt,)\n",
    "\n",
    "        i_ax, ax_ttl = vrnt_specs[nicknm]\n",
    "        v_mpldatasabs[f'cond.mabshist.cmprd/{i_ax}:{vrnt}:y'] = m_oinabs1.ravel()\n",
    "        v_mpldatasfrac[f'cond.mfrachist.cmprd/{i_ax}:{vrnt}:y'] = m_oinfrac1.ravel()\n",
    "\n",
    "        acc_vrnt = np.logical_and(m_oinfrac1 >= mfraclo, m_oinfrac1 < mfrachi).mean(axis=1)\n",
    "        assert acc_vrnt.shape == (n_seeds,)\n",
    "\n",
    "        acc_vrnts[f'{viz_id}/{vrnt}'] = acc_vrnt\n",
    "        \n",
    "v_mpldatasabshie = hie2deep(v_mpldatasabs, maxdepth=1)\n",
    "v_mpldatasfrachie = hie2deep(v_mpldatasfrac, maxdepth=1)\n",
    "\n",
    "########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "fig, axes = None, None\n",
    "for ax_id in v_mpldatasfrachie:\n",
    "    fig, axes = plot_mpl(data=v_mpldatasfrachie[ax_id], fig=fig, \n",
    "        axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "# Adding the top left april tag for axis identification\n",
    "with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "    for i_figrow in range(axes.shape[0]):\n",
    "        for i_figcol in range(axes.shape[1]):\n",
    "            i_figax = i_figrow * axes.shape[1] + i_figcol\n",
    "            ax = axes[i_figrow, i_figcol]\n",
    "            tag_text = f'({\"ab\"[i_figcol]})'\n",
    "            tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.9))\n",
    "\n",
    "    for nicknm, (i_ax, ax_ttl) in vrnt_specs.items():\n",
    "        ax = axes.ravel()[i_ax]\n",
    "        print_axheader(ax, ax_ttl, 'top', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Adding the cutoff indicators    \n",
    "for ax in axes.ravel():\n",
    "    for x_cutoff in [0.04, 0.55, 0.84]:\n",
    "        ax.axvline(x=x_cutoff, lw=1, ls='--', c='black')\n",
    "\n",
    "assert len(i_savefigs) > 0\n",
    "i_savefig = min(i_savefigs)\n",
    "pdfpath = f'{figdir}/{i_savefig:02d}_frachistcmprd.pdf'\n",
    "# fig.savefig(pdfpath, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for viz_id, acc_vizvrnts in hie2deep(acc_vrnts).items():\n",
    "    acc_vizid = 0\n",
    "    for vrnt, w_vrnt in [('gaus0', 0.4), ('gaus1', 0.2), ('gaus2', 0.2), ('gaus3', 0.2)]:\n",
    "        acc_vizid = acc_vizid + acc_vizvrnts[vrnt] * w_vrnt\n",
    "    assert acc_vizid.shape == (n_seeds,)\n",
    "\n",
    "    acc_vizidmean = np.mean(acc_vizid)\n",
    "    acc_vizidse = 2.5 * np.std(acc_vizid) / np.sqrt(n_seeds)\n",
    "\n",
    "    print(f'Accuracy of {viz_id}: {acc_vizidmean*100:.2f} +/- {acc_vizidse*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generated Aerosol Population Diagnostic Plots - Polished for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "v_mplcfgs['m_chmprthst.genr.mqnt']['plt.subplots/nrows'] = 2\n",
    "v_mplcfgs['m_chmprthst.genr.mqnt']['plt.subplots/ncols'] = 4\n",
    "n_figrows = v_mplcfgs['m_chmprthst.genr.mqnt']['plt.subplots/nrows']\n",
    "n_figcols = v_mplcfgs['m_chmprthst.genr.mqnt']['plt.subplots/ncols']\n",
    "\n",
    "vrnt2lbl = {'gaus': 'Random', 'gaus0': 'Low Dust', 'gaus1': 'Med-Low Dust', \n",
    "    'gaus2': 'Med-High Dust', 'gaus3': 'High Dust'}\n",
    "vrnts = list(vrnt2lbl)[1:]\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "    chem_species = viz_data['chem_species']\n",
    "\n",
    "    if (split != 'normal') or not exprmnt.startswith('cond.mqnt.'):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/massnum']\n",
    "\n",
    "    # v_mplcfgs['ccn_cdf.genr.mqnt']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    # v_mplcfgs['frznfrac_tmp.genr.mqnt']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    i_sel = i_sel[:, :n_figcols]\n",
    "    n_page, n_pgsamps = i_sel.shape\n",
    "    assert n_pgsamps == n_figcols\n",
    "\n",
    "    figs_dict = dict()\n",
    "    for i_page in range(n_page):\n",
    "        v_mpldatas = dict()\n",
    "        i_samps = i_sel[i_page]\n",
    "        # The speciated mass data\n",
    "        for i_figcol, i_samp in enumerate(i_samps):\n",
    "            ax_idx = i_figcol\n",
    "            vrnt = vrnts[i_figcol]\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'm_chmprthst.genr.mqnt/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                i_rcns, n_rcns = 0, 1\n",
    "                v_ynparr1 = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                assert v_ynparr1.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                v_ynparr2 = v_ynparr1.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                v_mpldatas[f'm_chmprthst.genr.mqnt/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "        \n",
    "        # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "        ax_ixyspec = []\n",
    "        ax_ixyspec.append([1, d_histbinsum, 'n_prthst'    ])\n",
    "        # ax_ixyspec.append([2, eps_histbins, 'ccn_cdf'     ])\n",
    "        # ax_ixyspec.append([3, temprtr_bins, 'frznfrac_tmp'])\n",
    "        # ax_ixyspec.append([4, len_wvum,     'qs_pop'      ])\n",
    "        # ax_ixyspec.append([5, len_wvum,     'qa_pop'      ])\n",
    "        for i_figcol, i_samp in enumerate(i_samps):\n",
    "            for i_row, xvals, ycol in ax_ixyspec:\n",
    "                ax_idx = i_row * n_figcols + i_figcol\n",
    "                vrnt = vrnts[i_figcol]\n",
    "                v_mpldatas[f'{ycol}.genr.mqnt/{ax_idx}:genr:x'] = xvals\n",
    "                i_rcns, n_rcns = 0, 1\n",
    "                v_ynparr1 = v_ydata[f'{ycol}/{vrnt}']\n",
    "                assert v_ynparr1.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                v_ynparr2 = v_ynparr1.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr1.shape[3:])\n",
    "                assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                v_mpldatas[f'{ycol}.genr.mqnt/{ax_idx}:genr:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "        \n",
    "        v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "        ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "        fig, axes = None, None\n",
    "        for ax_id in v_mpldatashie:\n",
    "            fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "    \n",
    "        figs_dict[f'{i_page}/diag'] = fig\n",
    "\n",
    "        # Adding the top left april tag for axis identification\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for i_figrow in range(axes.shape[0]):\n",
    "                for i_figcol in range(axes.shape[1]):\n",
    "                    ax = axes[i_figrow, i_figcol]\n",
    "                    tag_text = f'({\"ab\"[i_figrow]}$_{{{i_figcol + 1}}}$)'\n",
    "                    tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.9))\n",
    "\n",
    "            for i_figcol in range(axes.shape[1]):\n",
    "                vrnt = vrnts[i_figcol]\n",
    "                ax_ttl = vrnt2lbl[vrnt]\n",
    "                print_axheader(axes[0, i_figcol], ax_ttl, 'top', \n",
    "                    fontsize=14, fontweight='bold', pad=3)\n",
    "\n",
    "        # Adding the small identification text box\n",
    "        i_figrow, i_oin = 0, chem_species.index('Dust')\n",
    "        for i_figcol, ax in enumerate(axes[i_figrow]):\n",
    "            i_samp = i_samps[i_figcol]\n",
    "            vrnt = vrnts[i_figcol]\n",
    "            v_ynparr1 = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "            assert v_ynparr1.shape == (n_seeds, n_snrt, 1, n_chem, n_binsum)\n",
    "            v_ynparr2 = v_ynparr1.reshape(n_seeds * n_snrt, n_chem, n_binsum)\n",
    "            assert v_ynparr2.shape == (n_seeds * n_snrt, n_chem, n_binsum)\n",
    "            v_ynparr3 = v_ynparr2[i_samp]\n",
    "            assert v_ynparr3.shape == (n_chem, n_binsum)\n",
    "            v_ynparr4 = v_ynparr3.sum(axis=-1)\n",
    "            assert v_ynparr4.shape == (n_chem,)\n",
    "            v_ynparr5 = v_ynparr4 / v_ynparr4.sum()\n",
    "            assert v_ynparr5.shape == (n_chem,)\n",
    "            textstr = f'{100*v_ynparr5[i_oin].item():.1f}' + '\\% Dust'\n",
    "            props = dict(facecolor='none', edgecolor='none')\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                usetex=True, verticalalignment='top', bbox=props)\n",
    "\n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    os.makedirs(figdir, exist_ok=True)\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "    with PdfPages(pdfpath) as pdf:\n",
    "        for i_page, fig_clctn in figs_dict.items():\n",
    "            pdf.savefig(figure=fig_clctn, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generated OIN Histograms (Legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    if (split != 'normal') or not exprmnt.startswith('cond.mqnt.'):\n",
    "        continue\n",
    "    \n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/hist']\n",
    "\n",
    "    v_mpldatas = dict()\n",
    "    n_rcns, i_oin = 1, chem_species.index('Dust')\n",
    "    for vrnt in ['gaus0', 'gaus1', 'gaus2', 'gaus3']:\n",
    "        v_ydata = viz_data['v_ydata']\n",
    "        v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "        n_bins2 = v_ynparr.shape[-1]\n",
    "        assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_bins2)\n",
    "\n",
    "        # The Dust absolute mass of generated samples\n",
    "        m_oinabs1 = v_ynparr.sum(-1)[:, :, 0, i_oin]\n",
    "        assert m_oinabs1.shape == (n_seeds, n_snrt)\n",
    "\n",
    "        m_oinabs2 = m_oinabs1[0]\n",
    "        assert m_oinabs2.shape == (n_snrt,)\n",
    "\n",
    "        # The OIN mass fraction of generated samples\n",
    "        m_chmfracs = v_ynparr.sum(-1, keepdims=True) / v_ynparr.sum((-1, -2), keepdims=True)\n",
    "        assert m_chmfracs.shape == (n_seeds, n_snrt, n_rcns, n_chem, 1)\n",
    "        m_oinfrac1 = m_chmfracs[:, :, 0, i_oin, 0]\n",
    "        assert m_oinfrac1.shape == (n_seeds, n_snrt)\n",
    "        m_oinfrac2 = m_oinfrac1[0]\n",
    "        assert m_oinfrac2.shape == (n_snrt,)\n",
    "\n",
    "        v_mpldatas[f'cond.mfrachist.legacy/0:{vrnt}:y'] = m_oinfrac1.ravel()\n",
    "        v_mpldatas[f'cond.mabshist/1:{vrnt}:y'] = m_oinabs1.ravel()\n",
    "        \n",
    "    v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "    ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "    fig, axes = None, None\n",
    "    for ax_id in v_mpldatashie:\n",
    "        fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "            axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_hist.pdf'\n",
    "    fig.savefig(pdfpath, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generated Aerosol Population Diagnostic Plots (Legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "n_figrows = v_mplcfgs['m_chmprthst.genr']['plt.subplots/nrows']\n",
    "v_mplcfgs['m_chmprthst.genr']['plt.subplots/ncols'] = 5\n",
    "n_figcols = v_mplcfgs['m_chmprthst.genr']['plt.subplots/ncols']\n",
    "\n",
    "vrnt2lbl = {'gaus': 'Random', 'gaus0': 'Low OIN', 'gaus1': 'Med-Low OIN', \n",
    "    'gaus2': 'Med-High OIN', 'gaus3': 'High OIN'}\n",
    "vrnts = list(vrnt2lbl)\n",
    "\n",
    "for viz_id, viz_data in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/diag']\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.genr']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.genr']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "\n",
    "    if (split != 'normal') or not exprmnt.startswith('cond.mqnt.'):\n",
    "        continue\n",
    "    \n",
    "    n_page, n_pgsamps = i_sel.shape\n",
    "    assert n_pgsamps == n_figcols\n",
    "\n",
    "    figs_dict = dict()\n",
    "    for i_page in range(n_page):\n",
    "        v_mpldatas = dict()\n",
    "        i_samps = i_sel[i_page]\n",
    "        # The speciated mass data\n",
    "        for i_figcol, i_samp in enumerate(i_samps):\n",
    "            ax_idx = i_figcol\n",
    "            vrnt = vrnts[i_figcol]\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'm_chmprthst.genr/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                i_rcns, n_rcns = 0, 1\n",
    "                v_ynparr1 = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                assert v_ynparr1.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                v_ynparr2 = v_ynparr1.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                v_mpldatas[f'm_chmprthst.genr/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "        \n",
    "        # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "        ax_ixyspec = []\n",
    "        ax_ixyspec.append([1, d_histbinsum, 'n_prthst'    ])\n",
    "        ax_ixyspec.append([2, eps_histbins, 'ccn_cdf'     ])\n",
    "        ax_ixyspec.append([3, temprtr_bins, 'frznfrac_tmp'])\n",
    "        ax_ixyspec.append([4, len_wvum,     'qs_pop'      ])\n",
    "        ax_ixyspec.append([5, len_wvum,     'qa_pop'      ])\n",
    "        for i_figcol, i_samp in enumerate(i_samps):\n",
    "            for i_row, xvals, ycol in ax_ixyspec:\n",
    "                ax_idx = i_row * n_figcols + i_figcol\n",
    "                vrnt = vrnts[i_figcol]\n",
    "                v_mpldatas[f'{ycol}.genr/{ax_idx}:genr:x'] = xvals\n",
    "                i_rcns, n_rcns = 0, 1\n",
    "                v_ynparr1 = v_ydata[f'{ycol}/{vrnt}']\n",
    "                assert v_ynparr1.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                v_ynparr2 = v_ynparr1.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr1.shape[3:])\n",
    "                assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                v_mpldatas[f'{ycol}.genr/{ax_idx}:genr:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "        \n",
    "        v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "        ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "        fig, axes = None, None\n",
    "        for ax_id in v_mpldatashie:\n",
    "            fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "    \n",
    "        figs_dict[f'{i_page}/diag'] = fig\n",
    "\n",
    "        # Adding the small identification text box\n",
    "        axes1d = axes.ravel()\n",
    "        for i_figcol, i_samp in enumerate(i_samps):\n",
    "            ax = axes[0, i_figcol]\n",
    "            vrnt = vrnts[i_figcol]\n",
    "            textstr = vrnt2lbl[vrnt]\n",
    "            props = dict(facecolor='none', edgecolor='none')\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                usetex=True, verticalalignment='top', bbox=props)\n",
    "\n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    os.makedirs(figdir, exist_ok=True)\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "    with PdfPages(pdfpath) as pdf:\n",
    "        for i_page, fig_clctn in figs_dict.items():\n",
    "            pdf.savefig(figure=fig_clctn, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Data For MDS Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizsct_datas = dict()\n",
    "\n",
    "for exprm_id, exprm_info in hie2deep(exprm_infos).items():\n",
    "    (exprmnt, arch) = exprm_id.split(':')\n",
    "    fpidx = exprm_info['fpidx']\n",
    "    resdir = exprm_info['resdir']\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    \n",
    "    if fpidx is None:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        rio = resio(fpidx=fpidx, resdir=resdir)\n",
    "        rio_dtypes = rio.dtypes()\n",
    "    except Exception as exc:\n",
    "        print(f'There was an error opening {fpidx}. I will move on.')\n",
    "        continue\n",
    "\n",
    "    rio_keys = [key.split(':', 1)[-1] for key in rio_dtypes]\n",
    "\n",
    "    v_mdsdataraw = dict()\n",
    "    for key in rio_keys:\n",
    "        if not any(\n",
    "            key.startswith(f'var/eval/mds/{eid}:') or \n",
    "            key.startswith(f'var/eval/raw/{eid}:y:y/') \n",
    "            for eid in ['identity', 'polyhstbal']):\n",
    "            continue\n",
    "        val = rio(key)\n",
    "        n_pnts, n_rcns, *d_repr = val.shape[1:]\n",
    "        assert n_rcns == 1\n",
    "        assert val.shape == (n_epoch * n_seeds, n_pnts, n_rcns, *d_repr)\n",
    "        val_lastepoch = val.reshape(n_epoch, n_seeds, n_pnts, *d_repr)[-1]\n",
    "        assert val_lastepoch.shape == (n_seeds, n_pnts, *d_repr)\n",
    "        v_mdsdataraw[key] = val_lastepoch\n",
    "\n",
    "    # Example:\n",
    "    #   v_mdsdataraw = {\n",
    "    #       'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/normal/genr/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/test/orig/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/test/rcnst/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/train/orig/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/train/rcnst/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/normal/genr/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/mu/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/phi/data': np.randn(10, 5000, 1),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/test/orig/sig/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/test/rcnst/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/mu/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/phi/data': np.randn(10, 5000, 1),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/train/orig/sig/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1zpcalr:z:z/train/rcnst/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1ztsne:z:z/normal/genr/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1ztsne:z:z/test/orig/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1ztsne:z:z/test/rcnst/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1ztsne:z:z/train/orig/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/mds/identity:1x1ztsne:z:z/train/rcnst/pnts/data': np.randn(10, 5000, 2),\n",
    "    #       'var/eval/raw/identity:y:y/normal/genr/pnts/data': np.randn(10, 5000, 1, 4),\n",
    "    #       'var/eval/raw/identity:y:y/test/orig/pnts/data': np.randn(10, 5000, 1, 4),\n",
    "    #       'var/eval/raw/identity:y:y/test/rcnst/pnts/data': np.randn(10, 5000, 1, 4),\n",
    "    #       'var/eval/raw/identity:y:y/train/orig/pnts/data': np.randn(10, 5000, 1, 4),\n",
    "    #       'var/eval/raw/identity:y:y/train/rcnst/pnts/data': np.randn(10, 5000, 1, 4),\n",
    "    #   }\n",
    "\n",
    "    vizsct_datas[f'{exprmnt}:{arch}/v_mdsdataraw'] = v_mdsdataraw\n",
    "\n",
    "    pats_rnm1 = {\n",
    "        'var/eval/raw/identity:y:y/{split}/{vrnt}/pnts/data': \n",
    "        '{split}/{vrnt}'}\n",
    "    y_data = get_subdictrnmd(v_mdsdataraw, pats_rnm1)\n",
    "\n",
    "    for splitvrnt in y_data:\n",
    "        split, vrnt = splitvrnt.split('/')\n",
    "        # The one-hot class labels\n",
    "        y_ohpnts1 = y_data[f'{split}/{vrnt}']\n",
    "        n_pnts, n_class = y_ohpnts1.shape[1], y_ohpnts1.shape[-1]\n",
    "        assert y_ohpnts1.shape == (n_seeds, n_pnts, 1, n_class)\n",
    "\n",
    "        # The one-hot class labels for each point\n",
    "        y_ohpnts = y_ohpnts1.reshape(n_seeds, n_pnts, n_class)\n",
    "        assert y_ohpnts.shape == (n_seeds, n_pnts, n_class)\n",
    "\n",
    "        # Making sure the one-hot labels are actually one-hot\n",
    "        assert np.logical_or(y_ohpnts == 0, y_ohpnts == 1).all()\n",
    "        assert (y_ohpnts.sum(-1) == 1).all()\n",
    "\n",
    "        # Converting the one-hot labels to class indices for every point\n",
    "        y_pntscls = y_ohpnts.argmax(-1)\n",
    "        assert y_pntscls.shape == (n_seeds, n_pnts)\n",
    "\n",
    "        # The fraction of points in each class\n",
    "        y_clsfrac = (y_pntscls[..., None] == np.arange(n_class)[None, None]).mean(-2)\n",
    "        assert y_clsfrac.shape == (n_seeds, n_class)\n",
    "        assert np.allclose(y_clsfrac.sum(-1), 1)\n",
    "\n",
    "        vizsct_datas[f'{exprmnt}:{arch}/{split}/{vrnt}/n_seeds'] = n_seeds\n",
    "        vizsct_datas[f'{exprmnt}:{arch}/{split}/{vrnt}/n_pnts'] = n_pnts\n",
    "        vizsct_datas[f'{exprmnt}:{arch}/{split}/{vrnt}/n_class'] = n_class\n",
    "        vizsct_datas[f'{exprmnt}:{arch}/{split}/{vrnt}/y_pntscls'] = y_pntscls\n",
    "        vizsct_datas[f'{exprmnt}:{arch}/{split}/{vrnt}/y_clsfrac'] = y_clsfrac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Class-Specific MDS Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "n_figrows = v_mplcfgs['cond.ztsne']['plt.subplots/nrows']\n",
    "n_figcols = v_mplcfgs['cond.ztsne']['plt.subplots/ncols']\n",
    "\n",
    "for exprmntarch, viz_data in hie2deep(vizsct_datas, maxdepth=1).items():\n",
    "    exprmnt, arch = exprmntarch.split(':')\n",
    "    v_mdsdataraw = viz_data['v_mdsdataraw']\n",
    "    if not exprmnt.startswith('cond.mqnt.'):\n",
    "        continue\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/mdscls']\n",
    "\n",
    "    figs_dict = dict()\n",
    "    for i_page, (split, vrnt) in enumerate([('train', 'orig'), ('test', 'orig'), ('normal', 'gaus')]):\n",
    "        n_pnts = viz_data[f'{split}/{vrnt}/n_pnts']\n",
    "        n_class = viz_data[f'{split}/{vrnt}/n_class']\n",
    "        y_pntscls = viz_data[f'{split}/{vrnt}/y_pntscls']\n",
    "        y_clsfrac = viz_data[f'{split}/{vrnt}/y_clsfrac']\n",
    "        assert n_figcols == (n_class + 1)\n",
    "        \n",
    "        ########################### Collecting the plotting data ############################\n",
    "        ax_idx = 0\n",
    "        v_mpldatas1 = dict()\n",
    "        for mplid, vid in [\n",
    "            ('cond.ztsne', '1x1ztsne:z:z'), \n",
    "            ('cond.zpca', '1x1zpcalr:z:z'), \n",
    "            ('cond.xtsne', '1x1xtsne:x:m_chmprthst')]:\n",
    "            ###### The Z TSNE individual-class data ######\n",
    "            for i_class in range(n_class):\n",
    "                pats_rnm2 = {\n",
    "                    f'var/eval/mds/identity:{vid}/{split}/{vrnt}/{{vrepr}}/data': \n",
    "                    f'{mplid}/{ax_idx}:{split}/class/{i_class}:{{vrepr}}'}\n",
    "                i_seed = 0\n",
    "                ax_data1 = get_subdictrnmd(v_mdsdataraw, pats_rnm2)\n",
    "                ax_data = {key: val[i_seed, y_pntscls[i_seed] == i_class] \n",
    "                    for key, val in ax_data1.items()}\n",
    "                v_mpldatas1.update(ax_data)\n",
    "                ax_idx += 1\n",
    "\n",
    "            ###### The Z TSNE combined-class data ######\n",
    "            pats_rnm1 = {\n",
    "                f'{mplid}/{{ax_idx}}:{{split}}/class/{{i_class}}:{{vrepr}}': \n",
    "                f'{mplid}/{ax_idx}:{{split}}/class/{{i_class}}:{{vrepr}}'}\n",
    "            ax_data = get_subdictrnmd(v_mpldatas1, pats_rnm1)\n",
    "            v_mpldatas1.update(ax_data)\n",
    "            ax_idx += 1\n",
    "\n",
    "        # Splitting the various plotting configs\n",
    "        v_mpldatas2 = hie2deep(v_mpldatas1, maxdepth=1)\n",
    "\n",
    "        ################### Calling Matplotlib to Plot the Scatter Points ################### \n",
    "        fig, axes = None, None\n",
    "        for ax_id in v_mpldatas2:\n",
    "            fig, axes = plot_mpl(data=v_mpldatas2[ax_id], fig=fig, \n",
    "                axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "        # Adding the top left april tag for axis identification\n",
    "        n_figrows, n_figcols = axes.shape\n",
    "        with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "            for ax_idx, ax in enumerate(axes.ravel()):\n",
    "                i_figrow, i_figcol = ax_idx // n_figcols, ax_idx % n_figcols\n",
    "                tag_text = f'({\"abcde\"[i_figcol]}$_{{{i_figrow + 1}}}$)'\n",
    "                tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "        \n",
    "            for i_figrow, row_ttl in enumerate(['Latent t-SNE', 'Latent PCA', 'Mass t-SNE']):\n",
    "                print_axheader(axes[i_figrow, 0], row_ttl, 'left', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "            \n",
    "            lblnames = ['Low Dust', 'Med-Low Dust', 'Med-High Dust', 'High Dust', 'All']\n",
    "            split_frml = {'train': 'Train', 'test': 'Test', 'normal': 'Generated'}[split]\n",
    "            for i_figcol, col_ttl in enumerate(lblnames):\n",
    "                print_axheader(axes[0, i_figcol], f'{col_ttl}\\n({split_frml})', 'top', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        figs_dict[f'{i_page}/{split}/{vrnt}'] = fig\n",
    "\n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_mds.pdf'\n",
    "    with PdfPages(pdfpath) as pdf:\n",
    "        for fig_id, fig in figs_dict.items():\n",
    "            pdf.savefig(figure=fig, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "\n",
    "    for fig_id, fig in figs_dict.items():\n",
    "        i_page, split, vrnt = fig_id.split('/')\n",
    "        pngpath = pdfpath.replace('.pdf', f'_{split}.png')\n",
    "        fig.savefig(pngpath, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {pngpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Split-Specific MDS Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "    \n",
    "figs_dict = dict()\n",
    "for exprmntarch, viz_data in hie2deep(vizsct_datas, maxdepth=1).items():\n",
    "    exprmnt, arch = exprmntarch.split(':')\n",
    "    v_mdsdataraw = viz_data['v_mdsdataraw']\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/mdssplt']\n",
    "    \n",
    "    ########################### Collecting the plotting data ############################\n",
    "    v_mpldatas = dict()\n",
    "    nrmvrnt = 'gaus' if ('var/eval/mds/identity:1x1ztsne:z:z/normal/gaus/pnts/data' in v_mdsdataraw) else 'genr'\n",
    "\n",
    "    ###### The Z TSNE individual-split data ######\n",
    "    for ax_idx, split, vrnt in [(0, 'train', 'orig'), (1, 'test', 'orig'), (2, 'normal', nrmvrnt)]:\n",
    "        pats_rnm2 = {\n",
    "            f'var/eval/mds/identity:1x1ztsne:z:z/{split}/{vrnt}/pnts/data': \n",
    "            f'ztsne/{ax_idx}:{split}/{vrnt}:pnts'}\n",
    "        ax_data = get_subdictrnmd(v_mdsdataraw, pats_rnm2)\n",
    "        v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The Z TSNE combined-splits data ######\n",
    "    ax_idx = 3\n",
    "    pats_rnm1 = {\n",
    "        'ztsne/{ax_idx}:{split}/{vrnt}:{vrepr}': \n",
    "        f'ztsne/{ax_idx}:{{split}}/{{vrnt}}:{{vrepr}}'}\n",
    "    ax_data = get_subdictrnmd(v_mpldatas, pats_rnm1)\n",
    "    v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The Z PCA individual-split data ######\n",
    "    for ax_idx, split, vrnt in [(4, 'train', 'orig'), (5, 'test', 'orig'), (6, 'normal', nrmvrnt)]:\n",
    "        pats_rnm4 = {\n",
    "            f'var/eval/mds/identity:1x1zpcalr:z:z/{split}/{vrnt}/{{vrepr}}/data':\n",
    "            f'zpca/{ax_idx}:{split}/{vrnt}:{{vrepr}}'}\n",
    "        ax_data = get_subdictrnmd(v_mdsdataraw, pats_rnm4)\n",
    "        v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The Z PCA combined-splits data ######\n",
    "    ax_idx = 7\n",
    "    pats_rnm3 = {\n",
    "        'zpca/{ax_idx}:{split}/{vrnt}:{vrepr}': \n",
    "        f'zpca/{ax_idx}:{{split}}/{{vrnt}}:{{vrepr}}'}\n",
    "    ax_data = get_subdictrnmd(v_mpldatas, pats_rnm3)\n",
    "    v_mpldatas.update(ax_data)\n",
    "\n",
    "    ##### The X TSNE individual-split data #####\n",
    "    for ax_idx, split, vrnt in [(8, 'train', 'orig'), (8, 'train', 'rcnst'), \n",
    "        (9, 'test', 'orig'), (9, 'test', 'rcnst'), (10, 'normal', nrmvrnt)]:\n",
    "        pats_rnm6 = {\n",
    "            f'var/eval/mds/identity:1x1xtsne:x:m_chmprthst/{split}/{vrnt}/pnts/data': \n",
    "            f'xtsne/{ax_idx}:{split}/{vrnt}:pnts'}\n",
    "        ax_data = get_subdictrnmd(v_mdsdataraw, pats_rnm6)\n",
    "        v_mpldatas.update(ax_data)\n",
    "\n",
    "    ###### The X TSNE combined-splits data ######\n",
    "    ax_idx = 11\n",
    "    pats_rnm5 = {\n",
    "        'xtsne/{ax_idx}:{split}/{vrnt}:{vrepr}': \n",
    "        f'xtsne/{ax_idx}:{{split}}/{{vrnt}}:{{vrepr}}'}\n",
    "    ax_data = get_subdictrnmd(v_mpldatas, pats_rnm5)\n",
    "    v_mpldatas.update(ax_data)\n",
    "\n",
    "    # Restricting the data to the first seed\n",
    "    v_mpldatas2 = {key: val[0] for key, val in v_mpldatas.items()}\n",
    "\n",
    "    # Splitting the various plotting configs\n",
    "    v_mpldatas3 = hie2deep(v_mpldatas2, maxdepth=1)\n",
    "\n",
    "    ################### Calling Matplotlib to Plot the Scatter Points ################### \n",
    "    fig, axes = None, None\n",
    "    for ax_id in v_mpldatas3:\n",
    "        fig, axes = plot_mpl(data=v_mpldatas3[ax_id], fig=fig, \n",
    "            axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "    # Making sure the legend labels have full alpha!\n",
    "    leg, = fig.legends\n",
    "    for handle in leg.legend_handles:\n",
    "        handle.set_alpha(1.0)\n",
    "\n",
    "    # Adding the top left april tag for axis identification\n",
    "    n_figrows, n_figcols = axes.shape\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for ax_idx, ax in enumerate(axes.ravel()):\n",
    "            i_figrow, i_figcol = ax_idx // n_figcols, ax_idx % n_figcols\n",
    "            tag_text = f'({\"abcd\"[i_figcol]}$_{{{i_figrow + 1}}}$)'\n",
    "            tag_axis(ax, tag_text, fontsize=11, pad=(0.3, 0.6))\n",
    "    \n",
    "        for i_figrow, row_ttl in enumerate(['Latent t-SNE', 'Latent PCA', 'Mass t-SNE']):\n",
    "            print_axheader(axes[i_figrow, 0], row_ttl, 'left', \n",
    "                fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for i_figcol, col_ttl in enumerate(['Train', 'Test', 'Generated', 'All']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', \n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "    break\n",
    "\n",
    "    _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "    figpath1 = f'{figdir}/{i_savefig:02d}{_nicknm}_mds.pdf'\n",
    "    fig.savefig(figpath1, bbox_inches='tight')\n",
    "    print(f'Finished writing {figpath1}')\n",
    "\n",
    "    figpath2 = figpath1.replace('.pdf', '.png')\n",
    "    fig.savefig(figpath2, dpi=200, bbox_inches='tight')\n",
    "    print(f'Finished writing {figpath2}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-Label Condtional Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generated Aerosol Population Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "for (viz_id, viz_data) in hie2deep(viz_datas, maxdepth=1).items():\n",
    "    exprmnt, arch, split = viz_id.split(':')\n",
    "    v_ydata = viz_data['v_ydata']\n",
    "    d_histbinsum = viz_data['d_histbins']\n",
    "    n_binsum = viz_data['n_bins']\n",
    "    len_wvum = viz_data['len_wv']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    eps_histmin = viz_data['eps_histmin']\n",
    "    eps_histmax = viz_data['eps_histmax']\n",
    "    eps_histbins = viz_data['eps_histbins']\n",
    "    tmprtr_inpmin = viz_data['tmprtr_inpmin']\n",
    "    tmprtr_inpmax = viz_data['tmprtr_inpmax']\n",
    "    temprtr_bins = viz_data['temprtr_bins']\n",
    "    i_sel = viz_data['i_sel']\n",
    "    e_samps = viz_data['e_samps']\n",
    "    n_files, n_page = i_sel.shape\n",
    "\n",
    "    exprm_info = get_subdict(exprm_infos, f'{exprmnt}:{arch}')\n",
    "    n_seeds = exprm_info['n_seeds']\n",
    "    n_snrt = exprm_info['n_snrt']\n",
    "    nicknm = exprm_info['nicknm']\n",
    "    figdir = exprm_info['figdir']\n",
    "    i_savefig = exprm_info['i_fig/diag']\n",
    "\n",
    "    if (not exprmnt.startswith('cond.cont.')) or (split not in ('test',)):\n",
    "        continue\n",
    "\n",
    "    vrnt_specs = exprm_info.get('vrnt_specs', None)\n",
    "    if vrnt_specs is None:\n",
    "        vrnt_specs = [\n",
    "            ('orig', 1, 0, 'Original'), ('rcnst', 1, 0, 'Reconst' ), \n",
    "            ('yknn', 5, 0, '1st LNN' ), ('yknn',  5, 1, '1st LNN' ),\n",
    "            ('znrm', 5, 0, 'Norm Lat'), ('znrm',  5, 1, 'Norm Lat')]\n",
    "    n_figcols = len(vrnt_specs)\n",
    "    n_figrows = v_mplcfgs['m_chmprthst.cndrcn']['plt.subplots/nrows']\n",
    "    v_mplcfgs['m_chmprthst.cndrcn']['plt.subplots/ncols'] = n_figcols\n",
    "    assert len(vrnt_specs) == n_figcols\n",
    "\n",
    "    v_mplcfgs['ccn_cdf.cndrcn']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "    v_mplcfgs['frznfrac_tmp.cndrcn']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "    \n",
    "    for i_file in range(n_files):\n",
    "        figs_dict = dict()\n",
    "        for i_page in range(n_page):\n",
    "            v_mpldatas = dict()\n",
    "            i_samp = i_sel[i_file, i_page]\n",
    "            \n",
    "            # The speciated mass data\n",
    "            i_row = 0\n",
    "            for i_col in range(n_figcols):\n",
    "                vrnt, n_rcns, i_rcns, ttl = vrnt_specs[i_col]\n",
    "                for i_chem, chem in enumerate(chem_species):\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    v_mpldatas[f'm_chmprthst.cndrcn/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                    v_ynparr = v_ydata[f'm_chmprthst/{vrnt}']\n",
    "                    assert v_ynparr.shape == (n_seeds, n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    assert v_ynparr2.shape == (n_seeds * n_snrt, n_rcns, n_chem, n_binsum)\n",
    "                    v_mpldatas[f'm_chmprthst.cndrcn/{ax_idx}:{chem}:y'] = v_ynparr2[i_samp, i_rcns, i_chem]\n",
    "            \n",
    "            # The total mass, particle count, ccn, frozen fraction, and optical cross-section data\n",
    "            ax_ixyspec = []\n",
    "            ax_ixyspec.append([1, d_histbinsum, 'n_prthst'    ])\n",
    "            ax_ixyspec.append([2, eps_histbins, 'ccn_cdf'     ])\n",
    "            ax_ixyspec.append([3, temprtr_bins, 'frznfrac_tmp'])\n",
    "            ax_ixyspec.append([4, len_wvum,     'qs_pop'      ])\n",
    "            ax_ixyspec.append([5, len_wvum,     'qa_pop'      ])\n",
    "            for i_col in range(n_figcols):\n",
    "                for i_row, xvals, ycol in ax_ixyspec:\n",
    "                    ax_idx = i_row * n_figcols + i_col\n",
    "                    vrnt, n_rcns, i_rcns, ttl = vrnt_specs[i_col]\n",
    "                    v_mpldatas[f'{ycol}.cndrcn/{ax_idx}:cndrcn:x'] = xvals\n",
    "                    v_ynparr = v_ydata[f'{ycol}/{vrnt}']\n",
    "                    assert v_ynparr.shape[:3] == (n_seeds, n_snrt, n_rcns)\n",
    "                    v_ynparr2 = v_ynparr.reshape(n_seeds * n_snrt, n_rcns, *v_ynparr.shape[3:])\n",
    "                    assert v_ynparr2.shape[:2] == (n_seeds * n_snrt, n_rcns)\n",
    "                    v_mpldatas[f'{ycol}.cndrcn/{ax_idx}:cndrcn:y'] = np.squeeze(v_ynparr2[i_samp, i_rcns])\n",
    "            \n",
    "            v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "            ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "            fig, axes = None, None\n",
    "            for ax_id in v_mpldatashie:\n",
    "                fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "                    axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "\n",
    "            figs_dict[f'{i_page}/diag'] = fig\n",
    "\n",
    "            # Adding the small identification text box\n",
    "            axes1d = axes.ravel()\n",
    "            for i_col in range(n_figcols):\n",
    "                ax = axes[0, i_col]\n",
    "                vrnt, n_rcns, i_rcns, textstr = vrnt_specs[i_col]\n",
    "                props = dict(facecolor='none', edgecolor='none')\n",
    "                ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    usetex=True, verticalalignment='top', bbox=props)\n",
    "\n",
    "        _nicknm = '' if nicknm is None else f'_{nicknm}'\n",
    "        pdfpath = f'{figdir}/{i_savefig:02d}{_nicknm}_anec.pdf'\n",
    "        with PdfPages(pdfpath) as pdf:\n",
    "            for i_page, fig_clctn in figs_dict.items():\n",
    "                pdf.savefig(figure=fig_clctn, bbox_inches='tight')\n",
    "        print(f'Finished writing {pdfpath}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Compliance vs Label Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdir = f'{results_dir}/02_adhoc'\n",
    "rio = resio(fpidx='02_adhoc/22_mlpcnd.*.*', resdir=results_dir, driver='sec2')\n",
    "\n",
    "stdf1 = rio('stat')\n",
    "epoch_max = stdf1['epoch'].max()\n",
    "idx_lastep = stdf1['epoch'] == epoch_max\n",
    "stdf2 = stdf1[idx_lastep].reset_index(drop=True)\n",
    "\n",
    "hpdf1, hpdf_info = rio('hp', ret_info=True)\n",
    "hpdf2 = hpdf1[idx_lastep].reset_index(drop=True)\n",
    "hpdf2.insert(0, 'fpidx', hpdf_info[idx_lastep].reset_index(drop=True)['fpidx'])\n",
    "hpdf2.insert(1, 'fpidxgrp', hpdf2['fpidx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the data\n",
    "aggcfg = dict(type='bootstrap', n_boot=1000, q=[5, 95], stat='mean', device='cpu')\n",
    "agg_data = get_aggdf(hpdf2, stdf2, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "hpdf_agg, stdf_agg = agg_data['hpdf'], agg_data['stdf']\n",
    "hpdf_agg = drop_unqcols(hpdf_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating combined performance metrics\n",
    "ycol_rnmngs = {\n",
    "    'perf/compliance1/polyhst/normal/genr/polyhst/normal/genr.inp/same/0/y/y/abserr/mean/{stat}': 'cmplnc/encd/{stat}',\n",
    "    'perf/compliance2/tricmplnc/test/znrm/tricmplnc/test/orig/same/0/x/yvec/abserr/mean/{stat}':  'cmplnc/znrm/{stat}',\n",
    "    'perf/compliance2/tricmplnc/test/yknn/tricmplnc/test/orig/same/0/x/yvec/abserr/mean/{stat}':  'cmplnc/yknn/{stat}',\n",
    "    'perf/compliance2/tricmplnc/test/rcnst/tricmplnc/test/orig/same/0/x/yvec/abserr/mean/{stat}': 'cmplnc/rcnst/{stat}'}\n",
    "\n",
    "for ycolinp_fmt, ycolout_fmt in ycol_rnmngs.items():\n",
    "    for stat in ['mean', 'low', 'high']:\n",
    "        ycol_agginp = ycolinp_fmt.format(stat=stat)\n",
    "        ycol_aggout = ycolout_fmt.format(stat=stat)\n",
    "        stdf_agg[ycol_aggout] = stdf_agg[ycol_agginp]\n",
    "\n",
    "hpstdf_agg = pd.concat([hpdf_agg, stdf_agg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 10\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "huecol = 'cri/indzy/w'\n",
    "x_col = 'nn/enc/n_lenlbl'\n",
    "pltdf = hpstdf_agg.sort_values(by=[huecol, x_col])\n",
    "\n",
    "fig, axes = None, None\n",
    "for i_figcol, y_col in enumerate(['cmplnc/encd', 'cmplnc/znrm', 'cmplnc/yknn', 'cmplnc/rcnst']):\n",
    "    v_mpldatas = dict()\n",
    "    for hueval, huedf in pltdf.groupby(huecol):\n",
    "        v_mpldatas[f'{i_figcol}:{hueval}:x'] = huedf[x_col].values\n",
    "        v_mpldatas[f'{i_figcol}:{hueval}:y/mean'] = huedf[f'{y_col}/mean'].values\n",
    "        v_mpldatas[f'{i_figcol}:{hueval}:y/low'] = huedf[f'{y_col}/low'].values\n",
    "        v_mpldatas[f'{i_figcol}:{hueval}:y/high'] = huedf[f'{y_col}/high'].values\n",
    "    \n",
    "    v_mplcfg = v_mplcfgs[y_col.replace('/', '.')].copy()\n",
    "    fig, axes = plot_mpl(data=v_mpldatas, fig=fig, \n",
    "        axes=axes, mplopts=v_mplcfg)\n",
    "\n",
    "akws = {'xycoords': 'axes fraction', 'textcoords': 'axes fraction', \n",
    "    'fontsize': 10, 'bbox/pad': 90, 'bbox/facecolor': 'none', 'bbox/edgecolor': 'none', \n",
    "    'arrowprops/arrowstyle': '->', 'arrowprops/connectionstyle': 'arc3,rad=0.2'}\n",
    "annot_kws = dict()\n",
    "annot_kws['depzy'] = {'text': 'High Dependence', 'xytext': [0.4, 0.88], \n",
    "    'xy': [0.2, 0.82], 'arrowprops/relpos': [0.0, 0.5], **akws}\n",
    "annot_kws['indzy'] = {'text': 'Low Dependence', 'xytext': [0.05, 0.05], \n",
    "    'xy': [0.85, 0.14], 'arrowprops/relpos': [1.0, 0.5], **akws}\n",
    "annot_kws = hie2deep(deep2hie(annot_kws))\n",
    "\n",
    "with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "    for lbl, antkws in annot_kws.items():\n",
    "        axes[0, 0].annotate(**antkws)\n",
    "\n",
    "pdfpath = f'{suppdir}/07_nrmtrgcont/{i_fig:02d}_lbl_cmplnc_mlp.pdf'\n",
    "fig.savefig(pdfpath, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PP Simulation Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds, n_snrtsim = 10, 1000\n",
    "sim_h5path = './18_ccnmieinp/11_sim.h5'\n",
    "\n",
    "############## Collecting the plotting data ###############\n",
    "ycols = ['m_chmprthst', 'n_prthst', 'ccn_cdf', 'qs_prt', 'qscs_prt', 'qs_pop',\n",
    "    'qa_prt', 'qacs_prt', 'qa_pop', 'frznfrac_tmp', 'logfrznfrac_tmp']\n",
    "\n",
    "v_ydataraws = dict()\n",
    "sim_data = load_h5data(sim_h5path)\n",
    "for ycol in ycols:\n",
    "    n_ychnls, n_ylen = ycol2dims[ycol]\n",
    "    y_orig1 = sim_data[f'var/eval/yaero/orig/{ycol}']\n",
    "    assert y_orig1.shape == (n_seeds, n_snrtsim, n_ychnls, n_ylen)\n",
    "    for pptype in ['tuned', 'plain']:\n",
    "        y_rcnst1 = sim_data[f'var/eval/yaero/{pptype}/{ycol}']\n",
    "        assert y_rcnst1.shape == (n_seeds, n_snrtsim, n_ychnls, n_ylen)\n",
    "        v_ydataraws[f'{pptype}/{ycol}/orig'] = y_orig1[:, :, None, :, :]\n",
    "        v_ydataraws[f'{pptype}/{ycol}/rcnst'] = y_rcnst1[:, :, None, :, :]\n",
    "\n",
    "v_mtrcdatas, v_ydatas = dict(), dict()\n",
    "for pptype, v_ydataraw in hie2deep(v_ydataraws, maxdepth=1).items():\n",
    "    ########### Data Cleaning and Unit Conversions ############\n",
    "    v_ydata, aero_cstscnv = cnvrt_physunits(v_ydataraw, aero_csts)\n",
    "    ######## Computing the Aerosol Diagnostic Metrics #########\n",
    "    v_mtrcdata = calc_aerometrics(v_ydata, aero_cstscnv, avg_errs=True)\n",
    "    \n",
    "    v_mtrcdatas[pptype] = v_mtrcdata\n",
    "    v_ydatas[pptype] = v_ydata\n",
    "\n",
    "v_mtrcdatas = deep2hie(v_mtrcdatas)\n",
    "v_ydatas = deep2hie(v_ydatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Confidence Intervals with Statistical Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_amp = 0.3\n",
    "\n",
    "hpdf_lst, statdf_lst = [], []\n",
    "for pptype, v_mtrcdata in hie2deep(v_mtrcdatas).items():\n",
    "    for i_seed in range(n_seeds):\n",
    "        strowdict = dict()\n",
    "        strowdict['rng_seed'] = i_seed\n",
    "        strowdict['noise_amp'] = noise_amp\n",
    "        for ycol, val in v_mtrcdata.items():\n",
    "            if isinstance(val, dict) and (val['err'] is not None):\n",
    "                strowdict[f'{ycol}.err'] = val['err'][i_seed]\n",
    "        statdf_lst.append(strowdict)\n",
    "        hprowdict = {'pptype': pptype, 'fpidxgrp': pptype}\n",
    "        hpdf_lst.append(hprowdict)\n",
    "\n",
    "hpdf = pd.DataFrame(hpdf_lst)\n",
    "statdf = pd.DataFrame(statdf_lst)\n",
    "\n",
    "# Aggregating the data\n",
    "aggcfg = dict(type='bootstrap', n_boot=1000, q=[5, 95], stat='mean', device='cpu')\n",
    "agg_data = get_aggdf(hpdf, statdf, xcol='noise_amp', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "aggdf1 = pd.concat([agg_data['hpdf'], agg_data['stdf']], axis=1)\n",
    "aggdf1 = aggdf1.drop(columns='fpidxgrp')\n",
    "hpcols = ['pptype', 'noise_amp']\n",
    "\n",
    "# Example: \n",
    "#   aggdf1 = \n",
    "#       pptype  noise_amp  ccn_cdf.err/mean  ccn_cdf.err/low  ccn_cdf.err/high  ...\n",
    "#     0  tuned        0.3          0.067326         0.066693          0.067982   \n",
    "#     1  plain        0.3          0.401532         0.398868          0.403894 \n",
    "\n",
    "ycols = list({col.removesuffix('.err/mean'): None \n",
    "    for col in aggdf1.columns if col.endswith('.err/mean')})\n",
    "assert all((col in hpcols) or (col.split('.err/')[0] in ycols) \n",
    "    for col in aggdf1.columns)\n",
    "\n",
    "aggdf_melts = []\n",
    "for stat in ['mean', 'low', 'high']:\n",
    "    aggdf3 = aggdf1.melt(id_vars=hpcols, \n",
    "        value_vars=[f'{ycol}.err/{stat}' for ycol in ycols],\n",
    "        var_name='metric', value_name=f'err/{stat}')\n",
    "    aggdf3['metric'] = aggdf3['metric'].str.removesuffix(f'.err/{stat}')\n",
    "    aggdf3 = aggdf3.set_index(hpcols + ['metric'])\n",
    "    aggdf_melts.append(aggdf3)\n",
    "\n",
    "aggdf4 = pd.concat(aggdf_melts, axis=1).reset_index()\n",
    "\n",
    "# Example:\n",
    "#   aggdf4 = \n",
    "#         pptype  noise_amp           metric  err/mean   err/low  err/high\n",
    "#      0   tuned        0.3          ccn_cdf  0.067326  0.066693  0.067982\n",
    "#      1   plain        0.3          ccn_cdf  0.401532  0.398868  0.403894\n",
    "#      2   tuned        0.3           qs_pop  0.076846  0.075991  0.077730\n",
    "#      3   plain        0.3           qs_pop  0.913600  0.908511  0.919038\n",
    "#      4   tuned        0.3           qa_pop  0.165132  0.158923  0.170963\n",
    "#      5   plain        0.3           qa_pop  0.953014  0.946191  0.959708\n",
    "#      6   tuned        0.3  logfrznfrac_tmp  0.033548  0.030919  0.035911\n",
    "#      7   plain        0.3  logfrznfrac_tmp  0.140155  0.135256  0.144967\n",
    "#      8   tuned        0.3     frznfrac_tmp  0.341163  0.319734  0.361636\n",
    "#      9   plain        0.3     frznfrac_tmp  0.813503  0.804350  0.822792\n",
    "#      10  tuned        0.3      m_chmprthst  0.184169  0.180745  0.186895\n",
    "#      11  plain        0.3      m_chmprthst  0.805887  0.799019  0.813259\n",
    "#      12  tuned        0.3         n_prthst  0.056184  0.055867  0.056504\n",
    "#      13  plain        0.3         n_prthst  0.094233  0.093702  0.094785\n",
    "#      14  tuned        0.3         m_prthst  0.168582  0.165300  0.171351\n",
    "#      15  plain        0.3         m_prthst  0.854394  0.849895  0.859212\n",
    "#      16  tuned        0.3      m_perchmhst  0.489110  0.482695  0.495706\n",
    "#      17  plain        0.3      m_perchmhst  0.946817  0.945679  0.947944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tuned vs. Plain Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Grouped Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 86\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "ycols1 = {\n",
    "    'ccn_cdf': 'CCN Spectrum\\nRelative Error', \n",
    "    'logqs_pop': 'Scattering\\nLog-Rel Error', \n",
    "    'logqa_pop': 'Absorption\\nLog-Rel Error', \n",
    "    'logfrznfrac_tmp': 'Frozen Fraction\\nLog-Rel Error'}\n",
    "ycols2 = { \n",
    "    'm_chmprthst': 'Speciated Mass\\nRelative Error', \n",
    "    'n_prthst': 'Number\\nRelative Error', \n",
    "    'm_prthst': 'Total Mass\\nRelative Error', \n",
    "    'm_perchmhst': 'Species Bulk Mass\\nRelative Error'}\n",
    "\n",
    "for y_scale in ('log', 'lin'):\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=[4.0 * 2, 2.5 * 1], \n",
    "            dpi=100, sharex=False, sharey=False, squeeze=False)\n",
    "        axes = np.array(axes)\n",
    "\n",
    "        for ax, ycols, y_prtid in [(axes[0, 0], ycols1, 'p1'), (axes[0, 1], ycols2, 'p2')]:\n",
    "            aggdf5 = aggdf4[aggdf4['metric'].isin(ycols)].reset_index(drop=True)\n",
    "            aggdf6 = aggdf5.replace({'tuned': 'Tuned', 'plain': 'Plain', **ycols}).copy(deep=True)\n",
    "            plt_cfg = {'fig': fig, 'ax': ax, **v_mplcfgs[f'paper.ppsim.grperr.horz.{y_scale}.{y_prtid}']}\n",
    "            fig, ax = draw_matplotlib(plt_cfg, aggdf6)\n",
    "            ax.set_ylim(3.6, -0.6)\n",
    "\n",
    "        fig.subplots_adjust(wspace=0.5)\n",
    "        for ax, tag_txt in zip(axes.ravel(), 'ab'):\n",
    "            tag_axis(ax, f'$\\\\rm({tag_txt})$', fontsize=11, pad=[0.3, 0.5])\n",
    "\n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_ppsim_err{y_scale}_horz.pdf'\n",
    "    fig.savefig(pdfpath, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Grouped Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 86\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "ycols = {\n",
    "    'ccn_cdf': 'CCN Spectrum\\nRelative Error', \n",
    "    'qs_pop': 'Scattering\\nLog-Rel Error', \n",
    "    'qa_pop': 'Absorption\\nLog-Rel Error', \n",
    "    'logfrznfrac_tmp': 'Frozen Fraction\\nLog-Rel Error',\n",
    "    'm_chmprthst': 'Speciated Mass\\nRelative Error', \n",
    "    'n_prthst': 'Number\\nRelative Error', \n",
    "    'm_prthst': 'Total Mass\\nRelative Error', \n",
    "    'm_perchmhst': 'Per-Chem Mass\\nRelative Error'}\n",
    "\n",
    "aggdf5 = aggdf4[aggdf4['metric'].isin(ycols)].reset_index(drop=True)\n",
    "aggdf6 = aggdf5.replace({'tuned': 'Tuned', 'plain': 'Plain', **ycols}).copy(deep=True)\n",
    "\n",
    "for y_scale in ('log', 'lin'):\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        fig, axes = plt.subplots(1, 1, figsize=[9.6, 2.5], \n",
    "            dpi=100, sharex=False, sharey=False, squeeze=False)\n",
    "        axes = np.array(axes)\n",
    "        ax = axes[0, 0]\n",
    "        plt_cfg = {'fig': fig, 'ax': ax, **v_mplcfgs[f'paper.ppsim.grperr.vert.{y_scale}']}\n",
    "        fig, ax = draw_matplotlib(plt_cfg, aggdf6)\n",
    "        \n",
    "        ax.set_xlim(-0.6, 7.6)\n",
    "        fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    pdfpath = f'{workdir}/{i_fig:02d}_ppsim_err{y_scale}_vert.pdf'\n",
    "    fig.savefig(pdfpath, bbox_inches='tight')\n",
    "    print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Grouped Vertical Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 86\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "ycols = [\n",
    "    'ccn_cdf', 'qs_pop', 'qa_pop', 'logfrznfrac_tmp',\n",
    "    'm_chmprthst', 'n_prthst', 'm_prthst', 'm_perchmhst']\n",
    "\n",
    "with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "    fig, axes = plt.subplots(1, 8, figsize=[1.8 * 8, 3.0 * 1], \n",
    "        dpi=100, sharex=False, sharey=False, squeeze=False)\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    aggdf2 = aggdf1.replace({'tuned': 'Tuned', 'plain': 'Plain'}).copy(deep=True)\n",
    "    for ycol, ax in zip(ycols, axes.ravel()):\n",
    "        plt_cfg = {'fig': fig, 'ax': ax, **v_mplcfgs[f'paper.ppsim.{ycol}.err']}\n",
    "        fig, ax = draw_matplotlib(plt_cfg, aggdf2)\n",
    "        aa, bb = ax.get_xlim()\n",
    "        cc, dd = (aa + bb) / 2, (bb - aa) / 2\n",
    "        ax.set_xlim(cc - dd * 0.7, cc + dd * 0.7)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.6)\n",
    "\n",
    "    for ax, tag_txt in zip(axes.ravel(), 'abcdefgh'):\n",
    "        tag_axis(ax, f'({tag_txt})', fontsize=11, pad=[0.3, 0.5])\n",
    "\n",
    "pdfpath = f'{workdir}/{i_fig:02d}_ppsim_errlog_vert2.pdf'\n",
    "fig.savefig(pdfpath, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tuned vs. Plain Anecdotal Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 87\n",
    "plt.ioff()\n",
    "\n",
    "chem_species = aero_cstscnv['chem_species']\n",
    "eps_histmin = aero_cstscnv['eps_histmin']\n",
    "eps_histmax = aero_cstscnv['eps_histmax']\n",
    "tmprtr_inpmin = aero_cstscnv['tmprtr_inpmin']\n",
    "tmprtr_inpmax = aero_cstscnv['tmprtr_inpmax']\n",
    "d_histbinsum = aero_cstscnv['d_histbins']\n",
    "\n",
    "# Reading the plot configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "v_mplcfgs['ccn_cdf']['xlim'] = [eps_histmin * 0.9, eps_histmax * 1.1]\n",
    "v_mplcfgs['frznfrac_tmp']['xlim'] = [tmprtr_inpmin - 1, tmprtr_inpmax + 1]\n",
    "\n",
    "i_sel = np.array([[165, 58, 473]])\n",
    "vrnts = ['orig', 'tuned', 'plain']\n",
    "\n",
    "pats_rnm = {\n",
    "    'tuned/{x_node}/orig': '{x_node}/orig',\n",
    "    'tuned/{x_node}/rcnst': '{x_node}/tuned',\n",
    "    'plain/{x_node}/rcnst': '{x_node}/plain'}\n",
    "v_ydata2 = get_subdictrnmd(v_ydatas, pats_rnm)\n",
    "\n",
    "figs_dict = dict()\n",
    "i_seed = 0\n",
    "n_figrows, n_figcols = i_sel.shape[1], 4\n",
    "for i_figpage, i_samps in enumerate(i_sel):\n",
    "    v_mpldatas = dict()\n",
    "    for i_figrow, i_samp in enumerate(i_samps):\n",
    "        v_ydata3 = {key: val[i_seed, i_samp, 0] for key, val in v_ydata2.items()}\n",
    "        # The speciated mass data\n",
    "        for i_figcol, vrnt in enumerate(vrnts):\n",
    "            ax_idx = i_figrow * n_figcols + i_figcol\n",
    "            for i_chem, chem in enumerate(chem_species):\n",
    "                v_mpldatas[f'paper.ppsim.m_chmprthst.{vrnt}/{ax_idx}:{chem}:x'] = d_histbinsum\n",
    "                v_mpldatas[f'paper.ppsim.m_chmprthst.{vrnt}/{ax_idx}:{chem}:y'] = v_ydata3[f'm_chmprthst/{vrnt}'][i_chem]\n",
    "        \n",
    "        # The particle count data        \n",
    "        i_figcol = 3\n",
    "        ax_idx = i_figrow * n_figcols + i_figcol\n",
    "        for vrnt in vrnts:\n",
    "            v_mpldatas[f'paper.ppsim.n_prthst/{ax_idx}:{vrnt}:x'] = d_histbinsum\n",
    "            v_mpldatas[f'paper.ppsim.n_prthst/{ax_idx}:{vrnt}:y'] = np.squeeze(v_ydata3[f'n_prthst/{vrnt}'])\n",
    "    \n",
    "    v_mpldatashie = hie2deep(v_mpldatas, maxdepth=1)\n",
    "\n",
    "    ########## Calling Matplotlib to Plot the Diagnostics ########## \n",
    "    fig, axes = None, None\n",
    "    for ax_idx, ax_id in enumerate(v_mpldatashie):\n",
    "        fig, axes = plot_mpl(data=v_mpldatashie[ax_id], fig=fig, \n",
    "            axes=axes, mplopts=v_mplcfgs[ax_id])\n",
    "    \n",
    "    # Sharing the y-axis limits between the three axes\n",
    "    for i_figrow in range(n_figrows):\n",
    "        axes_yshrd = axes[i_figrow, :2]\n",
    "        ax_ylimlo = min(ax.get_ylim()[0] for ax in axes_yshrd)\n",
    "        ax_ylimhi = max(ax.get_ylim()[1] for ax in axes_yshrd)\n",
    "        for ax in axes_yshrd:\n",
    "            ax.set_ylim(ax_ylimlo, ax_ylimhi)\n",
    "\n",
    "    # Adding the top left april tag for axis identification\n",
    "    with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "        for i_figrow in range(n_figrows):\n",
    "            for i_figcol, tag_char in enumerate('abcd'):\n",
    "                ax = axes[i_figrow, i_figcol]\n",
    "                tag_axis(ax, f'({tag_char}$_{{{i_figrow+1}}}$)', \n",
    "                    fontsize=11, pad=(0.3, 0.6))\n",
    "\n",
    "        for i_figcol, col_ttl in enumerate(['Input\\n(Mass Distribution)', \n",
    "            'Simulated Reconst\\n$\\\\textbf{with}$ Preprocessing\\n(Mass Distribution)', \n",
    "            'Simulated Reconst\\n$\\\\textbf{without}$ Preprocessing\\n(Mass Distribution)', \n",
    "            'Number Distribution\\nReconst Simulations']):\n",
    "            print_axheader(axes[0, i_figcol], col_ttl, 'top', pad=4,\n",
    "                fontsize=13.5, fontweight='bold')\n",
    "\n",
    "        for i_figrow in range(n_figrows):\n",
    "            row_ttl = f'Sample {i_figrow + 1}'\n",
    "            print_axheader(axes[i_figrow, 0], row_ttl, 'left', \n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "    figs_dict[f'{i_figpage}/diag'] = fig\n",
    "\n",
    "pdfpath = f'{workdir}/{i_fig:02d}_ppsim_anec.pdf'\n",
    "with PdfPages(pdfpath) as pdf:\n",
    "    for ii_samp, fig3 in figs_dict.items():\n",
    "        pdf.savefig(figure=fig3, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "i_fig += 1\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tuned vs. Plain Q-Q Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 88\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "n_q = 3999\n",
    "q = np.linspace(0.0015, 0.9985, n_q)\n",
    "assert q.shape == (n_q,)\n",
    "\n",
    "nrml_qnts = norm.ppf(q)\n",
    "assert nrml_qnts.shape == (n_q,)\n",
    "\n",
    "rc_context = v_mplcfgs['rc_context'].copy()\n",
    "rc_context.pop('axes.autolimit_mode')\n",
    "with plt.rc_context(rc_context) as pltrcctx:\n",
    "    n_figrows, n_figcols = 2, 3\n",
    "    fig, axes = plt.subplots(n_figrows, n_figcols, figsize=[2.6 * n_figcols, 2.4 * n_figrows], \n",
    "        dpi=140, sharex=True, sharey=False, squeeze=False)\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    for ax_idx, (y_col, y_name, y_desc, y_low, y_high, tag_txt) in enumerate([\n",
    "        ('m_chmprthst', '$m$', 'Speciated Mass\\nDistribution ($m$)', -1, 3, '(a)'), \n",
    "        ('n_prthst', '$n$', 'Number\\nDistribution ($n$)', -1, 4, '(b)'), \n",
    "        (None, None, None, None, None, None), \n",
    "        ('m_prthstmag', '$u_1$', 'Speciated Mass\\nMagnitudes ($u_1$)', -4, 6, '(d)'), \n",
    "        ('m_chmprtnrm', '$u_2$', 'Speciated Mass\\nProportions ($u_2$)', -5, 5, '(c)'), \n",
    "        ('n_prthstnrm', '$u_3$', 'Normalized Number\\nDistribution ($u_3$)', -4, 6, '(e)')]):\n",
    "        i_figrow, i_figcol = ax_idx // n_figcols, ax_idx % n_figcols\n",
    "        ax = axes[i_figrow, i_figcol]\n",
    "\n",
    "        if y_col is None:\n",
    "            ax.remove()\n",
    "            continue\n",
    "\n",
    "        y_nparr = sim_data[f'var/eval/raw/tuned/{y_col}']\n",
    "        n_ychnls, n_ylen = y_nparr.shape[-2:]\n",
    "        assert y_nparr.shape == (n_seeds, n_snrtsim, n_ychnls, n_ylen)\n",
    "\n",
    "        y_nparr2 = (y_nparr - y_nparr.mean()) / y_nparr.std()\n",
    "        assert y_nparr2.shape == (n_seeds, n_snrtsim, n_ychnls, n_ylen)\n",
    "\n",
    "        y_qnts = np.quantile(y_nparr2.ravel(), q)\n",
    "        assert y_qnts.shape == (n_q,)\n",
    "\n",
    "        ax.scatter(nrml_qnts, y_qnts, marker='o', s=3, color='blue')\n",
    "        y_lim1 = ax.get_ylim()\n",
    "\n",
    "        y_line = nrml_qnts * y_qnts.std() + y_qnts.mean()\n",
    "        ax.plot(nrml_qnts, y_line, ls='--', color='black', lw=1)\n",
    "\n",
    "        # Adding the small identification text box\n",
    "        props = dict(facecolor='none', edgecolor='none')\n",
    "        ax.text(0.03, 0.97, y_desc, transform=ax.transAxes, fontsize=9,\n",
    "            usetex=True, verticalalignment='top', bbox=props)\n",
    "\n",
    "        if i_figrow == (n_figrows - 1):\n",
    "            ax.set_xlabel('$\\mathcal{N}(0,1)$ Quantiles')\n",
    "        ax.set_ylabel(f'Standardized {y_name} Quantiles')\n",
    "\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(y_low, y_high)\n",
    "        ax.set_xticks([-3, -2, -1, 0, 1, 2, 3])\n",
    "\n",
    "        tag_axis(ax, tag_txt, fontsize=11, pad=(0.3, 0.6))\n",
    "\n",
    "    for ax, row_ttl in [(axes[0, 0], 'Without Preprocessing'), (axes[1, 0], 'With Preprocessing')]:\n",
    "        print_axheader(ax, row_ttl, 'left', fontsize=14, pad=25, fontweight='bold')\n",
    "            \n",
    "    fig.subplots_adjust(wspace=0.35, hspace=0.3)\n",
    "\n",
    "pdfpath = f'{workdir}/{i_fig:02d}_ppsim_qqplot.pdf'\n",
    "fig.savefig(pdfpath, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8: Testing the New Pre-Processing Framework\n",
    "\n",
    "Related Configs: \n",
    "\n",
    "  * `configs/02_adhoc/11_mlphist.yml`\n",
    "\n",
    "  * `configs/02_adhoc/12_cnnhist.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * Following up on Experiment 7 with both MLP and CNN architectures.\n",
    "\n",
    "Issues:\n",
    "\n",
    "  * TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp8dashdata(hpdf, statdf, tabprfx):\n",
    "    # Squeezing the stat column names by removing singular levels\n",
    "    statdf = adjust_mtrcnames(statdf)\n",
    "    stcol_longnames = statdf.columns.tolist()\n",
    "    stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "    statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))\n",
    "\n",
    "    # Dropping the quantile data to save on space :)\n",
    "    drop_pats = ['q10', 'q25', 'q5', 'q75', 'q90', 'q95', 'median', '/kl:']\n",
    "    keepcols = [col for col in statdf.columns if not any(x in col for x in drop_pats)]\n",
    "    statdf = statdf[keepcols] \n",
    "\n",
    "    # Downcasting numerical types to save on space\n",
    "    statdf = downcast_df(statdf)\n",
    "\n",
    "    # `fpicols` will be an `fpidxgrp` to hp column mapping; each fpidxgrp \n",
    "    # is part of an ovat ablation defined by a single column.\n",
    "    ii_drop = hpdf['fpidxgrp'].drop_duplicates().index\n",
    "    hpdf2 = hpdf.loc[ii_drop].reset_index(drop=True)\n",
    "\n",
    "    fpicols = dict()\n",
    "    fpidxgrps = hpdf2['fpidxgrp']\n",
    "    main_fpidx = fpidxgrps.iloc[0]\n",
    "    for fpidx in fpidxgrps.iloc[1:]:\n",
    "        hpdf3 = hpdf2[fpidxgrps.isin([fpidx, main_fpidx])]\n",
    "        hpdf4 = drop_unqcols(hpdf3)\n",
    "        hpdf5 = hpdf4.drop(columns=['fpidx', 'fpidxgrp'], errors='ignore')\n",
    "        cols = hpdf5.columns.tolist()\n",
    "        fpicols[fpidx] = cols[0] if ('nn/pp/magdim' not in cols) else 'nn/pp/magdim'\n",
    "\n",
    "    aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "\n",
    "    tab_ttl1 = f'{tabprfx.lower()}gnrl'\n",
    "    tab_fpidxs1 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() if 'nn/pp' not in ovatcol]\n",
    "\n",
    "    tab_ttl2 = f'{tabprfx.lower()}ppmag'\n",
    "    tab_fpidxs2 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() \n",
    "        if ovatcol in ['nn/pp/magdim', 'nn/pp/magpnrm', 'nn/pp/magexp', \n",
    "                       'nn/pp/magshft', 'nn/pp/magscl', 'nn/pp/magscleps']]\n",
    "\n",
    "    tab_ttl3 = f'{tabprfx.lower()}ppnrm'\n",
    "    tab_fpidxs3 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() \n",
    "        if ovatcol in ['nn/pp/nrmeps', 'nn/pp/nrmexp', 'nn/pp/nrmshft', 'nn/pp/nrmscl', \n",
    "                       'nn/pp/nrmscleps', 'nn/pp/nrmscldim']]\n",
    "\n",
    "    tab_ttl4 = f'{tabprfx.lower()}ppcnt'\n",
    "    tab_fpidxs4 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() \n",
    "        if ovatcol in ['nn/pp/cnteps', 'nn/pp/cntexp', 'nn/pp/cntshft', 'nn/pp/cntscl', \n",
    "                       'nn/pp/cntscleps', 'nn/pp/cntscldim']]\n",
    "\n",
    "    src_colsset = set(fpicols) \n",
    "    plt_colsset = set(tab_fpidxs1 + tab_fpidxs2 + tab_fpidxs3 + tab_fpidxs4)\n",
    "    assert len(src_colsset - plt_colsset) == 0, f'unused ablations: {src_colsset - plt_colsset}'\n",
    "    \n",
    "    # Aggregating the data\n",
    "    data = []\n",
    "    for tab_ttl, tab_fpidxs in [(tab_ttl1, tab_fpidxs1), \n",
    "        (tab_ttl2, tab_fpidxs2), (tab_ttl3, tab_fpidxs3), (tab_ttl4, tab_fpidxs4)]:\n",
    "        tab_idx = (hpdf['fpidxgrp'].isin(tab_fpidxs))\n",
    "        hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "        stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "        agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "            huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "        hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "        stcols_tab = agg_data['stcols']\n",
    "        data.append((tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab))\n",
    "\n",
    "    return data\n",
    "\n",
    "def rnm_cols(colnames: list, renamecfg: dict):\n",
    "    renamecfg = dict(renamecfg)\n",
    "    col_renamer = renamecfg.get('columns', dict())\n",
    "    col_replacer = renamecfg.get('colrplc', dict())\n",
    "    colnames2 = [colnames] if isinstance(colnames, str) else colnames\n",
    "    assert isinstance(colnames2, (list, tuple))\n",
    "    assert all(isinstance(x, str) for x in colnames2)\n",
    "\n",
    "    # Renaming the columns (whole)\n",
    "    if col_renamer is not None:\n",
    "        assert isinstance(col_renamer, dict)\n",
    "        colnames3 = [col_renamer.get(col, col) for col in colnames2]\n",
    "    else:\n",
    "        colnames3 = colnames2[:]\n",
    "\n",
    "    # Renaming the columns (by parts)\n",
    "    if col_replacer is not None:\n",
    "        colnames4 = []\n",
    "        for col in colnames3:\n",
    "            colsplt = col.split('/')\n",
    "            colsplt = [col_replacer.get(val, val) for val in colsplt]\n",
    "            colrp = '/'.join(colsplt)\n",
    "            colnames4.append(colrp)\n",
    "    else:\n",
    "        colnames4 = colnames3[:]\n",
    "    \n",
    "    return colnames4[0] if isinstance(colnames, str) else colnames4\n",
    "\n",
    "def append_relcols(hpstdf, n_lvl, i_lvl, name_ref, name_prd, name_rel, rel_type):\n",
    "    cols_ref = [col for col in hpstdf.columns \n",
    "        if (col.count('/') == (n_lvl - 1)) and col.split('/')[i_lvl] == name_ref] \n",
    "    assert len(cols_ref) > 0, 'perhaps n_lvl or i_lvl is mis-specified since no ref cols exist'\n",
    "    cols_prd = [col for col in hpstdf.columns \n",
    "        if (col.count('/') == (n_lvl - 1)) and col.split('/')[i_lvl] == name_prd] \n",
    "\n",
    "    drop_val = lambda col: '/'.join(col.split('/')[:i_lvl] + col.split('/')[i_lvl+1:])\n",
    "    stdf_ref = hpstdf[cols_ref].rename(columns={col: drop_val(col) for col in cols_ref})\n",
    "    stdf_prd = hpstdf[cols_prd].rename(columns={col: drop_val(col) for col in cols_prd})\n",
    "\n",
    "    assert set(stdf_ref.columns) == set(stdf_prd.columns), dedent(f'''\n",
    "        The column sets between the ref and prd dataframes do not match:\n",
    "            missing columns: {set(stdf_ref.columns) - set(stdf_prd.columns)}\n",
    "            extra columns: {set(stdf_prd.columns) - set(stdf_ref.columns)}''')\n",
    "    stdf_prd = stdf_prd[stdf_ref.columns]\n",
    "\n",
    "    assert all(stdf_ref.columns == stdf_prd.columns)\n",
    "\n",
    "    ref_array, prd_array = stdf_ref.values, stdf_prd.values\n",
    "\n",
    "    if rel_type == 'mean+ci':\n",
    "        # Making sure the dataframe can be i_seed-factorized.\n",
    "        df1, df2 = decomp_df(hpstdf[['fpidxgrp', 'noise_amp', 'rng_seed']], \n",
    "            [['fpidxgrp', 'noise_amp'], ['rng_seed']], validate=True)\n",
    "        n_hps, n_seeds = df1.shape[0], df2.shape[0]\n",
    "\n",
    "        n_cols = ref_array.shape[1]\n",
    "        assert ref_array.shape == (n_hps * n_seeds, n_cols)\n",
    "        assert stdf_prd.shape == (n_hps * n_seeds, n_cols)\n",
    "\n",
    "        n_boot = 10 * n_seeds\n",
    "        np_random = np.random.RandomState(seed=12345)\n",
    "        i_btstrpref = (n_seeds * np_random.rand(n_boot * n_seeds)).astype(int)\n",
    "        i_btstrpprd = (n_seeds * np_random.rand(n_boot * n_seeds)).astype(int)\n",
    "        with torch.no_grad():\n",
    "            # Getting a bunch of bootstrap samples of the reference mean.\n",
    "            ref_array2 = torch.from_numpy(ref_array).reshape(n_hps, n_seeds, n_cols)\n",
    "            assert ref_array2.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "            ref_array3 = ref_array2[:, i_btstrpref, :].reshape(n_hps, n_boot, n_seeds, n_cols)\n",
    "            assert ref_array3.shape == (n_hps, n_boot, n_seeds, n_cols)\n",
    "\n",
    "            ref_array4 = ref_array3.mean(axis=-2)\n",
    "            assert ref_array4.shape == (n_hps, n_boot, n_cols)\n",
    "\n",
    "            ref_array5 = ref_array4.sort(axis=-2).values\n",
    "            assert ref_array5.shape == (n_hps, n_boot, n_cols)\n",
    "\n",
    "            ref_meansamps = ref_array5[:, ::10, :]\n",
    "            assert ref_meansamps.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "            # Getting a bunch of bootstrap samples of the prediction mean.\n",
    "            prd_array2 = torch.from_numpy(prd_array).reshape(n_hps, n_seeds, n_cols)\n",
    "            assert prd_array2.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "            prd_array3 = prd_array2[:, i_btstrpprd, :].reshape(n_hps, n_boot, n_seeds, n_cols)\n",
    "            assert prd_array3.shape == (n_hps, n_boot, n_seeds, n_cols)\n",
    "\n",
    "            prd_array4 = prd_array3.mean(axis=-2)\n",
    "            assert prd_array4.shape == (n_hps, n_boot, n_cols)\n",
    "\n",
    "            prd_array5 = prd_array4.sort(axis=-2).values\n",
    "            assert prd_array5.shape == (n_hps, n_boot, n_cols)\n",
    "\n",
    "            prd_meansamps = prd_array5[:, ::10, :]\n",
    "            assert prd_meansamps.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "            ref_samps = ref_meansamps.detach().cpu().numpy().reshape(n_hps * n_seeds, n_cols)\n",
    "            prd_samps = prd_meansamps.detach().cpu().numpy().reshape(n_hps * n_seeds, n_cols)\n",
    "    elif rel_type == 'mean':\n",
    "        # Making sure the dataframe can be i_seed-factorized.\n",
    "        df1, df2 = decomp_df(hpstdf[['fpidxgrp', 'noise_amp', 'rng_seed']], \n",
    "            [['fpidxgrp', 'noise_amp'], ['rng_seed']], validate=True)\n",
    "        n_hps, n_seeds = df1.shape[0], df2.shape[0]\n",
    "\n",
    "        n_cols = ref_array.shape[1]\n",
    "        assert ref_array.shape == (n_hps * n_seeds, n_cols)\n",
    "        assert stdf_prd.shape == (n_hps * n_seeds, n_cols)\n",
    "\n",
    "        # Getting a bunch of identical samples of the reference mean.\n",
    "        ref_array2 = ref_array.reshape(n_hps, n_seeds, n_cols)\n",
    "        assert ref_array2.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "        ref_array3 = ref_array2.mean(axis=1, keepdims=True)[:, [0] * n_seeds, :]\n",
    "        assert ref_array3.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "        ref_array4 = ref_array3.reshape(n_hps * n_seeds, n_cols)\n",
    "        assert ref_array4.shape == (n_hps * n_seeds, n_cols)\n",
    "\n",
    "        # Getting a bunch of identical samples of the prediction mean.\n",
    "        prd_array2 = prd_array.reshape(n_hps, n_seeds, n_cols)\n",
    "        assert prd_array2.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "        prd_array3 = prd_array2.mean(axis=1, keepdims=True)[:, [0] * n_seeds, :]\n",
    "        assert prd_array3.shape == (n_hps, n_seeds, n_cols)\n",
    "\n",
    "        prd_array4 = prd_array3.reshape(n_hps * n_seeds, n_cols)\n",
    "        assert prd_array4.shape == (n_hps * n_seeds, n_cols)\n",
    "\n",
    "        ref_samps, prd_samps = ref_array4, prd_array4\n",
    "    elif rel_type == 'samp':\n",
    "        ref_samps, prd_samps = ref_array, prd_array\n",
    "    else:\n",
    "        raise ValueError(f'undefined rel_type={rel_type}')\n",
    "\n",
    "    rel_array = 2.0 * np.abs(ref_samps - prd_samps) / (np.abs(ref_samps) + np.abs(prd_samps))\n",
    "    cols_rel = ['/'.join(col.split('/')[:i_lvl] + [name_rel] + col.split('/')[i_lvl:])\n",
    "        for col in stdf_prd.columns.tolist()]\n",
    "    stdf_rel = pd.DataFrame(rel_array, columns=cols_rel, index=stdf_ref.index)\n",
    "\n",
    "    return stdf_rel\n",
    "\n",
    "def rnm_fmt(pat_inp, pat_out, query):\n",
    "    pattern = re.escape(pat_inp)\n",
    "    pattern = re.sub(r'\\\\\\{(\\w+)\\\\\\}', r'(?P<\\1>.*)', pattern)\n",
    "    matchres = re.match(pattern, query)\n",
    "    return None if matchres is None else pat_out.format(**matchres.groupdict())\n",
    "\n",
    "def get_colrnmngs(df, rnm_patdict):\n",
    "    colrnmngs = dict()\n",
    "    for col in df.columns:\n",
    "        for pat_inp, pat_out in rnm_patdict.items():\n",
    "            col_rnmd = rnm_fmt(pat_inp, pat_out, col)\n",
    "            if (col_rnmd is not None) and (col not in colrnmngs):\n",
    "                colrnmngs[col] = col_rnmd\n",
    "            elif (col_rnmd is not None) and (col in colrnmngs):\n",
    "                assert colrnmngs[col] == col_rnmd, dedent(f'''\n",
    "                    Conflicting renamings for \"{col}\":\n",
    "                        Name 1: \"{colrnmngs[col]}\"\n",
    "                        Name 2: \"{col_rnmd}\"''')\n",
    "\n",
    "    return colrnmngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltcache_path = f'./{workdir}/32_pltdata.h5'\n",
    "remake_pltdata = not exists(pltcache_path)\n",
    "\n",
    "if remake_pltdata:\n",
    "    smrypath = '../summary/08_mlphist.h5'\n",
    "    get_h5du(smrypath, verbose=True, detailed=False)\n",
    "    data = load_h5data(smrypath)\n",
    "    hpdf_mlp = data['hp']\n",
    "    statdf_mlp = data['stat']\n",
    "\n",
    "    smrypath = '../summary/09_cnnhist.h5'\n",
    "    get_h5du(smrypath, verbose=True, detailed=False)\n",
    "    data = load_h5data(smrypath)\n",
    "    hpdf_cnn = data['hp']\n",
    "    statdf_cnn = data['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    # Making sure eps_sig*=0 can be displayed on a log scale\n",
    "    for col in ['nn/pp/cntscleps', 'nn/pp/magscleps', 'nn/pp/nrmscleps']:\n",
    "        hpdf_mlp[col] = hpdf_mlp[col].cat.rename_categories({0.0: 1e-6})\n",
    "        hpdf_cnn[col] = hpdf_cnn[col].cat.rename_categories({0.0: 1e-6})\n",
    "\n",
    "    # These columns are wreaking havoc on the OVAT detection for \n",
    "    # `nn/pp/magdim`. It's better to forget about them. \n",
    "    prblmtc_cols = ['nn/enc/n_chnlsmag', 'nn/enc/n_lenmag', \n",
    "        'nn/dec/n_chnlsmag', 'nn/dec/n_lenmag']\n",
    "    for col in prblmtc_cols:\n",
    "        hpdf_mlp.loc[hpdf_mlp['nn/pp/magdim'] != 'n_bins', col] = hpdf_mlp[col].iloc[0]\n",
    "        hpdf_cnn.loc[hpdf_cnn['nn/pp/magdim'] != 'n_bins', col] = hpdf_cnn[col].iloc[0]\n",
    "    \n",
    "    data_mlp = get_exp8dashdata(hpdf_mlp, statdf_mlp, tabprfx='MLP')\n",
    "    data_cnn = get_exp8dashdata(hpdf_cnn, statdf_cnn, tabprfx='CNN')\n",
    "    data_real = data_mlp + data_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Real Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    ymlpath_real = f'{workdir}/29_vaehist.yml'\n",
    "    with open(ymlpath_real, 'r') as fp:\n",
    "        dash_cfgreal = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "\n",
    "    rnm_cfgreal = dash_cfgreal.pop('rename')\n",
    "    dash_cfgreal['data/global']['smry/selcol'] = 'loss/net/mean'\n",
    "    dash_cfgreal['bokeh/global']['ycol'] = 'loss/net'\n",
    "\n",
    "    dashdata_real = get_dashdata(data_real, dash_cfgreal, write_yml=False)['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    cachepath = './18_ccnmieinp/04_sim.h5'\n",
    "    save_data = load_h5data(cachepath)\n",
    "    hpdf = save_data['hpdf']\n",
    "    statdf = save_data['statdf']\n",
    "\n",
    "    # Making sure eps_sig*=0 can be displayed on a log scale\n",
    "    hpdf.loc[hpdf['cntscleps'] == 0.0, 'cntscleps'] = 1e-6\n",
    "    hpdf.loc[hpdf['magscleps'] == 0.0, 'magscleps'] = 1e-6\n",
    "    hpdf.loc[hpdf['nrmscleps'] == 0.0, 'nrmscleps'] = 1e-6\n",
    "\n",
    "    hpdf['fpidxgrp'] = hpdf.groupby(hpdf.columns.tolist()).ngroup()\n",
    "    hpdf['fpidx'] = hpdf['fpidxgrp']\n",
    "    hpdf = hpdf.astype('category')\n",
    "\n",
    "    # Adding the relative difference columns\n",
    "    hpstdf = pd.concat([hpdf, statdf], axis=1)\n",
    "    statdf1 = append_relcols(hpstdf, n_lvl=4, i_lvl=2, name_ref='train', \n",
    "        name_prd='test', name_rel='trn2tst', rel_type='mean+ci')\n",
    "    statdf2 = append_relcols(hpstdf, n_lvl=4, i_lvl=2, name_ref='train', \n",
    "        name_prd='comb', name_rel='trn2cmb', rel_type='mean+ci')\n",
    "    statdf3 = append_relcols(hpstdf, n_lvl=4, i_lvl=3, name_ref='data', \n",
    "        name_prd='normal', name_rel='data2nrml', rel_type='mean+ci')\n",
    "    statdf = pd.concat([statdf, statdf1, statdf2, statdf3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "    # Aggregating the data\n",
    "    agg_data = get_aggdf(hpdf, statdf, xcol='noise_amp', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "    stcols_tab = agg_data['stcols']\n",
    "    data_sim = [('simpp', hpdf_tabagg, stdf_tabagg, stcols_tab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    ymlpath_sim = './18_ccnmieinp/05_simdash.yml'\n",
    "    with open(ymlpath_sim, 'r') as fp:\n",
    "        dash_cfgsim = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "    rnm_cfgsim = dash_cfgsim.pop('rename')\n",
    "    dashdata_sim = get_dashdata(data_sim, dash_cfgsim, write_yml=False)['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Real Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allxcols = [\n",
    "    'magdim', 'magpnrm', 'magexp', 'magshft', 'magscl', 'magscleps', \n",
    "    'nrmeps', 'nrmexp', 'nrmshft', 'nrmscl', 'nrmscleps', 'nrmscldim', \n",
    "    'cnteps', 'cntexp', 'cntshft', 'cntscl', 'cntscleps', 'cntscldim']\n",
    "    \n",
    "if remake_pltdata:\n",
    "    aggdf_ablsreal = dict()\n",
    "    for xcol in allxcols:        \n",
    "        col_patsreal = {\n",
    "            'perf/aerosol/test/{ymtrc}/mean/{stat}': '{ymtrc}/{stat}',\n",
    "            'nn/pp/{varbl}': '{varbl}', 'model': 'model', \n",
    "            'noise_amp': 'noise_amp', xcol: xcol}\n",
    "\n",
    "        aggdfs_lst = []\n",
    "        for tababl_id, tababl_data in dashdata_real.items():\n",
    "            tab_id, abl_id = tababl_id.split('/')\n",
    "            if (abl_id != xcol):\n",
    "                continue\n",
    "            # Example:\n",
    "            #   tababl_id = 'mlpppmag/magexp'\n",
    "            #   tab_id, abl_id = 'mlpppmag', 'magexp'\n",
    "            #   tabname = 'MLP General'\n",
    "            #   ablname = 'Mag Exponent'\n",
    "            #   ablhpcols = ['nn/pp/magexp']\n",
    "            tabname, ablname, abldf, ablhpcols, stcols, bkfigcfg, mpldict = tababl_data\n",
    "            abldf2 = abldf.copy(deep=True)\n",
    "            abldf2.insert(2, 'model', tab_id[:3])\n",
    "            abldf2.insert(3, 'noise_amp', 0.0)\n",
    "            abldf2 = abldf2.drop(columns=['fpidx', 'fpidxgrp', 'epoch'])\n",
    "            aggdfs_lst.append(abldf2)\n",
    "        \n",
    "        assert len(aggdfs_lst) == 2, dedent(f'''\n",
    "            The \"mlp\" and \"cnn\" tab data must be present for the \n",
    "            \"{xcol}\" ablation. However, \"{len(aggdfs_lst)}\" tab \n",
    "            data were available.''')\n",
    "\n",
    "        aggdf1_real = pd.concat(aggdfs_lst, axis=0, ignore_index=True)\n",
    "        # Removing categorical data types\n",
    "        for col, col_dtype in dict(aggdf1_real.dtypes).items():\n",
    "            if col_dtype == 'category':\n",
    "                aggdf1_real[col] = aggdf1_real[col].tolist()\n",
    "\n",
    "        cols_rnmsreal = get_colrnmngs(aggdf1_real, col_patsreal)\n",
    "        aggdf_real = aggdf1_real[list(cols_rnmsreal)].rename(columns=cols_rnmsreal)\n",
    "        aggdf_ablsreal[xcol] = aggdf_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    aggdf_ablssim = dict()\n",
    "    for xcol in allxcols:\n",
    "        col_patssim = {\n",
    "            '{ymtrc}/one/test/data/{stat}': '{ymtrc}/{stat}',\n",
    "            'model': 'model', 'noise_amp': 'noise_amp', xcol: xcol}\n",
    "\n",
    "        aggdf_sim1 = dashdata_sim[f'simpp/{xcol}'][2].copy(deep=True)\n",
    "        aggdf_sim1.insert(0, 'model', 'sim')\n",
    "        aggdf_sim1 = aggdf_sim1.drop(columns=['fpidx', 'fpidxgrp'])\n",
    "\n",
    "        # Removing categorical data types\n",
    "        for col, col_dtype in dict(aggdf_sim1.dtypes).items():\n",
    "            if col_dtype == 'category':\n",
    "                aggdf_sim1[col] = aggdf_sim1[col].tolist()\n",
    "\n",
    "        cols_rnmssim = get_colrnmngs(aggdf_sim1, col_patssim)\n",
    "        aggdf_sim = aggdf_sim1[list(cols_rnmssim)].rename(columns=cols_rnmssim)\n",
    "\n",
    "        aggdf_ablssim[xcol] = aggdf_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and Plotting the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remake_pltdata:\n",
    "    aggdf_abls = dict()\n",
    "    for xcol in allxcols:\n",
    "        aggdf_real = aggdf_ablsreal[xcol]\n",
    "        aggdf_sim = aggdf_ablssim[xcol]\n",
    "\n",
    "        assert set(aggdf_real.columns) == set(aggdf_sim.columns), dedent(f'''\n",
    "            Extra columns: {set(aggdf_sim.columns) - set(aggdf_real.columns)}\n",
    "            Missing columns: {set(aggdf_real.columns) - set(aggdf_sim.columns)}''')\n",
    "\n",
    "        aggdf = pd.concat([aggdf_real, aggdf_sim], axis=0, ignore_index=True)\n",
    "        aggdf_abls[xcol] = aggdf\n",
    "    \n",
    "    save_h5data(aggdf_abls, pltcache_path, driver='core')\n",
    "else:\n",
    "    aggdf_abls = load_h5data(pltcache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib Ablation Booklets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "ycols = ['ccn_err', 'opt_err', 'loginp_err', 'm_chmrelerr', 'n_relerr']\n",
    "\n",
    "abl2fig = dict()\n",
    "for xcol in allxcols:\n",
    "    aggdf = aggdf_abls[xcol]\n",
    "\n",
    "    n_ycols = len(ycols)\n",
    "    fig, axes = plt.subplots(1, n_ycols, figsize=[2.6 * n_ycols, 3.0], \n",
    "      dpi=100, sharex=True, sharey=False)\n",
    "\n",
    "    with open(f'./{workdir}/31_ppmpl.yml', 'r') as fp:\n",
    "        mpl_cfgdict1 = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "        mpl_cfgdict2 = parse_refs(mpl_cfgdict1, trnsfrmtn='hie', pathsep=' -> ')\n",
    "        mpl_cfgdict3 = hie2deep(mpl_cfgdict2)['huegrpd']\n",
    "        mplglbls = mpl_cfgdict3['mplglbls']\n",
    "        yspecs = mpl_cfgdict3['yspecs']\n",
    "        xspecs = mpl_cfgdict3['xspecs']\n",
    "        valrplcs = mpl_cfgdict3['valrplcs']\n",
    "\n",
    "    aggdf = aggdf.replace(valrplcs)\n",
    "    for ycol, ax in zip(ycols, axes):\n",
    "        plt_cfg = dict(xcol=xcol, ycol=ycol, fig=fig, ax=ax)\n",
    "        yspec = yspecs[ycol] if ycol in yspecs else dict()\n",
    "        xspec = xspecs[xcol] if xcol in xspecs else dict()\n",
    "        plt_cfg = {**mplglbls, **plt_cfg, **yspec, **xspec}\n",
    "        fig, ax = draw_matplotlib(plt_cfg, aggdf)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.35)\n",
    "    leg_handles, leg_lables = ax.get_legend_handles_labels()\n",
    "    fig.legend(leg_handles, leg_lables, loc='lower center', bbox_to_anchor=[0.5, -0.17], ncol=7)\n",
    "    abl2fig[xcol] = fig\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(f'./{workdir}/33_ppabls.pdf') as pdf:\n",
    "    for xcol, fig in abl2fig.items():\n",
    "        pdf.savefig(figure=fig, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib Trendy Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "xy_combos = [\n",
    "    ('magexp', 'm_chmrelerr'), ('magdim', 'ccn_err'), ('nrmexp', 'loginp_err'), ('cntshft', 'n_relerr'),   ('nrmexp', 'ccn_err'), \n",
    "    ('magdim', 'm_perchmrelerr'), ('magexp', 'opt_err'), ('magscl', 'ccn_err'), ('magexp', 'm_perchmrelerr'), ('magpnrm', 'opt_err'),   \n",
    "    ('magscleps', 'ccn_err'),  ('cntscl', 'loginp_err'), ('cntexp', 'ccn_err'), ('cntscldim', 'm_chmrelerr'),('magscleps', 'n_relerr')]\n",
    "\n",
    "n_figrows, n_figcols = 3, 5\n",
    "fig, axes = plt.subplots(n_figrows, n_figcols, \n",
    "    figsize=[2.6 * n_figcols, 3.0 * n_figrows], \n",
    "    dpi=100, sharex=False, sharey=False)\n",
    "axes = np.array(axes).ravel()\n",
    "\n",
    "for i_ax, (xcol, ycol) in enumerate(xy_combos):\n",
    "    aggdf = aggdf_abls[xcol]\n",
    "    ax = axes[i_ax]\n",
    "\n",
    "    with open(f'./{workdir}/31_ppmpl.yml', 'r') as fp:\n",
    "        mpl_cfgdict1 = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "        mpl_cfgdict2 = parse_refs(mpl_cfgdict1, trnsfrmtn='hie', pathsep=' -> ')\n",
    "        mpl_cfgdict3 = hie2deep(mpl_cfgdict2)['main']\n",
    "        mplglbls = mpl_cfgdict3['mplglbls']\n",
    "        yspecs = mpl_cfgdict3['yspecs']\n",
    "        xspecs = mpl_cfgdict3['xspecs']\n",
    "        valrplcs = mpl_cfgdict3['valrplcs']\n",
    "\n",
    "    aggdf = aggdf.replace(valrplcs)\n",
    "    plt_cfg = dict(xcol=xcol, ycol=ycol, fig=fig, ax=ax)\n",
    "    yspec = yspecs[ycol] if ycol in yspecs else dict()\n",
    "    xspec = xspecs[xcol] if xcol in xspecs else dict()\n",
    "    plt_cfg = {**mplglbls, **plt_cfg, **yspec, **xspec}\n",
    "    fig, ax = draw_matplotlib(plt_cfg, aggdf)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.35, hspace=0.25)\n",
    "leg_handles, leg_lables = ax.get_legend_handles_labels()\n",
    "fig.legend(leg_handles, leg_lables, loc='lower center', bbox_to_anchor=[0.5, 0.01], ncol=7)\n",
    "\n",
    "fig.savefig(f'./{workdir}/36_ppabls.pdf', bbox_inches='tight')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "xy_combos = [\n",
    "    ('magdim', 'ccn_err'),        ('magexp', 'm_chmrelerr'), ('cntshft', 'n_relerr'),   ('nrmexp', 'ccn_err'),        ('magpnrm', 'opt_err'),   \n",
    "    ('magdim', 'm_perchmrelerr'), ('magexp', 'opt_err'),     ('magscl', 'ccn_err'),     ('magexp', 'm_perchmrelerr'), ('cntscldim', 'm_chmrelerr')]\n",
    "\n",
    "rc_cntxt = {\n",
    "    'figure.max_open_warning': 0, 'font.family': 'serif', 'text.usetex': True,\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath} \\usepackage{amssymb}'}\n",
    "\n",
    "with plt.style.context('default') as psc, plt.rc_context(rc_cntxt) as pltrcctx:\n",
    "    n_figrows, n_figcols = 2, 5\n",
    "    fig, axes = plt.subplots(n_figrows, n_figcols, \n",
    "        figsize=[1.8 * n_figcols, 2.8 * n_figrows], \n",
    "        dpi=100, sharex=False, sharey=True)\n",
    "    axes = np.array(axes).ravel()\n",
    "\n",
    "    for i_ax, (xcol, ycol) in enumerate(xy_combos):\n",
    "        aggdf = aggdf_abls[xcol].copy(deep=True)\n",
    "        ax = axes[i_ax]\n",
    "\n",
    "        with open(f'./{workdir}/31_ppmpl.yml', 'r') as fp:\n",
    "            mpl_cfgdict1 = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "            mpl_cfgdict2 = parse_refs(mpl_cfgdict1, trnsfrmtn='hie', pathsep=' -> ')\n",
    "            mpl_cfgdict3 = hie2deep(mpl_cfgdict2)['main']\n",
    "            mplglbls = mpl_cfgdict3['mplglbls2']\n",
    "            yspecs = mpl_cfgdict3['yspecs2']\n",
    "            xspecs = mpl_cfgdict3['xspecs2']\n",
    "            valrplcs = mpl_cfgdict3['valrplcs2']\n",
    "\n",
    "        aggdf = aggdf.replace(valrplcs)\n",
    "        for ycol2, stat in product(\n",
    "            ['ccn_err', 'm_chmrelerr', 'm_perchmrelerr', 'm_relerr', \n",
    "            'n_relerr', 'opt_err', 'qa_relerr', 'qs_relerr'],\n",
    "            ['mean', 'low', 'high']):\n",
    "            aggdf[f'{ycol2}/{stat}'] = aggdf[f'{ycol2}/{stat}'] / 2\n",
    "\n",
    "        plt_cfg = dict(xcol=xcol, ycol=ycol, fig=fig, ax=ax)\n",
    "        yspec = yspecs[ycol] if ycol in yspecs else dict()\n",
    "        xspec = xspecs[xcol] if xcol in xspecs else dict()\n",
    "        plt_cfg = {**mplglbls, **plt_cfg, **yspec, **xspec}\n",
    "        fig, ax = draw_matplotlib(plt_cfg, aggdf)\n",
    "\n",
    "    for i_ax, ax in enumerate(axes):\n",
    "        tag_text = f'({\"abcdefghij\"[i_ax]})'\n",
    "        tag_axis(ax, tag_text, fontsize=11)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.35)\n",
    "    leg_handles, leg_lables = ax.get_legend_handles_labels()\n",
    "    fig.legend(leg_handles, leg_lables, loc='lower center', \n",
    "        bbox_to_anchor=[0.5, -0.035], edgecolor='black', ncol=7)\n",
    "\n",
    "fig.savefig(f'./{workdir}/36_ppablsfnlzd.pdf', bbox_inches='tight')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh Dahboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/08_mlphist.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf_mlp = data['hp']\n",
    "statdf_mlp = data['stat']\n",
    "\n",
    "smrypath = '../summary/09_cnnhist.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf_cnn = data['hp']\n",
    "statdf_cnn = data['stat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/27_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/28_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/29_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/30_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Hyper-Parameter Ablations on the Histogram VAE\n",
    "\n",
    "Related Configs: \n",
    "\n",
    "  * `configs/02_adhoc/01_mlphist.yml`\n",
    "\n",
    "  * `configs/02_adhoc/02_cnnhist.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * Quick and dirty OVAT experiment of most hyper-parameters in the MLP and CNN architectures.\n",
    "\n",
    "Issues:\n",
    "\n",
    "  * The evaluation metrics' pre-processing is not the same as the training. This was a poor decision.\n",
    "\n",
    "  * Nothing other than reconstruction is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/01_vaehist.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "data = []\n",
    "for arch in ('mlpenc', 'cnnenc'):\n",
    "    tab_idx = (hpdf['nn/enc/type'] == arch)\n",
    "    hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    tab_ttl = {'mlpenc': 'MLP Architecture', 'cnnenc': 'CNN Architecture'}[arch]\n",
    "\n",
    "    # Aggregating the data\n",
    "    agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "    stcols_tab = agg_data['stcols']\n",
    "    \n",
    "    data.append((tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/01_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/02_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/03_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/04_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: KL Weight Study on the Histogram VAE\n",
    "\n",
    "Related config: `configs/02_adhoc/04_klstudy.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * An initial experiment for studying the effect of KL mu and sigma weights on the newly implemented performance metrics.\n",
    "    \n",
    "  * The performance metric categories were congestion, proximity, realism, and curvature.\n",
    "\n",
    "Issues:\n",
    "\n",
    "  * The realism metric was too noisy; the sliced wasserstein was only using 10 slices.\n",
    "\n",
    "  * In the next run, I decided to do some importance sampling \n",
    "        \n",
    "    * I used the test data to define a set of principal components, and emphasized those components in the slices.\n",
    "\n",
    "  * The frequency of the evaluation was too high; the metric calculation time was more than quadruple the training time.\n",
    "\n",
    "  * During taking the runs, the config was changed; the `eval/underscale` samples exist in some runs and not the others. \n",
    "    \n",
    "    * None of the `eval/underscale` samples are important for any reason.\n",
    "\n",
    "    * However, this screwed up the ovat group and column detection.\n",
    "\n",
    "  * One of the runs could not fit 25 seeds into GPU RAM, so it was split into two runs with 24 and 1 seeds each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/02_klstudy.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = statdf['rng_seed'] <= 2300\n",
    "hpdf = hpdf.loc[i1].reset_index(drop=True)\n",
    "statdf = statdf.loc[i1].reset_index(drop=True)\n",
    "\n",
    "hpdf = hpdf[[col for col in hpdf.columns if not col.startswith('eval/underscale')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeezing the stat column names by removing singular levels  \n",
    "stcol_longnames = statdf.columns.tolist()\n",
    "stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cuda:0')\n",
    "data = []\n",
    "for ltnt_dim in (2, 10):\n",
    "    tab_idx = (hpdf['nn/ltnt/dim'] == ltnt_dim)\n",
    "    hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    tab_ttl = f'{ltnt_dim}-Dimensional'\n",
    "\n",
    "    # Aggregating the data\n",
    "    agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "    stcols_tab = agg_data['stcols']\n",
    "    \n",
    "    data.append((tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/05_klstudy.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/06_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/03_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/04_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: KL Weight Study on the Histogram VAE\n",
    "\n",
    "Related config: `configs/02_adhoc/05_klstudy.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * The second round of studying the effect of KL mu and sigma weights on the newly implemented performance metrics.\n",
    "    \n",
    "  * See Experiment 2 for previous issues.\n",
    "\n",
    "Issues:\n",
    "\n",
    "  * The realism performance was peaking at both KL weights of 0.001. \n",
    "  \n",
    "    * This happens to be the central set of values for the OVAT-style experiment.\n",
    "\n",
    "      * We did an OVAT sweep on the KL mu and sigma weights in each latent dimension.\n",
    "\n",
    "    * I got suspicious that we hit the jackpot without any tuning.\n",
    "      \n",
    "      * Maybe, this peak performance may have been related to the fact that the mu and sigma weights are the same for this particular run.\n",
    "    \n",
    "    * To check for this, I performed another round of experiments where the mu and sigma KL weights were identical all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/03_klstudy.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeezing the stat column names by removing singular levels  \n",
    "stcol_longnames = statdf.columns.tolist()\n",
    "stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "data = []\n",
    "\n",
    "for tab_ttl, inc_spec, sort_cols in [\n",
    "    ('KL Weight (Mu)', {'cri/kl/sig/w': 0.001}, ['cri/kl/mu/w', 'nn/ltnt/dim']), \n",
    "    ('KL Weight (Sigma)', {'cri/kl/mu/w': 0.001}, ['cri/kl/sig/w', 'nn/ltnt/dim'])]:\n",
    "    \n",
    "    tab_idx = get_dfidxs(hpdf, inc_spec)\n",
    "    hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "\n",
    "    tab_idx2 = hpdf_tab.sort_values(sort_cols, kind='stable').index\n",
    "    hpdf_tab = hpdf_tab.loc[tab_idx2, :].reset_index(drop=True)\n",
    "    stdf_tab = stdf_tab.loc[tab_idx2, :].reset_index(drop=True)\n",
    "\n",
    "    # Aggregating the data\n",
    "    agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "    stcols_tab = agg_data['stcols']\n",
    "\n",
    "    data.append((tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/07_klstudy.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/08_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "hpdf_tab, statdf_tab = hpdf, statdf\n",
    "hpdf_tab = hpdf_tab.rename(columns={'nn/ltnt/dim': 'nn/latent/dim'})\n",
    "agg_data = get_aggdf(hpdf_tab, statdf_tab, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "data2 = [('All Dimensions', agg_data['hpdf'], agg_data['stdf'], agg_data['stcols'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/09_klstudy.yml'\n",
    "dashdata = get_dashdata(data2, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/10_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following plot to get a sense of the TV and Hellinger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(3*3.0, 2*2.4), sharex=True, sharey=True, dpi=100)\n",
    "    axes = np.array(axes).ravel()\n",
    "\n",
    "df_rowdicts = []\n",
    "x = np.linspace(-5, 12, 10000)\n",
    "for ax_idx, loc in enumerate((1, 2, 3, 4, 5, 6)):\n",
    "    p1 = norm.pdf(x, loc=0)\n",
    "    p2 = norm.pdf(x, loc=loc)\n",
    "    tv_trg = 0.5 * np.abs(p1 - p2).sum() * x.ptp() / x.size\n",
    "    h2_trg = 1 - np.sqrt(p1 * p2).sum() * x.ptp() / x.size\n",
    "    h1_trg = np.sqrt(h2_trg)\n",
    "    row_dict = {'Delta Mu': loc, 'TV': tv_trg, \n",
    "        'Hellinger': h1_trg, 'Hellinger-Squared': h2_trg}\n",
    "    df_rowdicts.append(row_dict)\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax.plot(x, p1, color='blue', lw=3)\n",
    "    ax.plot(x, p2, color='red', lw=3)\n",
    "\n",
    "    ax.text(0.95, 0.70, f'$\\Delta\\mu={loc}$\\nTV={tv_trg:0.2f}\\nH1={h1_trg:0.2f}',\n",
    "        verticalalignment='bottom', horizontalalignment='right',\n",
    "        transform=ax.transAxes, color='white', fontsize=10)\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "pd.DataFrame(df_rowdicts).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: KL Weight Study on the Histogram VAE\n",
    "\n",
    "Related Configs:\n",
    "\n",
    "  * `configs/02_adhoc/06_klstudy.yml`\n",
    "\n",
    "  * `configs/02_adhoc/07_klstudy.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * The third round of studying the effect of KL mu and sigma weights on the newly implemented performance metrics.\n",
    "    \n",
    "  * I used identical KL mu and sigma weights, and studied the unified KL weight effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/04_klstudy.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']\n",
    "\n",
    "# Squeezing the stat column names by removing singular levels  \n",
    "stcol_longnames = statdf.columns.tolist()\n",
    "stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "\n",
    "tab_ttl = 'KL Weight'\n",
    "tab_idx = hpdf.sort_values(['nn/ltnt/dim', 'cri/kl/w'], kind='stable').index\n",
    "hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "\n",
    "# Aggregating the data\n",
    "agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "stcols_tab = agg_data['stcols']\n",
    "\n",
    "data = [(tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/11_klstudy.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/12_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "hpdf_tab, statdf_tab = hpdf, statdf\n",
    "hpdf_tab = hpdf_tab.rename(columns={'nn/ltnt/dim': 'nn/latent/dim'})\n",
    "agg_data = get_aggdf(hpdf_tab, statdf_tab, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "data2 = [('All Dimensions', agg_data['hpdf'], agg_data['stdf'], agg_data['stcols'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/13_klstudy.yml'\n",
    "dashdata = get_dashdata(data2, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/14_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: KL Weight Study on the Histogram VAE\n",
    "\n",
    "Related config: `configs/02_adhoc/08_klstudy.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * The fourth round of studying the effect of KL mu and sigma weights on the newly implemented performance metrics.\n",
    "    \n",
    "  * See Experiment 4 for previous issues.\n",
    "\n",
    "Issues:\n",
    "\n",
    "  * This is a similar config to `configs/02_adhoc/05_klstudy.yml`\n",
    "\n",
    "    * The KL mu and sigma weights are sweeped over independently at a base value of 0.001\n",
    "    \n",
    "  * There was a bug in TV calculation in the previous run. \n",
    "  \n",
    "    * That's the main reason I took this run. \n",
    "  \n",
    "  * I made a few other adjustments as well\n",
    "    \n",
    "    * The metric evaluation reductions were changed to `mean` instead of `sum`.\n",
    "    \n",
    "    * The Hellinger's TV approximantion is now capped at `1.0`.\n",
    "    \n",
    "    * A legend labeling bug in the plots was fixed in this revision.\n",
    "    \n",
    "    * Noisy reconstruction evaluation and metrics were added here.\n",
    "    \n",
    "    * `10th` and `100th` neighbors were added to congestion metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/05_klstudy.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeezing the stat column names by removing singular levels\n",
    "statdf = adjust_mtrcnames(statdf)\n",
    "stcol_longnames = statdf.columns.tolist()\n",
    "stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "data = []\n",
    "\n",
    "for tab_ttl, inc_spec, sort_cols in [\n",
    "    ('KL Weight (Mu)', {'cri/kl/sig/w': 0.001}, ['cri/kl/mu/w', 'nn/ltnt/dim']), \n",
    "    ('KL Weight (Sigma)', {'cri/kl/mu/w': 0.001}, ['cri/kl/sig/w', 'nn/ltnt/dim'])]:\n",
    "    \n",
    "    tab_idx = get_dfidxs(hpdf, inc_spec)\n",
    "    hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "\n",
    "    tab_idx2 = hpdf_tab.sort_values(sort_cols, kind='stable').index\n",
    "    hpdf_tab = hpdf_tab.loc[tab_idx2, :].reset_index(drop=True)\n",
    "    stdf_tab = stdf_tab.loc[tab_idx2, :].reset_index(drop=True)\n",
    "\n",
    "    # Aggregating the data\n",
    "    agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "    stcols_tab = agg_data['stcols']\n",
    "\n",
    "    data.append((tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/15_klstudy.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/16_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "hpdf_tab, statdf_tab = hpdf, statdf\n",
    "hpdf_tab = hpdf_tab.rename(columns={'nn/ltnt/dim': 'nn/latent/dim'})\n",
    "agg_data = get_aggdf(hpdf_tab, statdf_tab, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "data2 = [('All Dimensions', agg_data['hpdf'], agg_data['stdf'], agg_data['stcols'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/17_klstudy.yml'\n",
    "dashdata = get_dashdata(data2, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/18_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following plot to get a sense of the TV and Hellinger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(3*3.0, 2*2.4), sharex=True, sharey=True, dpi=100)\n",
    "    axes = np.array(axes).ravel()\n",
    "\n",
    "df_rowdicts = []\n",
    "x = np.linspace(-5, 12, 10000)\n",
    "for ax_idx, loc in enumerate((1, 2, 3, 4, 5, 6)):\n",
    "    p1 = norm.pdf(x, loc=0)\n",
    "    p2 = norm.pdf(x, loc=loc)\n",
    "    tv_trg = 0.5 * np.abs(p1 - p2).sum() * x.ptp() / x.size\n",
    "    h2_trg = 1 - np.sqrt(p1 * p2).sum() * x.ptp() / x.size\n",
    "    h1_trg = np.sqrt(h2_trg)\n",
    "    row_dict = {'Delta Mu': loc, 'TV': tv_trg, \n",
    "        'Hellinger': h1_trg, 'Hellinger-Squared': h2_trg}\n",
    "    df_rowdicts.append(row_dict)\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax.plot(x, p1, color='blue', lw=3)\n",
    "    ax.plot(x, p2, color='red', lw=3)\n",
    "\n",
    "    ax.text(0.95, 0.70, f'$\\Delta\\mu={loc}$\\nTV={tv_trg:0.2f}\\nH1={h1_trg:0.2f}',\n",
    "        verticalalignment='bottom', horizontalalignment='right',\n",
    "        transform=ax.transAxes, color='white', fontsize=10)\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "pd.DataFrame(df_rowdicts).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib Realism vs. KL Weight Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 89\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "pltdfmu = pd.concat(data[0][1: 3], axis=1)\n",
    "for col in pltdfmu.columns:\n",
    "    pltdfmu[col] = pltdfmu[col].tolist()\n",
    "pltdfsig = pd.concat(data[1][1: 3], axis=1)\n",
    "for col in pltdfsig.columns:\n",
    "    pltdfsig[col] = pltdfsig[col].tolist()\n",
    "\n",
    "with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "    n_figrows, n_figcols = 2, 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_figrows, n_figcols, \n",
    "        figsize=[2.6 * n_figcols, 2.2 * n_figrows], \n",
    "        dpi=140, sharex=False, sharey=True, squeeze=False)\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    for i_figrow, kl_wmuorsig, pltdf in [(0, 'klwsig', pltdfsig), (1, 'klwmu', pltdfmu)]:\n",
    "        for i_figcol, svd_alpha in [(0, 1), (1, 0), (2, -1)]:\n",
    "            ax = axes[i_figrow, i_figcol]\n",
    "            plt_cfg = {'fig': fig, 'ax': ax, **v_mplcfgs[f'paper.realism.{kl_wmuorsig}.ldim']}\n",
    "            plt_cfg['ycol'] = f'perf/realism/wslcwass2:{svd_alpha:.1f}/abserr/median'\n",
    "            plt_cfg['ylabel'] = 'Realism Error' if (i_figcol == 0) else None\n",
    "            if (i_figrow, i_figcol) != (0, 0):\n",
    "                plt_cfg = {key: val for key, val in deep2hie(plt_cfg).items() \n",
    "                    if 'ax.annotate/text' not in key}\n",
    "                plt_cfg = hie2deep(plt_cfg)\n",
    "\n",
    "            fig, ax = draw_matplotlib(plt_cfg, pltdf)\n",
    "            tag_axis(ax, f'({\"abc\"[i_figcol]}$_{{{i_figrow + 1}}}$)', fontsize=12, pad=(0.3, 0.6))\n",
    "    \n",
    "    for i_figrow, row_ttl in enumerate(['KL-$\\sigma$ Study', 'KL-$\\mu$ Study']):\n",
    "        print_axheader(axes[i_figrow, 0], row_ttl, 'left', \n",
    "            fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for i_figcol, svd_alpha in [(0, 1), (1, 0), (2, -1)]:\n",
    "        col_ttl = f'$\\\\alpha={svd_alpha}$'\n",
    "        print_axheader(axes[0, i_figcol], col_ttl, 'top', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.5)\n",
    "\n",
    "pdfpath = f'{workdir}/{i_fig:02d}_realism_klw.pdf'\n",
    "fig.savefig(pdfpath, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib Realism vs. KL Weight Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fig = 90\n",
    "plt.ioff()\n",
    "\n",
    "# Reading the experiment to fpidx specification configs\n",
    "with open(f'{workdir}/37_aerompl.yml', 'r') as fp:\n",
    "    v_mplcfgsraw = ruyaml.load(fp, ruyaml.RoundTripLoader)\n",
    "v_mplcfgs = hie2deep(parse_refs(v_mplcfgsraw), maxdepth=1)\n",
    "\n",
    "pltdfmu = pd.concat(data[0][1: 3], axis=1)\n",
    "for col in pltdfmu.columns:\n",
    "    pltdfmu[col] = pltdfmu[col].tolist()\n",
    "pltdfsig = pd.concat(data[1][1: 3], axis=1)\n",
    "for col in pltdfsig.columns:\n",
    "    pltdfsig[col] = pltdfsig[col].tolist()\n",
    "\n",
    "with plt.rc_context(v_mplcfgs['rc_context']) as pltrcctx:\n",
    "    n_figrows, n_figcols = 2, 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_figrows, n_figcols, \n",
    "        figsize=[2.6 * n_figcols, 2.2 * n_figrows], \n",
    "        dpi=140, sharex=True, sharey=True, squeeze=False)\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    for i_figrow, kl_wmuorsig, pltdf in [(0, 'klwsig', pltdfsig), (1, 'klwmu', pltdfmu)]:\n",
    "        for i_figcol, svd_alpha in [(0, 1), (1, 0), (2, -1)]:\n",
    "            ax = axes[i_figrow, i_figcol]\n",
    "            plt_cfg = {'fig': fig, 'ax': ax, **v_mplcfgs[f'paper.realism.ldim.{kl_wmuorsig}']}\n",
    "            plt_cfg['ycol'] = f'perf/realism/wslcwass2:{svd_alpha:.1f}/abserr/median'\n",
    "            plt_cfg['ylabel'] = 'Realism Error' if (i_figcol == 0) else None\n",
    "            plt_cfg['xlabel'] = 'Latent Dimension' if (i_figrow == n_figrows - 1) else None\n",
    "            if i_figcol != 2:\n",
    "                plt_cfg = {key: val for key, val in deep2hie(plt_cfg).items() \n",
    "                    if 'ax.annotate/text' not in key}\n",
    "                plt_cfg = hie2deep(plt_cfg)\n",
    "            fig, ax = draw_matplotlib(plt_cfg, pltdf)\n",
    "        \n",
    "            tag_axis(ax, f'({\"abc\"[i_figcol]}$_{{{i_figrow + 1}}}$)', fontsize=12, pad=(0.3, 0.6))\n",
    "    \n",
    "    for i_figrow, row_ttl in enumerate(['KL-$\\sigma$ Study', 'KL-$\\mu$ Study']):\n",
    "        print_axheader(axes[i_figrow, 0], row_ttl, 'left', \n",
    "            fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for i_figcol, svd_alpha in [(0, 1), (1, 0), (2, -1)]:\n",
    "        col_ttl = f'$\\\\alpha={svd_alpha}$'\n",
    "        print_axheader(axes[0, i_figcol], col_ttl, 'top', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.25)\n",
    "\n",
    "pdfpath = f'{workdir}/{i_fig:02d}_realism_dltnt.pdf'\n",
    "fig.savefig(pdfpath, bbox_inches='tight')\n",
    "print(f'Finished writing {pdfpath}')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: KL Weight Study on the Histogram VAE\n",
    "\n",
    "Related Configs:\n",
    "\n",
    "  * `configs/02_adhoc/09_klstudy.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * This is a similar config to `configs/02_adhoc/06_klstudy.yml` and `configs/02_adhoc/07_klstudy.yml`\n",
    "\n",
    "    * The KL mu and sigma weights are sweeped over identically.\n",
    "    \n",
    "  * There was a bug in TV calculation in the previous run. That's why I took this run. \n",
    "\n",
    "  * I made a few other adjustments as well\n",
    "    \n",
    "    * The metric evaluation reductions were changed to `mean` instead of `sum`.\n",
    "    \n",
    "    * The Hellinger's TV approximantion is now capped at `1.0`.\n",
    "    \n",
    "    * A legend labeling bug in the plots was fixed in this revision.\n",
    "    \n",
    "    * Noisy reconstruction evaluation and metrics were added here.\n",
    "    \n",
    "    * `10th` and `100th` neighbors were added to congestion metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/06_klstudy.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']\n",
    "\n",
    "# Squeezing the stat column names by removing singular levels\n",
    "statdf = adjust_mtrcnames(statdf)\n",
    "stcol_longnames = statdf.columns.tolist()\n",
    "stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "\n",
    "tab_ttl = 'KL Weight'\n",
    "tab_idx = hpdf.sort_values(['nn/ltnt/dim', 'cri/kl/w'], kind='stable').index\n",
    "hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "\n",
    "# Aggregating the data\n",
    "agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "stcols_tab = agg_data['stcols']\n",
    "\n",
    "data = [(tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/19_klstudy.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/20_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "hpdf_tab, statdf_tab = hpdf, statdf\n",
    "hpdf_tab = hpdf_tab.rename(columns={'nn/ltnt/dim': 'nn/latent/dim'})\n",
    "agg_data = get_aggdf(hpdf_tab, statdf_tab, xcol='epoch', \n",
    "    huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "data2 = [('All Dimensions', agg_data['hpdf'], agg_data['stdf'], agg_data['stcols'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/21_klstudy.yml'\n",
    "dashdata = get_dashdata(data2, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/22_klstudy.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: Testing the New Pre-Processing Framework\n",
    "\n",
    "Related Configs: \n",
    "\n",
    "  * `configs/02_adhoc/10_pppoly.yml`\n",
    "\n",
    "Goals: \n",
    "\n",
    "  * Just getting a VAE training with the new PP framework.\n",
    "\n",
    "  * Quick and dirty OVAT experiment of most hyper-parameters in the MLP architectures.\n",
    "\n",
    "Issues:\n",
    "\n",
    "  * I made a mistake in preparing the training configs:\n",
    "\n",
    "    1. The realism metric was defined on the pre-processed mass variables.\n",
    "\n",
    "    2. For this reason, it cannot be trusted as an apple to apple comparison.\n",
    "\n",
    "    3. The future training configs will contain seperate x-realism and u-realism metrics.\n",
    "\n",
    "  * My main observation was that, in terms of the realism metric, the `nn/pp/nrmscl` made a lot of impact:\n",
    "\n",
    "    1. When turned on the realism was 0.15 vs. when it was turned off the realism was 0.02.\n",
    "\n",
    "    2. That being said, the aerosol metrics did not make much of a deal about this particular hyper-parameter.\n",
    "\n",
    "    3. Since the realism metric for this training was defined on the pre-processed variables, this observation cannot be trusted.\n",
    "\n",
    "  * Another issue was that the `nn/pp/magdim` ablations were absent:\n",
    "\n",
    "    1. The VAE trainings failed with `NaN` values when `nn/pp/magdim` was either `n_chem` or `one`.\n",
    "\n",
    "    2. This may be related to the mis-specification of the `nn/pp/nrmscl` hyper-parameter again.\n",
    "\n",
    "  * The next issue with the data was that in the training curves of `notebooks/08_plotting/24_vaehist.html`:\n",
    "\n",
    "    1. The `performane/reconstruction/noisy/test` metric had a wild range.\n",
    "    \n",
    "    2. This range specifically expanded in the early epochs for the `nn/pp/nrmexp` (normalization exponent) hyper-parameter ablations.\n",
    "\n",
    "  * Another poor outcome was the proximity metric:\n",
    "\n",
    "    1. The KL weight ablations caused a lot of unbounded behavior for the proximity metric.\n",
    "\n",
    "    2. Other hyper-parameters were also sporadically causing this metric to go wild.\n",
    "\n",
    "Incident Report:\n",
    "\n",
    "  * One issue with the results was that the `nn/pp/magdim` ablations were absent:\n",
    "\n",
    "    1. The VAE trainings failed with `NaN` values generated at the first epoch when `nn/pp/magdim` was either `n_chem` or `one`.\n",
    "\n",
    "    2. This may be related to the mis-specification of the `nn/pp/nrmscl` hyper-parameter again.\n",
    "\n",
    "    3. Upon further investigation, I realized that with `nn/pp/magdim=n_chem`,\n",
    "\n",
    "      1. Some of of the `sig08` pp parameters were on the order of 3e-7. \n",
    "\n",
    "      2. This was happening since some of the leftmost bins in the normalized mass tensor could have been zero all the time.\n",
    "\n",
    "      3. In the test split, a value of 0.4 was getting divided by this 3e-7 element, producing a value of roughly 600_000.\n",
    "\n",
    "      4. This value of 600_000 was getting fed to the encoder!\n",
    "\n",
    "      5. The encoder was using ReLU. This resulted in a very large log-sigma predictions by the encoder.\n",
    "\n",
    "      6. Therefore, the z sigma value for this data point were becoming inf. \n",
    "\n",
    "      7. When sampling z, this infinite sigma was multiplied by a sigscale of 0, producing a NaN!\n",
    "\n",
    "  * The following measures were taken to address this issue in the next rounds:\n",
    "\n",
    "    1. I added an `nrmscleps` hyper-parameter to the pre-porcessor in the next revision.\n",
    "\n",
    "        1. Similarly, the `magscleps` and `cntscleps` hyper-parameters were also added.\n",
    "\n",
    "        2. I've set the default for these to zero (i.e., identical to before).\n",
    "\n",
    "        3. If these hyper-parameters had showen promise after *full trainings*, I would have adjusted their defaults.\n",
    "\n",
    "        4. However, as I saw in later experiments, they made marginal improvements to test performance metrics.\n",
    "\n",
    "    2. I also changed the exp function, into a piecewise exp-linear function to get the z sigma \n",
    "    \n",
    "        1. See the related `nn/ltnt/sig/tnsfm` hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smrypath = '../summary/07_pppoly.h5'\n",
    "get_h5du(smrypath, verbose=True, detailed=False)\n",
    "data = load_h5data(smrypath)\n",
    "hpdf = data['hp']\n",
    "statdf = data['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeezing the stat column names by removing singular levels\n",
    "statdf = adjust_mtrcnames(statdf)\n",
    "stcol_longnames = statdf.columns.tolist()\n",
    "stcol_sqzdnames = squeeze_colnames(stcol_longnames, mindepth=2)\n",
    "statdf = statdf.rename(columns=dict(zip(stcol_longnames, stcol_sqzdnames)))\n",
    "\n",
    "# Dropping the quantile data to save on space :)\n",
    "drop_pats = ['q10', 'q25', 'q5', 'q75', 'q90', 'q95', 'median', '/kl:']\n",
    "keepcols = [col for col in statdf.columns if not any(x in col for x in drop_pats)]\n",
    "statdf = statdf[keepcols] \n",
    "\n",
    "# Downcasting numerical types to save on space\n",
    "statdf = downcast_df(statdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `fpicols` will be an `fpidxgrp` to hp column mapping; each fpidxgrp \n",
    "# is part of an ovat ablation defined by a single column.\n",
    "ii_drop = hpdf['fpidxgrp'].drop_duplicates().index\n",
    "hpdf2 = hpdf.loc[ii_drop].reset_index(drop=True)\n",
    "\n",
    "fpicols = dict()\n",
    "fpidxgrps = hpdf2['fpidxgrp']\n",
    "main_fpidx = fpidxgrps.iloc[0]\n",
    "for fpidx in fpidxgrps.iloc[1:]:\n",
    "    hpdf3 = hpdf2[fpidxgrps.isin([fpidx, main_fpidx])]\n",
    "    hpdf4 = drop_unqcols(hpdf3)\n",
    "    hpdf5 = hpdf4.drop(columns=['fpidx', 'fpidxgrp'], errors='ignore')\n",
    "    cols = hpdf5.columns.tolist()\n",
    "    fpicols[fpidx] = hpdf5.columns.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcfg = dict(type='bootstrap', n_boot=40, q=[5, 95], stat='mean', device='cpu')\n",
    "\n",
    "tab_ttl1 = 'General Ablations'\n",
    "tab_fpidxs1 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() if 'nn/pp' not in ovatcol]\n",
    "\n",
    "tab_ttl2 = 'PP Magnitude Ablations'\n",
    "tab_fpidxs2 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() \n",
    "    if ovatcol in ['nn/pp/magpnrm', 'nn/pp/magexp', 'nn/pp/magshft', 'nn/pp/magscl']]\n",
    "\n",
    "tab_ttl3 = 'PP Normalization Ablations'\n",
    "tab_fpidxs3 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() \n",
    "    if ovatcol in ['nn/pp/eps', 'nn/pp/nrmexp', 'nn/pp/nrmshft', 'nn/pp/nrmscl', 'nn/pp/nrmscldim']]\n",
    "\n",
    "tab_ttl4 = 'PP Count Ablations'\n",
    "tab_fpidxs4 = [main_fpidx] + [fpidx for fpidx, ovatcol in fpicols.items() \n",
    "    if ovatcol in ['nn/pp/cnteps', 'nn/pp/cntexp', 'nn/pp/cntshft', 'nn/pp/cntscldim']]\n",
    "\n",
    "# Aggregating the data\n",
    "data = []\n",
    "for tab_ttl, tab_fpidxs in [(tab_ttl1, tab_fpidxs1), \n",
    "    (tab_ttl2, tab_fpidxs2), (tab_ttl3, tab_fpidxs3), (tab_ttl4, tab_fpidxs4)]:\n",
    "    tab_idx = (hpdf['fpidxgrp'].isin(tab_fpidxs))\n",
    "    hpdf_tab = hpdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    stdf_tab = statdf.loc[tab_idx, :].reset_index(drop=True)\n",
    "    agg_data = get_aggdf(hpdf_tab, stdf_tab, xcol='epoch', \n",
    "        huecol='fpidxgrp', rngcol='rng_seed', aggcfg=aggcfg)\n",
    "    hpdf_tabagg, stdf_tabagg = agg_data['hpdf'], agg_data['stdf']\n",
    "    stcols_tab = agg_data['stcols']\n",
    "    data.append((tab_ttl, hpdf_tabagg, stdf_tabagg, stcols_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/23_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/24_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymlpath = f'{workdir}/25_vaehist.yml'\n",
    "dashdata = get_dashdata(data, ymlpath, write_yml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllayout = build_dashboard(None, **dashdata)\n",
    "output_file(f'{workdir}/26_vaehist.html')\n",
    "save(fulllayout, title=dashdata['header'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
