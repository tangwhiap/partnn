{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BOKEH_VALIDATE_DOC'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "from copy import deepcopy\n",
    "from textwrap import dedent\n",
    "from string import Formatter\n",
    "from ruamel import yaml as ruyaml\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import bokeh\n",
    "# Don't remove the stack import line; for some buggy \n",
    "# reason Bokeh stacked bars don't work properly without it!\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.transform import stack\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.models import RadioButtonGroup\n",
    "from bokeh.models import CustomJSTickFormatter\n",
    "from bokeh.models import TabPanel, Tabs, Tooltip\n",
    "from bokeh.models import Select, Div, BoxZoomTool\n",
    "from bokeh.models import Slider, CategoricalSlider\n",
    "from bokeh.layouts import layout, row, column, gridplot, grid\n",
    "from bokeh.models import ColumnDataSource, CDSView, IndexFilter, HoverTool, CustomJSHover, BasicTicker\n",
    "\n",
    "from partnn.io_cfg import cache_dir\n",
    "from partnn.io_utils import eval_formula, deep2hie\n",
    "from partnn.io_utils import resio, drop_unqcols, hie2deep, filter_df, CartDF \n",
    "from partnn.io_utils import decomp_df, save_h5data, load_h5data, parse_refs, get_subdict\n",
    "from bokeh.core.properties import without_property_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './10_bokeh'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "suppdir = f'{workdir}/supplement'\n",
    "os.makedirs(suppdir, exist_ok=True)\n",
    "\n",
    "# The CLI-invoked dashboard theme should be light to be able to differentiate the colors and blobs\n",
    "doc_theme = 'light_minimal'\n",
    "background_color = 'white'\n",
    "header_color = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# The default theme inside the jupyter notebook should be dark (less differentiation between blobs)\n",
    "pn.config.theme = 'dark'\n",
    "pn.extension('katex', theme=\"dark\")\n",
    "doc_theme = 'dark_minimal'\n",
    "background_color = 'black'\n",
    "header_color = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Renamer:\n",
    "    def __init__(self, rnm_dict=None):\n",
    "        self.dict = dict() if rnm_dict is None else rnm_dict\n",
    "        self.invdict = {val: key for key, val in self.dict.items()}\n",
    "        assert len(self.dict) == len(self.invdict), dedent(f'''\n",
    "            Conflicting target names found:\n",
    "                Renaming Dictionary: {self.dict}''')\n",
    "    \n",
    "    def encode(self, key):\n",
    "        return self.dict.get(key, key)\n",
    "\n",
    "    def decode(self, key):\n",
    "        return self.invdict.get(key, key)\n",
    "\n",
    "class ColorManager:\n",
    "    def __init__(self, dflt_colors, color_spec):\n",
    "        self.dflt_colors = dflt_colors\n",
    "        self.color_spec = color_spec\n",
    "        self.color_palette = list(color_spec.values())\n",
    "        self.dict = dict()\n",
    "    \n",
    "    def __call__(self, lbl_name):\n",
    "        color = self.dict.get(lbl_name, None)\n",
    "\n",
    "\n",
    "        if color is None:\n",
    "            for pat, clr in self.dflt_colors.items():\n",
    "                if fnmatch.fnmatch(lbl_name, pat):\n",
    "                    color = self.color_spec.get(clr, clr)\n",
    "                    self.dict[lbl_name] = color\n",
    "                    break\n",
    "\n",
    "        if color is None:\n",
    "            cntr = Counter(self.dict.values())\n",
    "            color = min(self.color_palette, key=lambda color: cntr[color])\n",
    "            self.dict[lbl_name] = color\n",
    "\n",
    "        return color\n",
    "\n",
    "def scifrmt(val, latex=True):\n",
    "    val2 = f'{val:1.1e}'.replace(\"e-0\", \"e-\")\n",
    "    if latex:\n",
    "        aa, bb = val2.split('e')\n",
    "        out = fr'$${aa}\\times 10^{{{bb}}}$$'\n",
    "    else:\n",
    "        out = val2\n",
    "    return out\n",
    "\n",
    "def get_ychemgrp(i_fig, fig_name, n_framerows, n_framecols):\n",
    "    chem_name = fig_name.split('/')[-1]\n",
    "    assert chem_name in ('SO4', 'NO3', 'Cl', 'NH4', 'ARO1', 'ARO2', 'ALK1', \n",
    "        'OLE1', 'API1', 'Na', 'OIN', 'OC', 'BC', 'MOC', 'H2O', 'every')\n",
    "    return chem_name\n",
    "\n",
    "code_engfmttr = '''\n",
    "    if (Number.isNaN(tick)) {\n",
    "        console.log(\"The tick value is NaN: \" + tick);\n",
    "        return tick;\n",
    "    } else if (tick == 0) {\n",
    "        // console.log(\"The tick value is zero: \" + tick);\n",
    "        return tick;\n",
    "    } else if (!(!isNaN(tick))) {\n",
    "        console.log(\"The tick value is not a number: \" + tick);\n",
    "        return tick;\n",
    "    }\n",
    "    \n",
    "    let tick_abs = Math.abs(tick);\n",
    "    let tick_log10 = Math.log10(tick_abs);\n",
    "    let tick_log10d3 = 3 * Math.floor(tick_log10 / 3);\n",
    "\n",
    "    if (Number.isNaN(tick_log10)) {\n",
    "        console.log(\"The abs tick value is non zero yet the log abs is nan: tick=\" + tick + \", logabs: \" + tick_log10);\n",
    "        return tick;\n",
    "    }\n",
    "\n",
    "    console.assert(tick_log10 >= tick_log10d3, \n",
    "        \"tick_log10=\" + tick_log10 + \" is not greater than tick_log10d3=\" + tick_log10d3);\n",
    "\n",
    "    if (tick_log10d3 in pstfxs) {\n",
    "        var tick_pstfx = pstfxs[tick_log10d3];\n",
    "        var tick_log10_rmng = tick_log10 - tick_log10d3;\n",
    "    } else if (tick_log10d3 < minpstfx) {\n",
    "        var tick_pstfx = pstfxs[minpstfx];\n",
    "        var tick_log10_rmng = tick_log10 - minpstfx;\n",
    "    } else if (tick_log10d3 > maxpstfx) {\n",
    "        var tick_pstfx = pstfxs[maxpstfx];\n",
    "        var tick_log10_rmng = tick_log10 - maxpstfx;\n",
    "    } else {\n",
    "        throw new Error(\"Odd case happened! \" + tick_log10d3);\n",
    "    }\n",
    "\n",
    "    let tick_prfx = Math.pow(10, tick_log10_rmng);\n",
    "    let precision2 = (precision === null) ? (tick_prfx < 10) ? 2 : ((tick_prfx < 100) ? 1 : 0) : precision;\n",
    "    let tick_eng = \" \" + parseFloat(Math.sign(tick) * tick_prfx).toFixed(precision2) + tick_pstfx;\n",
    "    return tick_eng;\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating the Pre-Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import netCDF4\n",
    "from ruamel import yaml as ruyaml\n",
    "from partnn.io_cfg import data_dir\n",
    "from partnn.tch_utils import BatchRNG\n",
    "from partnn.notebooks.n09_utils import make_pp, get_snrtspltidxs\n",
    "from partnn.io_utils import get_subdict, preproc_cfgdict, parse_refs\n",
    "\n",
    "json_cfgpath = '../configs/02_adhoc/08_klstudy.yml'\n",
    "if json_cfgpath.endswith('.json'):\n",
    "    with open(json_cfgpath, 'r') as fp:\n",
    "        json_cfgdict = json.load(fp, object_pairs_hook=dict)\n",
    "elif json_cfgpath.endswith('.yml'):\n",
    "    with open(json_cfgpath, \"r\") as fp:\n",
    "        json_cfgdict = dict(ruyaml.safe_load(fp))\n",
    "else:\n",
    "    raise RuntimeError(f'unknown config extension: {json_cfgpath}')\n",
    "\n",
    "json_cfgdict['io/config_id'] = '08_klstudy_00'\n",
    "json_cfgdict['io/results_dir'] = './09_vaehist/results'\n",
    "json_cfgdict['io/storage_dir'] = './09_vaehist/storage'\n",
    "json_cfgdict['io/tch/device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Applying the looping processes\n",
    "all_cfgdicts1 = preproc_cfgdict(json_cfgdict)[:1]\n",
    "\n",
    "# Parsing all the \"/ref\"-, \"/def\"-, and \"/pop\"-ending keys\n",
    "all_cfgdicts = [parse_refs(cfgdict, trnsfrmtn='hie', pathsep=' -> ') \n",
    "    for cfgdict in all_cfgdicts1]\n",
    "\n",
    "# Dropping the trailing references section\n",
    "all_refdicts = [get_subdict(cfgdict, prefix='refs', pop=True) \n",
    "    for cfgdict in all_cfgdicts]\n",
    "\n",
    "cfg_dict_input = all_cfgdicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = cfg_dict_input.copy()\n",
    "rng_seed_list = cfg_dict.pop('rng_seed/list')\n",
    "pp_mdlscfg = get_subdict(cfg_dict, 'pp/modules', pop=True)\n",
    "ppcri_type = cfg_dict.pop('cri/rcnst/pp/type')\n",
    "#########################################################\n",
    "###################### Data Options #####################\n",
    "#########################################################\n",
    "data_cfg = get_subdict(cfg_dict, 'data', pop=False)\n",
    "data_tree = cfg_dict.pop('data/tree')\n",
    "data_chems = cfg_dict.pop('data/chems', None)\n",
    "data_binsi1 =  cfg_dict.pop('data/bins/idx/start', 0)\n",
    "data_binsi2 =  cfg_dict.pop('data/bins/idx/end', None)\n",
    "split_cfg = get_subdict(cfg_dict, 'split', pop=True)\n",
    "\n",
    "device_name = cfg_dict.pop('io/tch/device')\n",
    "dtype_name = cfg_dict.pop('io/tch/dtype')\n",
    "name2dtype = dict(float64=torch.double,\n",
    "    float32=torch.float32,\n",
    "    float16=torch.float16)\n",
    "tch_device = torch.device(device_name)\n",
    "tch_dtype = name2dtype[dtype_name]\n",
    "\n",
    "eval_bs = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#################### Loading the Data ###################\n",
    "#########################################################\n",
    "data_path = f'{data_dir}/{data_tree}'\n",
    "assert data_path.endswith('.nc')\n",
    "with netCDF4.Dataset(data_path, \"r\") as fp:\n",
    "    ################# The Array Dimensions #################\n",
    "    # The number of scenarios\n",
    "    n_snr = fp.dimensions['n_snr'].size\n",
    "    # The number of time steps in each scenario\n",
    "    n_t = fp.dimensions['n_t'].size\n",
    "    # The number of diameter histogram bins\n",
    "    k_bins = fp.dimensions['k_bins'].size\n",
    "    # The number of chemical specices\n",
    "    n_chem = fp.dimensions['n_chem'].size\n",
    "\n",
    "    #################### The Data Arrays ####################\n",
    "    # The number of particles within each diameter bin\n",
    "    n_prthst = fp.variables['n_prthst'][:]\n",
    "    assert n_prthst.shape == (n_snr, n_t, k_bins)\n",
    "\n",
    "    # The mass within each diameter bin (the mass histogram)\n",
    "    m_prthst = fp.variables['m_prthst'][:]\n",
    "    assert m_prthst.shape == (n_snr, n_t, k_bins)\n",
    "\n",
    "    # The mass of each chemical species within the diameter bins\n",
    "    m_chmprthst = np.moveaxis(fp.variables['m_chmprthst'][:], -1, -2)\n",
    "    assert m_chmprthst.shape == (n_snr, n_t, n_chem, k_bins)\n",
    "\n",
    "    ####################### Meta-Data #######################\n",
    "    # The Chemical Species\n",
    "    chem_species_str  = 'SO4,NO3,Cl,NH4,MSA,ARO1,ARO2,ALK1,OLE1,API1'\n",
    "    chem_species_str += ',API2,LIM1,LIM2,CO3,Na,Ca,OIN,OC,BC,MOC,H2O'\n",
    "    chem_species = chem_species_str.split(',')\n",
    "\n",
    "    # The histogram bin ranges of value\n",
    "    diam_low, diam_high, diam_logbase = 1e-9, 1e-4, np.log(10)\n",
    "    logdiam_low, logdiam_high = np.log(diam_low) / diam_logbase, np.log(diam_high) / diam_logbase\n",
    "    logdiams = np.linspace(logdiam_low, logdiam_high, k_bins + 1, endpoint=True)\n",
    "    x_histbins = np.exp(logdiams * diam_logbase)\n",
    "    assert x_histbins.shape == (k_bins + 1,)\n",
    "\n",
    "# Revising the chemical species if necessary\n",
    "if data_chems not in (None, 'all'):    \n",
    "    i_keepchems = [chem_species.index(chem) for chem in data_chems]\n",
    "    chem_species = [chem_species[i_chm] for i_chm in i_keepchems]\n",
    "    chem_species_str = ','.join(chem_species)\n",
    "\n",
    "    n_chem = len(chem_species)\n",
    "    m_chmprthst = m_chmprthst[:, :, i_keepchems, :]\n",
    "    assert m_chmprthst.shape == (n_snr, n_t, n_chem, k_bins)\n",
    "\n",
    "    m_prthst = m_chmprthst.sum(axis=-2)\n",
    "    assert m_prthst.shape == (n_snr, n_t, k_bins)\n",
    "\n",
    "    # # number of particles must be re-calculated from the full data\n",
    "    # n_prthst = np.full_like(n_prthst, -1)\n",
    "    # assert n_prthst.shape == (n_snr, n_t, k_bins)\n",
    "\n",
    "# Revising the histogram bins if necessary\n",
    "data_binsi2 = k_bins if data_binsi2 is None else data_binsi2\n",
    "if (data_binsi1, data_binsi2) != (0, k_bins):\n",
    "    i_keepbins = list(range(data_binsi1, data_binsi2))\n",
    "    k_bins = len(i_keepbins)\n",
    "\n",
    "    # The number of particles within each diameter bin\n",
    "    n_prthst = n_prthst[:, :, i_keepbins]\n",
    "    assert n_prthst.shape == (n_snr, n_t, k_bins)\n",
    "\n",
    "    # The mass within each diameter bin (the mass histogram)\n",
    "    m_prthst = m_prthst[:, :, i_keepbins]\n",
    "    assert m_prthst.shape == (n_snr, n_t, k_bins)\n",
    "\n",
    "    # The mass of each chemical species within the diameter bins\n",
    "    m_chmprthst = m_chmprthst[:, :, :, i_keepbins]\n",
    "    assert m_chmprthst.shape == (n_snr, n_t, n_chem, k_bins)\n",
    "\n",
    "    # The histogram bin ranges of value\n",
    "    logdiams = logdiams[i_keepbins + [i_keepbins[-1] + 1]]\n",
    "    assert logdiams.shape == (k_bins + 1,)\n",
    "    logdiam_low, logdiam_high = logdiams[0], logdiams[-1]\n",
    "    diam_low, diam_high, diam_logbase = np.exp(logdiam_low), np.exp(logdiam_high), diam_logbase\n",
    "    x_histbins = np.exp(logdiams * diam_logbase)\n",
    "    assert x_histbins.shape == (k_bins + 1,)\n",
    "    \n",
    "#######################################\n",
    "#   Making the Data Variables Dict    #\n",
    "#######################################\n",
    "# `datanp_dict` is an input variable (str) to array (np.ndarray) mapping.\n",
    "datanp_dict = dict(\n",
    "    n_prthst=n_prthst.reshape(n_snr * n_t, 1, k_bins),\n",
    "    m_prthst=m_prthst.reshape(n_snr * n_t, 1, k_bins),\n",
    "    m_chmprthst=m_chmprthst.reshape(n_snr * n_t, n_chem, k_bins))\n",
    "\n",
    "# `data_dict` is an input variable (str) to tensor (torch.tensor) mapping.\n",
    "data_dict = {key: torch.from_numpy(varnp).to(device=tch_device, dtype=tch_dtype)\n",
    "            for key, varnp in datanp_dict.items()}\n",
    "\n",
    "# Preparing the data hash for later caching\n",
    "hash_data = {key: data_cfg for key in data_dict}\n",
    "\n",
    "# `node2chnlnames` is an input variable to labels (list) mapping.\n",
    "node2chnlnames = {'n_prthst': ['total number'],\n",
    "    'm_prthst': ['total mass'],\n",
    "    'm_prthstlog': ['log total mass'],\n",
    "    'm_chmprthst': chem_species_str.split(','),\n",
    "    'm_chmprtnrm': chem_species_str.split(','),\n",
    "    'm_chmhst': chem_species_str.split(',')}\n",
    "\n",
    "# Example:\n",
    "#   data_dims == {\n",
    "#       'n_prthst': (1, k_bins),\n",
    "#       'm_prthst': (1, k_bins),\n",
    "#       'm_chmprthst': (n_chem, k_bins)\n",
    "#   }\n",
    "data_dims = {key: tuple(varnp.shape[1:]) for key, varnp in datanp_dict.items()}\n",
    "\n",
    "# The shape variables\n",
    "shapevars = dict(n_snr=n_snr, n_t=n_t, n_chem=n_chem, k_bins=k_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "########### Constructing the Batch RNG Object ###########\n",
    "#########################################################\n",
    "n_seeds = len(rng_seed_list)\n",
    "rng_seeds = np.array(rng_seed_list)\n",
    "rng = BatchRNG(shape=(n_seeds,), lib='torch',\n",
    "    device=tch_device, dtype=tch_dtype,\n",
    "    unif_cache_cols=1_000_000,\n",
    "    norm_cache_cols=5_000_000)\n",
    "rng.seed(np.broadcast_to(rng_seeds, rng.shape))\n",
    "\n",
    "#########################################################\n",
    "######## Instantiating Train/Test Split Indecis #########\n",
    "#########################################################\n",
    "split_vars = split_cfg['vars']\n",
    "split_idxdict = get_snrtspltidxs(split_cfg, n_snr, n_t, n_seeds,\n",
    "    rng, tch_device, pop_opts=True)\n",
    "trn_spltidxs = split_idxdict['split_idxs']\n",
    "n_trn = trn_spltidxs.shape[-1]\n",
    "n_tst = n_snr * n_t - n_trn\n",
    "assert trn_spltidxs.shape == (n_seeds, n_trn)\n",
    "tst_spltidxs = split_idxdict['negsplit_idxs']\n",
    "assert tst_spltidxs.shape == (n_seeds, n_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dashboard Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_type = None\n",
    "\n",
    "# The chemical names\n",
    "allchem_names = ['SO4', 'NO3', 'Cl', 'NH4', 'ARO1', 'ARO2', 'ALK1', \n",
    "    'OLE1', 'API1', 'Na', 'OIN', 'OC', 'BC', 'MOC', 'H2O']\n",
    "n_chem = len(allchem_names)\n",
    "# A smaller subset of the chemicals\n",
    "chem_names = allchem_names\n",
    "chem_name2idxs = {chem_name: allchem_names.index(chem_name) for chem_name in chem_names}\n",
    "\n",
    "# The number of seeds in the experiments\n",
    "n_seeds, n_epochs, n_t, n_bins = 16, 2, 25, 20\n",
    "\n",
    "# The histogram bin ranges of value\n",
    "diam_low, diam_high, diam_logbase = 1e-9, 1e-4, np.log(10)\n",
    "logdiam_low, logdiam_high = np.log(diam_low) / diam_logbase, np.log(diam_high) / diam_logbase\n",
    "logdiams = np.linspace(logdiam_low, logdiam_high, n_bins + 1, endpoint=True)\n",
    "assert logdiams.shape == (n_bins + 1,)\n",
    "x_histbins = np.exp(logdiams * diam_logbase)\n",
    "assert x_histbins.shape == (n_bins + 1,)\n",
    "# Dropping the first histogram bins\n",
    "n_bins, x_histbins = n_bins - 1, x_histbins[1:]\n",
    "assert x_histbins.shape == (n_bins + 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_cfgpath = f'{workdir}/07_frames.yml'\n",
    "with open(dash_cfgpath, \"r\") as fp:\n",
    "    dash_cfgdict1 = dict(ruyaml.safe_load(fp))\n",
    "\n",
    "dash_cfgdict2 = deep2hie(dash_cfgdict1)\n",
    "    \n",
    "# Parsing all the \"/ref\"-, \"/def\"-, and \"/pop\"-ending keys\n",
    "dash_cfgdict3 = parse_refs(dash_cfgdict2, trnsfrmtn='hie', pathsep=' -> ')\n",
    "\n",
    "dash_cfgdict = dash_cfgdict3.copy() \n",
    "allframe_infoshie = get_subdict(dash_cfgdict, prefix='frames', pop=True)\n",
    "tnsvars = {'chem_names': chem_names, 'x_histbins': x_histbins, 'get_ychemgrp': get_ychemgrp}\n",
    "for key, val in list(allframe_infoshie.items()):\n",
    "    if isinstance(val, str) and any(var in val for var in tnsvars):\n",
    "        allframe_infoshie[key] = eval_formula(val, tnsvars, catch=True)\n",
    "allframe_infos = hie2deep(allframe_infoshie, sep='/')\n",
    "\n",
    "# pop: [chembars, heatmap1, heatmap2, heatmap3, heatmap4, heatmap5, sctrplts]\n",
    "frame_infos = {frame_name: frame_info for frame_name, frame_info in allframe_infos.items() \n",
    "    # if frame_info['type'] in ('bar',)\n",
    "    if not any (fnmatch.fnmatch(frame_name, pat) for pat in \n",
    "        ['heatmap*', 'sctrplts'])}\n",
    "\n",
    "# The available colors for the dashboard to choose from\n",
    "color_spec = get_subdict(dash_cfgdict, prefix='color/spec', pop=True)\n",
    "\n",
    "# Default colors for different labels\n",
    "dflt_colors = get_subdict(dash_cfgdict, prefix='color/defaults', pop=True)\n",
    "\n",
    "# The column name and values renaming\n",
    "rnm_dict = get_subdict(dash_cfgdict, prefix='renames', pop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dirctn = 'col'\n",
    "\n",
    "# The glyph to settings mapping\n",
    "ctrls_cfghie = get_subdict(dash_cfgdict, prefix='ctrls', pop=True)\n",
    "tnsvars = {'list': list, 'range': range}\n",
    "for key, val in list(ctrls_cfghie.items()):\n",
    "    if isinstance(val, str) and any(var in val for var in tnsvars):\n",
    "        ctrls_cfghie[key] = eval_formula(val, tnsvars, catch=True)\n",
    "ctrls_cfg = hie2deep(ctrls_cfghie, sep='/')\n",
    "\n",
    "ctrl_opts = {col: colspec['opts'] for col, colspec in ctrls_cfg['spec'].items()}\n",
    "ctrl_types = {col: colspec['type'] for col, colspec in ctrls_cfg['spec'].items()}\n",
    "\n",
    "# The number of control columns\n",
    "n_ctrlrows, n_ctrlcols = ctrls_cfg['nrows'], ctrls_cfg['ncols']\n",
    "assert len(ctrl_types) <= (n_ctrlrows * n_ctrlcols)\n",
    "# The control margins\n",
    "m_ctrltop, m_ctrlright, m_ctrlbottom, m_ctrlleft = (5, 15, 5, 15)\n",
    "# The control sizes\n",
    "ctrl_height = 50\n",
    "if layout_dirctn == 'col':\n",
    "    ctrl_width = (1700 - (m_ctrlright + m_ctrlleft) * (n_ctrlcols - 1)) // (n_ctrlcols)\n",
    "elif layout_dirctn == 'row':\n",
    "    ctrl_width = 400\n",
    "else:\n",
    "    raise ValueError(f'undefined layout_dirctn={layout_dirctn}')\n",
    "\n",
    "# Categorical slider formatter\n",
    "strfmt_catslider = '{val}'\n",
    "\n",
    "slider_kwargs = dict(width=ctrl_width, height=ctrl_height, min_height=ctrl_height, \n",
    "    margin=(m_ctrltop, m_ctrlright, m_ctrlbottom, m_ctrlleft), sizing_mode='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_layout = ctrls_cfg['layout']\n",
    "\n",
    "ctrl_optscp = set(ctrl_opts)\n",
    "for i_rowctrl, ctrl_layrow in enumerate(ctrl_layout):\n",
    "    for i_colctrl, ctrl_col in enumerate(ctrl_layrow):\n",
    "        assert i_rowctrl < n_ctrlrows, dedent(f'''\n",
    "            Control Layout Row Overflow:\n",
    "                ctrl_col: {ctrl_col}\n",
    "                Row Index: {i_rowctrl}\n",
    "                Number of Rows: {n_ctrlrows}''')\n",
    "        assert i_colctrl < n_ctrlcols, dedent(f'''\n",
    "            Control Layout Col Overflow:\n",
    "                ctrl_col: {ctrl_col}\n",
    "                Col Index: {i_colctrl}\n",
    "                Number of Cols: {n_ctrlcols}''')\n",
    "        if ctrl_col is None:\n",
    "            continue\n",
    "        assert ctrl_col in ctrl_optscp, dedent(f'''\n",
    "            The \"{ctrl_col}\" ctrl exists in the layout but \n",
    "            is either (1) not defined in the options, or \n",
    "            (2) defined multiple times in the layout:\n",
    "                i_rowctrl: {i_rowctrl}\n",
    "                i_colctrl: {i_colctrl}''')\n",
    "        ctrl_optscp.remove(ctrl_col)\n",
    "\n",
    "    # Appending some None values to complete the row\n",
    "    ctrl_layrow += [None] * (n_ctrlcols - len(ctrl_layrow))\n",
    "ctrl_layout += [[None] * n_ctrlcols] * (n_ctrlrows - len(ctrl_layout))\n",
    "\n",
    "assert len(ctrl_optscp) == 0, dedent(f'''\n",
    "    Some controls were not specified in the control layout:\n",
    "        unspecified controls: {ctrl_optscp}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of columns needed to specify the data starting and ending indices\n",
    "# Note: You can substitute `fpidx` with `list(ctrl_types)` as each set \n",
    "#   of hyper-parameters is supposed to define a unique `fpidx`.\n",
    "# Note: These columns may be over-specified, and the same data could be \n",
    "#   shared across some of them. If you need to update or tailor them to \n",
    "#   your specific needs, feel free to use your judgement.\n",
    "glyph_spcfyngcols = list(ctrl_opts) + ['v_node', 'v_varname']\n",
    "\n",
    "# The glyph to settings mapping\n",
    "glyph_infoshie = get_subdict(dash_cfgdict, prefix='glyphs', pop=True)\n",
    "tnsvars = {'glyph_spcfyngcols': glyph_spcfyngcols, 'chem_names': chem_names, \n",
    "    'x_histbins': x_histbins, 'chem_name2idxs': chem_name2idxs, 'n_bins': n_bins, 'n_chem': n_chem}\n",
    "for key, val in list(glyph_infoshie.items()):\n",
    "    if isinstance(val, str) and any(var in val for var in tnsvars):\n",
    "        glyph_infoshie[key] = eval_formula(val, tnsvars, catch=True)\n",
    "glyph_infos = hie2deep(glyph_infoshie, sep='/')\n",
    "\n",
    "# The mapping of each glyph type to the required `v_repr`s in the data\n",
    "glyph_type2vreprs = {'sct': ['pnts'], 'blb': ['mu', 'sig', 'phi'], \n",
    "    'hm': ['pnts'], 'vbs': ['pnts'], 'vas': ['pnts']}\n",
    "\n",
    "# The excluded set of labels\n",
    "gnrl_incdict = None\n",
    "gnrl_excdict = None\n",
    "\n",
    "# The maximum number of streamed rows to the client before we flush the entire data so far\n",
    "max_strmdrows = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data-frame cartesian components\n",
    "hpcdf0_data = dict()\n",
    "hpcdf0_data['v_node'] = ['m_chmprthst', 'm01', 'm02', 'm03', 'm04', 'm05', 'm06', 'm06__NRMLZD', \n",
    "    'm07', 'm08', 'm09', 'n_prthst', 'n01', 'n02', 'n03', 'n04']\n",
    "hpcdf0_data['chems'] = ['every'] + chem_names\n",
    "hpcdf0_data['v_varname'] = ['orig', 'rcnst']\n",
    "hpcdf0_data['v_repr'] = ['pnts']\n",
    "hpcdf0_data.update(ctrl_opts)\n",
    "\n",
    "hpcdf0 = CartDF({key: {key: vals} for key, vals in hpcdf0_data.items()})\n",
    "hpcdf4 = hpcdf0.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the Glyph Name \n",
    "for frame_name, frame_info in frame_infos.items():\n",
    "    frame_type = frame_info['type']\n",
    "    hpcdf4frm = hpcdf4.select(frame_info.get('inc', None), frame_info.get('exc', None), \n",
    "        copy='shallow', reset_index=False, has_wildcards=True)\n",
    "\n",
    "    # The figure division columns inside the frame\n",
    "    fig_vizcols = frame_info['fig_vizcols']\n",
    "    # The hue division columns inside the frame\n",
    "    hue_vizcols = frame_info['hue_vizcols']\n",
    "\n",
    "    idcols = list(ctrl_types) + list(hue_vizcols) + list(fig_vizcols)\n",
    "    assert set(idcols).issubset(set(hpcdf4frm.columns)), dedent(f'''\n",
    "        Missing columns in the frame database:\n",
    "            {set(idcols) - set(hpcdf4frm.columns)}''')\n",
    "    \n",
    "    # Making sure the idcols define a \n",
    "    for compid, compdf in hpcdf4frm.data.items():\n",
    "        idcols_comp = [col for col in idcols if col in compdf.columns]\n",
    "        hpcdf4frm_dups = compdf[idcols_comp].duplicated()\n",
    "        if hpcdf4frm_dups.any():\n",
    "            # Raising an informative error about duplication groups in \n",
    "            # Pandas is unnecessarily complicated...\n",
    "            dup_groups = compdf.groupby(idcols_comp, sort=False, observed=True)\\\n",
    "                .filter(lambda x: len(x) > 1)\\\n",
    "                .groupby(idcols_comp, group_keys=False, sort=False, observed=True)\n",
    "            raise ValueError(dedent(f'''\n",
    "                The control columns do not define a unique fpidx all the time. \n",
    "                You may have under-specified them:\n",
    "                    Group ID columns: {idcols_comp}\n",
    "                    Sample duplicated rows: {dup_groups.first()}'''))\n",
    "\n",
    "    # Adding the glyph_name column to hpdf\n",
    "    # Note: If you need some data to be used in multiple glyphs, make sure \n",
    "    #       to duplicate its row with various `glyph_name` values.\n",
    "    vrepr2glyph = frame_info['vrepr2glyph']\n",
    "    vrepr_comp = hpcdf4frm.get_comp('v_repr')\n",
    "    hpcdf_framedata = hpcdf4frm.data\n",
    "    vrepr_compdf = hpcdf_framedata[vrepr_comp].copy(deep=True)\n",
    "    vrepr_compdf['glyph_name'] = [vrepr2glyph[v_repr] for v_repr in vrepr_compdf['v_repr']]\n",
    "    vrepr_compdf['glyph_name'] = vrepr_compdf['glyph_name'].astype('category')\n",
    "    hpcdf_framedata[vrepr_comp] = vrepr_compdf\n",
    "    vrepr_compdf['glyph_name'] = [vrepr2glyph[v_repr] for v_repr in vrepr_compdf['v_repr']]\n",
    "    hpcdf_frame = CartDF(hpcdf_framedata)\n",
    "    \n",
    "    frame_info['hpcdf'] = hpcdf_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bokeh Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the renaming business\n",
    "renamer = Renamer(rnm_dict)\n",
    "# Instantiating the color manager\n",
    "colormanger = ColorManager(dflt_colors, color_spec)\n",
    "\n",
    "# The bokeh figures collection\n",
    "fig_infos = dict()\n",
    "\n",
    "# The following defines the figure type, which is used to determine the axis labels, etc.\n",
    "def get_figtype(fig_name, frame_type, dash_type):\n",
    "    return frame_type\n",
    "\n",
    "for frame_name, frame_info in frame_infos.items():\n",
    "    # The frame type\n",
    "    frame_type = frame_info['type']\n",
    "    # The figure division columns inside the frame\n",
    "    fig_vizcols = frame_info['fig_vizcols']\n",
    "    # The figure name formatter string\n",
    "    fig_namfmt = frame_info['fig_namfmt']\n",
    "    # The figure title formatter string\n",
    "    fig_ttlfmt = frame_info['fig_ttlfmt']\n",
    "    # The number of frame columns\n",
    "    n_framecols = frame_info['ncols']\n",
    "    # The number of frame rows\n",
    "    n_framerows = frame_info['nrows']\n",
    "    # The frame catesian dataframe\n",
    "    hpcdf_frame = frame_info['hpcdf']\n",
    "\n",
    "    # # hpdf5gbfigs = hpdf5_frame.groupby(fig_vizcols, sort=False, observed=True).groups \\\n",
    "    # #     if len(fig_vizcols) > 0 else {tuple(): hpdf5_frame.index}\n",
    "    # if len(fig_vizcols) > 0:\n",
    "    #     hpdf5gbfigs = dict(list(hpdf5_frame.groupby(fig_vizcols, sort=False, observed=True)))\n",
    "    #     hpdf5gbfigs = {fig_vizvals: figdf.index for fig_vizvals, figdf in hpdf5gbfigs.items()}\n",
    "    # else:\n",
    "    #     hpdf5gbfigs = {tuple(): hpdf5_frame.index}\n",
    "\n",
    "    frame_fignames = []\n",
    "    for i_fig, (fig_vizvals, hpcdf_fig) in enumerate(hpcdf_frame.groupby(fig_vizcols)):\n",
    "        # Example:\n",
    "        #   fpidx = 02_adhoc/09_klstudy.0.0\n",
    "        #   eid = 'identity'\n",
    "        #   vid in ('1x1xtnse', '1x1zpcalr', '1x1ztsne', ...)\n",
    "        #   v_space in ('x', 'z')\n",
    "        #   v_node in ('m_chmhist', ...)\n",
    "        #   v_split in ('train', 'test', 'normal', ...)\n",
    "        #   v_varname in ('orig', 'rcnst', 'genr', 'ltnt', ...)\n",
    "        #   v_repr in ('pnts', 'mu', 'sig', 'phi')\n",
    "        #   i_seed in (0, 1, ..., n_seeds - 1)\n",
    "        #   i_epoch in (0, 1, ..., n_epochs - 1)\n",
    "        #   i_time in (0, 1, ..., n_t - 1)\n",
    "        fig_vizspec = dict(zip(fig_vizcols, fig_vizvals))\n",
    "        fig_vizspec['frame_name'] = frame_name\n",
    "        \n",
    "        # The figure id/name\n",
    "        fig_name = fig_namfmt.format(**fig_vizspec)\n",
    "        # The formal figure title\n",
    "        fig_title = fig_ttlfmt.format(**{col: renamer.encode(val) \n",
    "            for col, val in fig_vizspec.items()})\n",
    "\n",
    "        assert fig_name not in fig_infos, dedent(f'''\n",
    "            The \"fig_namfmt\" is under-specific. I found two figures with clashing names:\n",
    "                fig_namfmt = {fig_namfmt}\n",
    "                fig_vizcols = {fig_vizcols}\n",
    "                fig_vizvals = {fig_vizvals}''')\n",
    "\n",
    "        # Getting the figure type (e.g., 'scatter', 'bar', etc.)\n",
    "        fig_type = get_figtype(fig_name, frame_type, dash_type)\n",
    "\n",
    "        fig_infos[fig_name] = {'title': fig_title, 'type': fig_type, \n",
    "            'hpcdf': hpcdf_fig, 'frame': frame_name}\n",
    "        \n",
    "        frame_fignames.append(fig_name)\n",
    "\n",
    "    # Registering the figure names associated with this frame\n",
    "    frame_info['fig_names'] = frame_fignames\n",
    "\n",
    "    # Adjusting the number of frame rows and columns\n",
    "    n_framerows1, n_framecols1 = frame_info['nrows'], frame_info['ncols']\n",
    "    n_framefigs = len(frame_fignames)\n",
    "    if n_framerows1 is None:\n",
    "        assert n_framecols1 is not None\n",
    "        n_framecols2 = min(n_framecols1, n_framefigs)\n",
    "        n_framerows2 = math.ceil(n_framefigs / n_framecols)\n",
    "    elif n_framecols1 is None:\n",
    "        assert n_framerows1 is not None\n",
    "        n_framerows2 = min(n_framerows2, n_framefigs)\n",
    "        n_framecols2 = math.ceil(n_framefigs / n_framerows2)\n",
    "    else:\n",
    "        raise ValueError(f'undefined case: {n_framerows1, n_framecols1}')\n",
    "    frame_info['nrows'], frame_info['ncols'] = n_framerows2, n_framecols2\n",
    "    n_framerows, n_framecols = frame_info['nrows'], frame_info['ncols']\n",
    "\n",
    "    # Determining the figure width and height and color-bar's existence\n",
    "    for i_fig, fig_name in enumerate(frame_fignames):\n",
    "        fig_info = fig_infos[fig_name]\n",
    "        fig_type = fig_info['type']\n",
    "        fig_title = fig_info['title']\n",
    "        frame_name = fig_info['frame']\n",
    "        frame_info = frame_infos[frame_name]\n",
    "        fig_kwargs = deepcopy(frame_info['figkwa'])\n",
    "        # Whether the figure has color bar or not\n",
    "        fig_hascb = (fig_type == 'hmap') and ((i_fig + 1) % n_framecols == 0)\n",
    "        fig_width = fig_kwargs.pop('width', 600)\n",
    "        fig_height = fig_kwargs.pop('height', 600)\n",
    "        fig_width = int(fig_width * (1 + 0.2 * fig_hascb))\n",
    "        # Writing down these decisions\n",
    "        fig_info['width'] = fig_width\n",
    "        fig_info['height'] = fig_height\n",
    "        fig_info['has_cb'] = fig_hascb\n",
    "        fig_info['kwargs'] = fig_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bokeh figures\n",
    "diam_engpstfxs = {'0': 'm', '-3': 'mm', '-6': 'um', '-9': 'nm', '-12': 'pm', '-15': 'fm', '-18': 'am'}\n",
    "diam_engargs = {'pstfxs': diam_engpstfxs, 'precision': 0, \n",
    "    'minpstfx': min(diam_engpstfxs, key=float), 'maxpstfx': max(diam_engpstfxs, key=float)}\n",
    "mass_engpstfxs = {'0': ' kg', '-3': ' g', '-6': ' mg', '-9': ' ug', '-12': ' ng', '-15': ' pg', '-18': ' fg', '-21': ' ag'}\n",
    "mass_engargs = {'pstfxs': mass_engpstfxs, 'precision': None, \n",
    "    'minpstfx': min(mass_engpstfxs, key=float), 'maxpstfx': max(mass_engpstfxs, key=float)}\n",
    "count_engpstfxs = {'0': ' ', '3': ' K', '6': ' M', '9': ' B', '12': ' T', '15': ' Q'}\n",
    "count_engargs = {'pstfxs': count_engpstfxs, 'precision': None, \n",
    "    'minpstfx': min(count_engpstfxs, key=float), 'maxpstfx': max(count_engpstfxs, key=float)}\n",
    "\n",
    "diam_tickfrmter = CustomJSTickFormatter(args=diam_engargs, code=code_engfmttr)\n",
    "diam_hoverfrmtr = CustomJSHover(args=diam_engargs, code='var tick = value;\\n' + code_engfmttr)\n",
    "mass_tickfrmter = CustomJSTickFormatter(args=mass_engargs, code=code_engfmttr)\n",
    "mass_hoverfrmtr = CustomJSHover(args=mass_engargs, code='var tick = value;\\n' + code_engfmttr)\n",
    "count_tickfrmter = CustomJSTickFormatter(args=count_engargs, code=code_engfmttr)\n",
    "count_hoverfrmtr = CustomJSHover(args=count_engargs, code='var tick = value;\\n' + code_engfmttr)\n",
    "\n",
    "name2tckfrmtr = {'mass': mass_tickfrmter, 'diam': diam_tickfrmter, 'count': count_tickfrmter}\n",
    "name2hvrfrmtr = {'mass': mass_hoverfrmtr, 'diam': diam_hoverfrmtr, 'count': count_hoverfrmtr}\n",
    "\n",
    "for fig_name, fig_info in fig_infos.items():\n",
    "    fig_title = fig_info['title']\n",
    "    fig_type = fig_info['type']\n",
    "    fig_width = fig_info['width']\n",
    "    fig_height = fig_info['height']\n",
    "    hpcdf_fig = fig_info['hpcdf']\n",
    "    fig_kwargs = fig_info['kwargs'].copy()\n",
    "    # Getting the x and y axis labels\n",
    "    x_axis_label = fig_kwargs.pop('x_label')\n",
    "    y_axis_label = fig_kwargs.pop('y_label')\n",
    "    hue_label = fig_kwargs.pop('hue_label')\n",
    "    x_tickfrmtrnm = fig_kwargs.pop('x_tick_frmtr', None)\n",
    "    y_tickfrmtrnm = fig_kwargs.pop('y_tick_frmtr', None)\n",
    "    hue_tickfrmtrnm = fig_kwargs.pop('hue_tick_frmtr', None)\n",
    "    x_range_padding = fig_kwargs.pop('x_range_padding', None)\n",
    "\n",
    "    # Creating the main figure\n",
    "    zoomtool = BoxZoomTool()\n",
    "    figtools = [zoomtool, 'reset,pan,wheel_zoom,save']\n",
    "\n",
    "    bkfig_kwargs = dict(width=fig_width, height=fig_height, title=fig_title, \n",
    "        tooltips=None, output_backend=\"webgl\",\n",
    "        sizing_mode=\"stretch_both\", tools=figtools)\n",
    "\n",
    "    # Applying the user-provided figure keyword arguments\n",
    "    bkfig_kwargs.update(fig_kwargs)\n",
    "    \n",
    "    # Configuring the tooltips and hover tools \n",
    "    if fig_type == 'bar':\n",
    "        tooltips = [(hue_label, '$name')]\n",
    "        formatters = dict()\n",
    "        if x_tickfrmtrnm is not None:\n",
    "            tooltips.append((x_axis_label, \"@x{custom}\"))\n",
    "            formatters['@x'] = name2hvrfrmtr[x_tickfrmtrnm]\n",
    "        else:\n",
    "            tooltips.append((x_axis_label, \"@x\"))\n",
    "        tooltips.append((y_axis_label, \"@$name\"))\n",
    "        hovertool = HoverTool(tooltips=tooltips, formatters=formatters, point_policy='follow_mouse')\n",
    "        figtools += [hovertool]\n",
    "    elif fig_type == 'hmap':\n",
    "        tooltips = [(y_axis_label, '@y')]\n",
    "        formatters = dict()\n",
    "        if x_tickfrmtrnm is not None:\n",
    "            tooltips.append((x_axis_label, \"@x{custom}\"))\n",
    "            formatters['@x'] = name2hvrfrmtr[x_tickfrmtrnm]\n",
    "        else:\n",
    "            tooltips.append((x_axis_label, \"@x\"))\n",
    "        if hue_tickfrmtrnm is not None:\n",
    "            tooltips.append((hue_label, \"@v{custom}\"))\n",
    "            formatters['@v'] = name2hvrfrmtr[hue_tickfrmtrnm]\n",
    "        else:\n",
    "            tooltips.append((hue_label, \"@v\"))\n",
    "        hovertool = HoverTool(tooltips=tooltips, formatters=formatters)\n",
    "        figtools += [hovertool]\n",
    "    \n",
    "    fig = figure(**bkfig_kwargs)\n",
    "    \n",
    "    # Setting the axis labels\n",
    "    fig.xaxis.axis_label = x_axis_label\n",
    "    fig.yaxis.axis_label = y_axis_label\n",
    "    fig.xgrid.grid_line_color = None\n",
    "    fig.ygrid.grid_line_color = None\n",
    "    if x_range_padding is not None:\n",
    "        fig.x_range.range_padding = x_range_padding\n",
    "\n",
    "    # # The following restricts the data to relevant `v_repr`.\n",
    "    # # Note: I have not tested the following, and it may even \n",
    "    #   be better the next alternative.\n",
    "    # vreprs_rlvnt = set()\n",
    "    # for glyph_name in hpcdf_fig.unqs('glyph_name'):\n",
    "    #     glyph_type = glyph_infos[glyph_name]['type']\n",
    "    #     vreprs_rlvnt.update(set(glyph_type2vreprs[glyph_type]))\n",
    "    # incdict_fig = {'v_repr': list(vreprs_rlvnt)}\n",
    "    \n",
    "    # The following restricts the data to relevant `glyph_type`s \n",
    "    #   based on the figure type. If this seems like a poor idea, \n",
    "    #   give the previous lines a shot!\n",
    "    glyphs_rlvnt = []\n",
    "    for glyph_name in hpcdf_fig.unqs('glyph_name'):\n",
    "        # Here you can customize which data points appear in each figure and glyph combination\n",
    "        assert fig_type in ('scatter', 'bar', 'hmap'), 'the next line should be updated'\n",
    "        glyph_type = glyph_infos[glyph_name]['type']\n",
    "\n",
    "        if (fig_type, glyph_type) in (('scatter', 'sct'), ('scatter', 'blb'), \n",
    "            ('bar', 'vas'), ('bar', 'vbs'), ('hmap', 'hm')):\n",
    "            glyphs_rlvnt.append(glyph_name)\n",
    "    incdict_fig = {'glyph_name': glyphs_rlvnt}\n",
    "\n",
    "    hpcdf_fig = hpcdf_fig.select(incdict={'glyph_name': glyphs_rlvnt}, \n",
    "        excdict=None, copy=False, reset_index=False, has_wildcards=False)\n",
    "    fig_info['hpcdf'] = hpcdf_fig\n",
    "\n",
    "    # Adding tick formatters\n",
    "    if x_tickfrmtrnm is not None:\n",
    "        fig.xaxis.formatter = name2tckfrmtr[x_tickfrmtrnm]\n",
    "    if y_tickfrmtrnm is not None:\n",
    "        fig.yaxis.formatter = name2tckfrmtr[y_tickfrmtrnm]\n",
    "\n",
    "    fig_info['fig'] = fig\n",
    "\n",
    "# The Visual Attributes Controllers\n",
    "need_sctvisualctrls = any(fig_info['type'] == 'scatter' \n",
    "    for fig_name, fig_info in fig_infos.items())\n",
    "\n",
    "if need_sctvisualctrls:\n",
    "    # Blob Alpha slider\n",
    "    alpha_slider = Slider(start=0.0, end=0.1, step=0.005, \n",
    "        value=dflt_blbalpha, title='Blob Alpha', **slider_kwargs)\n",
    "\n",
    "    # Scatter points size slider\n",
    "    size_slider = Slider(start=0, end=10, step=1, value=dflt_pntssize, \n",
    "        title='Points Size', **slider_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bokeh Data Sources and Glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bokeh sources, views, and glyphs\n",
    "bk_srcshie = dict()\n",
    "bk_glyphviewshie = dict()\n",
    "\n",
    "for fig_name, fig_info in fig_infos.items():\n",
    "    # The Bokeh figure object\n",
    "    fig = fig_info['fig']\n",
    "    # The formal figure title\n",
    "    fig_title = fig_info['title']\n",
    "    hpcdf_fig = fig_info['hpcdf']\n",
    "    for glyph_name, glyph_cdf in hpcdf_fig.groupby('glyph_name', sort=False, observed=True):\n",
    "        glyph_info = glyph_infos[glyph_name]\n",
    "        # Example:\n",
    "        #    glyph_name = 'sctr'\n",
    "        #    glyph_type = 'sct'\n",
    "        #    glyph_bkcols = ['x', 'y']\n",
    "        #    glyph_keydims = ['lbl_name']\n",
    "        \n",
    "        # The glyph column names inside the bokeh sources \n",
    "        # (e.g., glyph_bkcols = ['x', 'y'])\n",
    "        glyph_bkcols = glyph_info['bkcols']\n",
    "        # The glyph key dimensions (e.g., the 'sct' glyph needs to be `lbl_name`-specific).\n",
    "        glyph_keydims = glyph_info['keydims']\n",
    "        # The glyph type\n",
    "        glyph_type = glyph_info['type']\n",
    "        # The frame catesian dataframe\n",
    "        glyph_cdfgb = glyph_cdf.groupby(glyph_keydims, sort=False, observed=True)\n",
    "\n",
    "        for glyphkeyvals, figglyph_cdf in glyph_cdfgb:\n",
    "            # Example: \n",
    "            #   glyph_keydims = ['lbl_name']\n",
    "            #   glyphkeyvals = ('train/orig',)\n",
    "            #   glyph_vars = {'lbl_name': 'train/orig'}\n",
    "            glyph_bksrckey = (glyph_name, *glyphkeyvals)\n",
    "            glyph_vars = dict(zip(glyph_keydims, glyphkeyvals))\n",
    "\n",
    "            # Creating a new ColumnDataSource if it doesn't exist in bk_srcshie\n",
    "            if glyph_bksrckey not in bk_srcshie:\n",
    "                bk_src = ColumnDataSource(data={col: [] for col in glyph_bkcols})\n",
    "                # Storing the Bokeh ColumnDataSource for this particular glyph\n",
    "                bk_srcinfo = {'src': bk_src, 'len': 0, 'book': dict()}\n",
    "                bk_srcshie[glyph_bksrckey] = bk_srcinfo\n",
    "            else:\n",
    "                bk_srcinfo = bk_srcshie[glyph_bksrckey]\n",
    "                bk_src = bk_srcinfo['src']\n",
    "            \n",
    "            # Storing the Bokeh CDSView for this particular glyph\n",
    "            bk_view = CDSView(filter=IndexFilter(indices=[]))\n",
    "\n",
    "            if 'lbl_name' in glyph_vars:\n",
    "                lbl_name = glyph_vars['lbl_name']\n",
    "                # The label's color (e.g., `lbl_color = '#001c7f'`)\n",
    "                lbl_color = colormanger(lbl_name)\n",
    "                # The labels's formal name (e.g., `lbl_frml = 'Train Original'`)\n",
    "                lbl_frml = renamer.encode(lbl_name)\n",
    "            else:\n",
    "                lbl_name, lbl_frml, lbl_color = None, None, None\n",
    "\n",
    "            if glyph_type in ('vas', 'vbs'):\n",
    "                glyph_ysubcol = glyph_info.get('y_subcol', None)\n",
    "                if glyph_ysubcol is not None:\n",
    "                    # `bar_names` is the same as the `chem_names` for typical `vas`.\n",
    "                    fig_chemslst = figglyph_cdf.unqs(glyph_ysubcol)\n",
    "                    assert len(fig_chemslst) == 1\n",
    "                    fig_chems = fig_chemslst[0]\n",
    "                    bar_names = list(glyph_info['y_names']) if fig_chems == 'every' else fig_chems.split('_')\n",
    "                else:\n",
    "                    bar_names = list(glyph_info['y_names'])\n",
    "                assert {'x', *bar_names}.issubset(set(glyph_bkcols))\n",
    "            else:\n",
    "                bar_names = None\n",
    "\n",
    "            if glyph_type == 'sct':\n",
    "                assert {'x', 'y'}.issubset(set(glyph_bkcols))\n",
    "                # Creating the scatter points glyph\n",
    "                bk_glyph = fig.scatter(x=\"x\", y=\"y\", \n",
    "                    source=bk_src, view=bk_view, color=lbl_color, legend_label=lbl_frml,\n",
    "                    size=dflt_pntssize, fill_alpha=1.0, muted_alpha=0.01)\n",
    "                \n",
    "                # Linking the visual attribute controllers\n",
    "                size_slider.js_link('value', bk_glyph.glyph, 'size')\n",
    "            elif glyph_type == 'blb':\n",
    "                assert {'x', 'y', 'w', 'h', 'a'}.issubset(set(glyph_bkcols))\n",
    "                # Creating the blob glyph\n",
    "                bk_glyph = fig.ellipse(x=\"x\", y=\"y\", width=\"w\", height=\"h\", angle=\"a\", \n",
    "                    source=bk_src, view=bk_view, fill_color=lbl_color, legend_label=lbl_frml, \n",
    "                    fill_alpha=dflt_blbalpha, line_alpha=dflt_blbalpha, \n",
    "                    muted_alpha=0.001, line_color='black')\n",
    "\n",
    "                # Linking the visual attribute controllers\n",
    "                alpha_slider.js_link('value', bk_glyph.glyph, 'fill_alpha')\n",
    "                alpha_slider.js_link('value', bk_glyph.glyph, 'line_alpha')\n",
    "            elif glyph_type == 'vas':\n",
    "                color_list = [colormanger(stckr) for stckr in bar_names]\n",
    "                bk_glyph = fig.varea_stack(stackers=bar_names, x='x', fill_color=color_list, \n",
    "                    source=bk_src, view=bk_view, legend_label=bar_names, muted_alpha=0.001)\n",
    "            elif glyph_type == 'vbs':\n",
    "                color_list = [colormanger(stckr) for stckr in bar_names]\n",
    "                bk_glyph = fig.vbar_stack(stackers=bar_names, x='x', fill_color=color_list, \n",
    "                    source=bk_src, view=bk_view, legend_label=bar_names, muted_alpha=0.001)\n",
    "            elif glyph_type == 'hm':\n",
    "                assert {'x', 'y', 'w'}.issubset(set(glyph_bkcols))\n",
    "                # `bar_names` is the same as the `chem_names` for typical `vas`, `vbs`, and `hm`.\n",
    "                bar_names = list(glyph_info['y_names']) \n",
    "                color_low, color_high = glyph_info['v_range']\n",
    "                lowhigh_dict = dict()\n",
    "                lowhigh_dict['low'] = color_low\n",
    "                lowhigh_dict['high'] = color_high\n",
    "                cmap = linear_cmap(\"v\", ['white'] + list(bokeh.palettes.Reds[256][::-1]), \n",
    "                    nan_color='white', **lowhigh_dict)\n",
    "                bk_rectglyph = fig.rect(y='y', x='x', width='w', height=1, fill_color=cmap, \n",
    "                    legend_label=fig_name, source=bk_src, view=bk_view, line_color='black')\n",
    "                    \n",
    "                fig_hascb = fig_info['has_cb']\n",
    "                if fig_hascb:\n",
    "                    cb_kwargs = dict(ticker=BasicTicker(desired_num_ticks=5),\n",
    "                        label_standoff=6, border_line_color=None, padding=5)\n",
    "\n",
    "                    fig_kwargs = fig_info['kwargs']\n",
    "                    hue_tickfrmtrnm = fig_kwargs.get('hue_tick_frmtr', None)\n",
    "                    if hue_tickfrmtrnm in name2tckfrmtr:\n",
    "                        cb_kwargs['formatter'] = name2tckfrmtr[hue_tickfrmtrnm] \n",
    "\n",
    "                    bk_cbglyph = bk_rectglyph.construct_color_bar(**cb_kwargs)\n",
    "                    fig.add_layout(bk_cbglyph, 'right')\n",
    "                else:\n",
    "                    bk_cbglyph = None\n",
    "                \n",
    "                bk_glyph = (bk_rectglyph, bk_cbglyph)\n",
    "            else:\n",
    "                raise ValueError(f'undefined glyph_type={glyph_type}')\n",
    "            \n",
    "            # Storing the Bokeh glyph for this particular figure and label and glyph\n",
    "            bk_glyphinfo = {'glyph': bk_glyph, 'view': bk_view}\n",
    "            bk_glyphviewshie[(fig_name, glyph_name, *glyphkeyvals)] = bk_glyphinfo\n",
    "\n",
    "for fig_name, fig_info in fig_infos.items():\n",
    "    fig = fig_info['fig']\n",
    "    fig_type = fig_info['type']\n",
    "    # Clicking on each legend iterm makes it transparent\n",
    "    if fig_type == 'scatter':\n",
    "        fig.legend.click_policy = \"mute\"\n",
    "        fig.legend.location = \"top_right\"\n",
    "    elif fig_type in ('bar', 'hmap'):\n",
    "        fig.legend.visible = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bokeh Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to compartmentalize the controls into the cartdf components\n",
    "ctrl_compcols = defaultdict(dict)\n",
    "for ctrl_col, ctrl_type in ctrl_types.items():\n",
    "    compid = hpcdf4.get_comp(ctrl_col)\n",
    "    ctrl_compcols[compid][ctrl_col] = ctrl_type\n",
    "\n",
    "ctrl_refs = dict()\n",
    "for compid, control_col2types in ctrl_compcols.items():\n",
    "    # Example:\n",
    "    #    compid in ('hp', 'var', 'i_time', ...)\n",
    "    #    compid = 'hp'\n",
    "    #    control_col2types = {'cri/kl/w/sig': 'catslider', 'cri/kl/w/sig': 'catslider'}\n",
    "\n",
    "    # `hpdf5` is a regular pd.DataFrame for this particular component of the `hpcdf`.\n",
    "    hpdf5 = hpcdf4.data[compid]\n",
    "    \n",
    "    ctrl_compinfos = []\n",
    "    for ctrl_col, ctrl_type in control_col2types.items():\n",
    "        ctrl_ttl = renamer.encode(ctrl_col)\n",
    "        vals_allnp = hpdf5[ctrl_col].values\n",
    "        vals_allunq = np.unique(vals_allnp) if isinstance(vals_allnp, np.ndarray) else vals_allnp.unique()\n",
    "        vals_allunqfrml = [renamer.encode(val) for val in vals_allunq]\n",
    "        \n",
    "        # Inferring the control type (i.e., slider or menu)\n",
    "        if ctrl_type is None:\n",
    "            ctrl_type = 'catslider' if np.issubdtype(vals_allnp.dtype, np.number) else 'menu'\n",
    "        \n",
    "        # The categorical slider needs a mapping from integers to strings. \n",
    "        # I wish bokeh was not so strict about requiring any notion of \n",
    "        # categories to be strictly string.\n",
    "        ctrl_enc, ctrl_dec = None, None\n",
    "        if ctrl_type in ('catslider', 'radiobtngrp'):\n",
    "            ctrl_encdict = dict()\n",
    "            # for val in vals_allnp.tolist():\n",
    "            for val in vals_allunq.tolist():\n",
    "                val_frml = renamer.encode(val)\n",
    "                ctrl_encdict[val_frml] = strfmt_catslider.format(val=val_frml)\n",
    "            ctrl_encoder = Renamer(ctrl_encdict)\n",
    "            ctrl_enc = ctrl_encoder.dict\n",
    "            ctrl_dec = ctrl_encoder.invdict\n",
    "\n",
    "        # The first key in the control book options will define the default value\n",
    "        top_val = hpdf5[ctrl_col].iloc[0]\n",
    "        topval_frml = renamer.encode(top_val)\n",
    "        if ctrl_enc is not None:\n",
    "            topval_frml = ctrl_enc[topval_frml]\n",
    "\n",
    "        ctrl_info = {'col': ctrl_col, 'title': ctrl_ttl, 'type': ctrl_type, \n",
    "            'enc': ctrl_enc, 'dec': ctrl_dec, 'dflt': topval_frml}\n",
    "        ctrl_compinfos.append(ctrl_info)\n",
    "\n",
    "    ctrl_ref = {'infos': ctrl_compinfos, 'hpdf': hpdf5}\n",
    "    ctrl_refs[compid] = ctrl_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compid, ctrl_ref in ctrl_refs.items():\n",
    "    ctrl_infos = ctrl_ref['infos']\n",
    "    hpdf5 = ctrl_ref['hpdf']\n",
    "    \n",
    "    # # Deprecated: The following is slow\n",
    "    # # This is a deep mapping of control values to the scene id\n",
    "    # # For example:\n",
    "    # #    ctrl_cols =[    'vid', 'i_seed', 'cri/kl/mu/w', 'cri/kl/sig/w', 'nn/ltnt/dim']\n",
    "    # #    ctrl_book   ['X TSNE']     ['0']     ['1e-06']       ['0.001']           ['9'] == 864\n",
    "    # #    sceneid == 864 \n",
    "    # ctrl_book = make_ctrlbook(ctrl_infos, hpdf5, renamer, i_ctrl=0)\n",
    "\n",
    "    # Here's a more performant replacement\n",
    "    ctrl_cols = [ctrl_info['col'] for ctrl_info in ctrl_infos]\n",
    "    # hpdf5ctrl1 = hpdf5[ctrl_cols + ['sceneid']].drop_duplicates()\n",
    "    hpdf5ctrl1 = hpdf5[ctrl_cols].drop_duplicates()\n",
    "    hpdf5ctrl2 = hpdf5ctrl1.sort_values(ctrl_cols).reset_index(drop=True)\n",
    "    # `hpdf5ctrl3` will contain the original values of each column\n",
    "    hpdf5ctrl3 = hpdf5ctrl2[ctrl_cols]\n",
    "    # `hpdf5ctrl4` will contain the renamed values of each column\n",
    "    hpdf5ctrl4 = hpdf5ctrl3.copy(deep=True)\n",
    "    # Applying the renaming business\n",
    "    for ctrl_info in ctrl_infos:\n",
    "        ctrl_col = ctrl_info['col']\n",
    "        ctrl_enc = ctrl_info['enc']\n",
    "        is_colcat = (hpdf5ctrl4.dtypes[ctrl_col] == 'category')\n",
    "        if not is_colcat:\n",
    "            hpdf5ctrl4[f'{ctrl_col}/frml'] = None\n",
    "        val2frml = dict()\n",
    "        for val, hpdf_val in hpdf5ctrl4.groupby(ctrl_col, sort=True, observed=True):\n",
    "            val_frml = renamer.encode(val)\n",
    "            if ctrl_enc is not None:\n",
    "                val_frml = ctrl_enc[val_frml]\n",
    "            \n",
    "            if is_colcat:\n",
    "                val2frml[val] = val_frml\n",
    "            else:\n",
    "                hpdf5ctrl4.loc[hpdf_val.index, f'{ctrl_col}/frml'] = val_frml\n",
    "        \n",
    "        if len(val2frml) > 0:\n",
    "            hpdf5ctrl4[ctrl_col] = hpdf5ctrl4[ctrl_col].cat.rename_categories(val2frml)\n",
    "        if not is_colcat:\n",
    "            hpdf5ctrl4[ctrl_col] = hpdf5ctrl4[f'{ctrl_col}/frml']\n",
    "            hpdf5ctrl4 = hpdf5ctrl4.drop(columns=f'{ctrl_col}/frml')\n",
    "\n",
    "    # ctrl_bookhie = dict(zip(hpdf5ctrl4.itertuples(index=False, name='Control'), hpdf5ctrl2['sceneid']))\n",
    "    ctrl_bookhie = dict(zip(hpdf5ctrl4.itertuples(index=False, name='FormalControl'), \n",
    "        hpdf5ctrl3.itertuples(index=False, name='RawControl')))\n",
    "    ctrl_book = hie2deep(ctrl_bookhie)\n",
    "    ctrl_ref['book'] = ctrl_book\n",
    "    ctrl_ref['cols'] = ctrl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compid, ctrl_ref in ctrl_refs.items():\n",
    "    hpdf5 = ctrl_ref['hpdf']\n",
    "    ctrl_book = ctrl_ref['book']\n",
    "    ctrl_infos = ctrl_ref['infos']\n",
    "    ctrl_cols = ctrl_ref['cols']\n",
    "\n",
    "    # Instantiating the Bokeh controllers\n",
    "    ctrl_bookrcrsd = ctrl_book\n",
    "    for ctrl_info in ctrl_infos:\n",
    "        ctrl_col = ctrl_info['col']\n",
    "        ctrl_ttl = ctrl_info['title']\n",
    "        ctrl_type = ctrl_info['type']\n",
    "        ctrl_enc = ctrl_info['enc']\n",
    "        ctrl_dec = ctrl_info['dec']\n",
    "        ctrl_dflt = ctrl_info['dflt']\n",
    "        ctrl_vals = list(ctrl_bookrcrsd)\n",
    "        \n",
    "        if ctrl_type == 'catslider':\n",
    "            control = CategoricalSlider(categories=ctrl_vals, value=ctrl_dflt, \n",
    "                title=ctrl_ttl, **slider_kwargs)\n",
    "            container = control\n",
    "        elif ctrl_type == 'radiobtngrp':\n",
    "            control = RadioButtonGroup(labels=ctrl_vals, active=ctrl_vals.index(ctrl_dflt))\n",
    "            title_div = Div(text=f'{ctrl_ttl}:', styles={'color': header_color})\n",
    "            container = column([title_div, control], background=background_color, \n",
    "                width=ctrl_width, height=ctrl_height, min_height=ctrl_height, \n",
    "                margin=(0, m_ctrlright, 0, m_ctrlleft), sizing_mode='fixed')\n",
    "        elif ctrl_type == 'menu':\n",
    "            options = [(val, str(val)) for val in ctrl_vals]\n",
    "            control = Select(options=options, value=ctrl_dflt, \n",
    "                title=ctrl_ttl, width=ctrl_width, height=ctrl_height, \n",
    "                margin=(m_ctrltop, m_ctrlright, m_ctrlbottom, m_ctrlleft),\n",
    "                sizing_mode='fixed')\n",
    "            container = control\n",
    "        else:\n",
    "            raise ValueError(f'undefined ctrl_type = {ctrl_type}')\n",
    "        \n",
    "        # Adding the controller\n",
    "        ctrl_info['control'] = control\n",
    "        ctrl_info['container'] = container\n",
    "\n",
    "        # Restricting `hpdf6` for the next round\n",
    "        ctrl_bookrcrsd = ctrl_bookrcrsd[ctrl_dflt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Axes in Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_name, frame_info in frame_infos.items():\n",
    "    frame_type = frame_info['type']\n",
    "    n_framerows = frame_info['nrows']\n",
    "    n_framecols = frame_info['ncols']\n",
    "\n",
    "    if frame_type in ('bar', 'hmap', 'scatter'):\n",
    "        first_fig = None\n",
    "        sharex = frame_info['sharex']\n",
    "        sharey = frame_info['sharey']\n",
    "\n",
    "        get_xfiggrp, get_yfiggrp = None, None\n",
    "        for axname, shareax in (('x', sharex), ('y', sharey)):\n",
    "            if shareax in ('all', True):\n",
    "                def get_axfiggrp(*args, **kwargs):\n",
    "                    return 0\n",
    "            elif shareax in ('none', False, None):\n",
    "                def get_axfiggrp(i_fig, *args, **kwargs):\n",
    "                    return i_fig\n",
    "            elif shareax == 'row':\n",
    "                def get_axfiggrp(i_fig, fig_name, n_framerows, n_framecols):\n",
    "                    return i_fig // n_framecols\n",
    "            elif shareax == 'col':\n",
    "                def get_axfiggrp(i_fig, fig_name, n_framerows, n_framecols):\n",
    "                    return i_fig % n_framecols\n",
    "            elif callable(shareax):\n",
    "                get_axfiggrp = shareax\n",
    "            else:\n",
    "                raise ValueError(f'undefined shareax={shareax} for {frame_name}')\n",
    "\n",
    "            if axname == 'x':\n",
    "                get_xfiggrp = get_axfiggrp\n",
    "            elif axname == 'y':\n",
    "                get_yfiggrp = get_axfiggrp\n",
    "            else:\n",
    "                raise ValueError(f'undefined axname={axname}')\n",
    "\n",
    "        frame_figs = frame_info['fig_names']\n",
    "\n",
    "        frame_xgrp2fignms = defaultdict(list)\n",
    "        frame_ygrp2fignms = defaultdict(list)\n",
    "        frame_xgrpicol2fignmshie = defaultdict(list)\n",
    "        frame_ygrpirow2fignmshie = defaultdict(list)\n",
    "        for i_fig, fig_name in enumerate(frame_figs):\n",
    "            # The row index of the figure within the frame\n",
    "            i_frmrow = i_fig // n_framecols\n",
    "            # The column index of the figure within the frame\n",
    "            i_frmcol = i_fig % n_framecols\n",
    "            # The x axis group index of the figure within the frame\n",
    "            fig_frmgrpx = get_xfiggrp(i_fig=i_fig, fig_name=fig_name, \n",
    "                n_framerows=n_framerows, n_framecols=n_framecols)\n",
    "            # The y axis group index of the figure within the frame\n",
    "            fig_frmgrpy = get_yfiggrp(i_fig=i_fig, fig_name=fig_name, \n",
    "                n_framerows=n_framerows, n_framecols=n_framecols)\n",
    "            \n",
    "            # Saving these variables to the figure infos\n",
    "            fig_info = fig_infos[fig_name]\n",
    "            fig_info['i_frmrow'] = i_frmrow\n",
    "            fig_info['i_frmcol'] = i_frmcol\n",
    "            fig_info['framegrp/x'] = fig_frmgrpx\n",
    "            fig_info['framegrp/y'] = fig_frmgrpy\n",
    "\n",
    "            # Compiling an inverse lookup table of the same vars\n",
    "            frame_xgrp2fignms[fig_frmgrpx].append(fig_name)\n",
    "            frame_ygrp2fignms[fig_frmgrpy].append(fig_name)\n",
    "            frame_xgrpicol2fignmshie[(i_frmcol, fig_frmgrpx)].append((i_frmrow, fig_name))\n",
    "            frame_ygrpirow2fignmshie[(i_frmrow, fig_frmgrpy)].append((i_frmcol, fig_name))\n",
    "        \n",
    "        # Configuring the x axis visibility among groups\n",
    "        for (i_frmcol, fig_frmgrpx), fig_icolnames in frame_xgrpicol2fignmshie.items():\n",
    "            # The bottom row's x axis should certainly be visible\n",
    "            last_ifrmrow, btm_figname = fig_icolnames[-1]\n",
    "            fig_infos[btm_figname]['fig'].xaxis.visible = True\n",
    "            for i_frmrow, fig_name in fig_icolnames[-2::-1]:\n",
    "                fig = fig_infos[fig_name]['fig']\n",
    "                # The other figures in the same frame group should only have their \n",
    "                # x axis invisible if the figure to their bottom is in their group.\n",
    "                fig.xaxis.visible = (i_frmrow != (last_ifrmrow - 1))\n",
    "                last_ifrmrow = i_frmrow\n",
    "\n",
    "        # Configuring the y axis visibility among groups\n",
    "        for (i_frmrow, fig_frmgrpy), fig_irownames in frame_ygrpirow2fignmshie.items():\n",
    "            # The leftmost column's y axis should certainly be visible\n",
    "            last_ifrmcol, left_figname = fig_irownames[0]\n",
    "            fig_infos[left_figname]['fig'].yaxis.visible = True\n",
    "            for i_frmcol, fig_name in fig_irownames[1:]:\n",
    "                fig = fig_infos[fig_name]['fig']\n",
    "                # The other figures in the same frame group should only have their \n",
    "                # y axis invisible if the figure to their left is in their group.\n",
    "                fig.yaxis.visible = (i_frmcol != (last_ifrmcol + 1))\n",
    "                last_ifrmcol = i_frmcol\n",
    "        \n",
    "        # All the figures within the same x frame group should share the same x range\n",
    "        for fig_frmgrpx, fig_names in frame_xgrp2fignms.items():\n",
    "            first_fig = fig_infos[fig_names[0]]['fig']\n",
    "            for fig_name in fig_names[1:]:\n",
    "                fig = fig_infos[fig_name]['fig']\n",
    "                fig.x_range = first_fig.x_range\n",
    "\n",
    "        # All the figures within the same y frame group should share the same y range\n",
    "        for fig_frmgrpy, fig_names in frame_ygrp2fignms.items():\n",
    "            first_fig = fig_infos[fig_names[0]]['fig']\n",
    "            for fig_name in fig_names[1:]:\n",
    "                fig = fig_infos[fig_name]['fig']\n",
    "                fig.y_range = first_fig.y_range\n",
    "\n",
    "        # Some additional frame figure tweaks\n",
    "        for i_fig, fig_name in enumerate(frame_figs):\n",
    "            fig = fig_infos[fig_name]['fig']\n",
    "            fig.min_border = 10\n",
    "            fig.sizing_mode = 'stretch_both'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RioHandler:\n",
    "    def __init__(self, data_dict, hash_data, trn_spltidxs, n_seeds, \n",
    "        eval_bs, n_snr, n_t, tch_device, tch_dtype, \n",
    "        n_ppcache, n_origcache, n_rcnstcache, verbose):\n",
    "\n",
    "        # The set of already instantiated and loaded PP objects\n",
    "        self.pp_cache = dict()\n",
    "        self.orig_cache = dict()\n",
    "        self.rcnst_cache = dict()\n",
    "        \n",
    "        self.data_dict = data_dict\n",
    "        self.hash_data = hash_data\n",
    "        self.trn_spltidxs = trn_spltidxs\n",
    "        self.n_seeds = n_seeds\n",
    "        self.eval_bs = eval_bs\n",
    "        self.n_snr = n_snr\n",
    "        self.n_t = n_t\n",
    "        self.tch_device = tch_device\n",
    "        self.tch_dtype = tch_dtype\n",
    "        self.n_ppcache = n_ppcache\n",
    "        self.n_origcache = n_origcache\n",
    "        self.n_rcnstcache = n_rcnstcache\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_ppmdlcfg(config):\n",
    "        \"\"\"\n",
    "        Generates the PP config according to the framework described in the \n",
    "        presentation slides.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config: (dict) The dictionary of hyper-parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pp_mdlcfg: (dict) The PP module config. \n",
    "        \"\"\"\n",
    "        configcp = dict() if config is None else dict(config)\n",
    "\n",
    "        eps = configcp.pop('eps', 1e-100)\n",
    "        # The magnitude dimensionality; one of `'n_chem', 'k_bins', 'one'`.\n",
    "        magdim = configcp.pop('magdim')\n",
    "        # The magnitude normalization norm\n",
    "        magpnrm = configcp.pop('magpnrm')\n",
    "        # The magnitude exponent\n",
    "        magexp = configcp.pop('magexp')\n",
    "        # Whether to subtract mu3\n",
    "        magshft = configcp.pop('magshft')\n",
    "        # Whether to scale sig4\n",
    "        magscl = configcp.pop('magscl')\n",
    "        # The normalized image exponent\n",
    "        nrmexp = configcp.pop('nrmexp')\n",
    "        # Whether to subtract mu7\n",
    "        nrmshft = configcp.pop('nrmshft')\n",
    "        # Whether to scale sig8\n",
    "        nrmscl = configcp.pop('nrmscl')\n",
    "        # The mu7 and sig8 dimensionality; one of `'n_chem', 'k_bins', 'one', 'n_chem, k_bins'`.\n",
    "        nrmscldim = configcp.pop('nrmscldim')\n",
    "\n",
    "        # The epsilon added to the particle counts\n",
    "        cnteps = configcp.pop('cnteps', 1000)\n",
    "        # The counts exponent\n",
    "        cntexp = configcp.pop('cntexp')\n",
    "        # Whether to subtract mu2 for the counts\n",
    "        cntshft = configcp.pop('cntshft')\n",
    "        # Whether to scale by sig3 for the counts\n",
    "        cntscl = configcp.pop('cntscl')\n",
    "        # The mu3 and sig3 dimensionality; one of `'k_bins', 'one'`.\n",
    "        cntscldim = configcp.pop('cntscldim')\n",
    "\n",
    "        # Translating some of the options\n",
    "        onoff_dict = {'on': True, 'off': False, True: True, False: False}\n",
    "        magshft = onoff_dict[magshft]\n",
    "        magscl = onoff_dict[magscl]\n",
    "        nrmshft = onoff_dict[nrmshft]\n",
    "        nrmscl = onoff_dict[nrmscl]\n",
    "        cntshft = onoff_dict[cntshft]\n",
    "        cntscl = onoff_dict[cntscl]\n",
    "        magpnrm = {1: 1, 2: 2, 'l1': 1, 'l2': 2}[magpnrm]\n",
    "\n",
    "        assert len(configcp) == 0, f'unused options: {configcp}'\n",
    "\n",
    "        pp_mdlcfg = dict()\n",
    "\n",
    "        # The input and output\n",
    "        pp_mdlcfg.update(ruyaml.safe_load('''\n",
    "            input:\n",
    "                m_chmprthst: [n_seeds, n_mb, n_chem, k_bins]\n",
    "                n_prthst:    [n_seeds, n_mb, 1,      k_bins]\n",
    "            output: \n",
    "                m_chmprtnrm: m09b\n",
    "                m_prthstmag: m05b\n",
    "                n_prthstnrm: n04b\n",
    "        '''))\n",
    "\n",
    "        # Defining the layers\n",
    "        pp_mdllayers = dict()\n",
    "        #######################################\n",
    "        ######## M1: Adding an Epsilon ########\n",
    "        #######################################\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # M1 = m0 + eps\n",
    "            m01:\n",
    "                type: add\n",
    "                value: {eps}\n",
    "                input: m_chmprthst\n",
    "                shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "        '''))\n",
    "\n",
    "        ############################################################\n",
    "        ################## Part I: The Magnitude ###################\n",
    "        ############################################################\n",
    "\n",
    "        #######################################\n",
    "        #### M2: Taking the Lp-Norm of X1 #####\n",
    "        #######################################\n",
    "        # The magnitude reduction dimensions\n",
    "        mag_rdcdims = {'n_chem': [-1], 'k_bins': [-2], 'one': [-1, -2]}[magdim]\n",
    "        # The magnitude's number of channels and bins\n",
    "        n_chnlsmag, n_binsmag = {'n_chem': ('n_chem', '1'), 'k_bins': ('1', 'k_bins'), 'one': ('1', '1')}[magdim]\n",
    "\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # M2 = \\| M1 \\| \n",
    "            m02:\n",
    "                type: pnorm\n",
    "                dim: {mag_rdcdims}\n",
    "                pnorm: {magpnrm}\n",
    "                shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "        '''))\n",
    "\n",
    "        #######################################\n",
    "        ## M3: Exponentiating the magnitude ###\n",
    "        #######################################\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # M3 = M2 ^ alpha\n",
    "            m03:\n",
    "                type: sgnpow\n",
    "                exponent: {magexp}\n",
    "                shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "        '''))\n",
    "\n",
    "        #######################################\n",
    "        ###### M4: Subtracting the Mean #######\n",
    "        #######################################\n",
    "        if magshft:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # M4 = M3 - mu3\n",
    "                m04:\n",
    "                    type: shift\n",
    "                    dim: {mag_rdcdims}\n",
    "                    shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "            '''))\n",
    "        else:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # M4 = M3\n",
    "                m04:\n",
    "                    type: add\n",
    "                    value: 0\n",
    "                    shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "            '''))\n",
    "\n",
    "        #######################################\n",
    "        ####### M5: Dividing the Scale ########\n",
    "        #######################################\n",
    "        if magscl:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # M5 = M4 / sigma4\n",
    "                m05:\n",
    "                    type: scale\n",
    "                    dim: {mag_rdcdims}\n",
    "                    shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "            '''))\n",
    "        else:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # M5 = M4\n",
    "                m05:\n",
    "                    type: add\n",
    "                    value: 0\n",
    "                    shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "            '''))\n",
    "\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # Scalar shift and scale for properly-scaled noise injection\n",
    "            m05a:\n",
    "                type: shift\n",
    "                dim: [-1, -2]\n",
    "                shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "            m05b:\n",
    "                type: scale\n",
    "                dim: [-1, -2]\n",
    "                shape: [n_seeds, n_mb, {n_chnlsmag}, {n_binsmag}]\n",
    "        '''))\n",
    "        \n",
    "        ############################################################\n",
    "        ################ Part II: The Normalization ################\n",
    "        ############################################################\n",
    "\n",
    "        #######################################\n",
    "        ######### M6: Normalization ###########\n",
    "        #######################################\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # M6 = M1 / M2\n",
    "            m06:\n",
    "                type: normalize\n",
    "                dim: {mag_rdcdims}\n",
    "                pnorm: {magpnrm}\n",
    "                input: m01\n",
    "                shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "        '''))\n",
    "\n",
    "        #######################################\n",
    "        ## M7: Exponentiating the normalztn ###\n",
    "        #######################################\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # M7 = M6 ^ alpha\n",
    "            m07:\n",
    "                type: sgnpow\n",
    "                exponent: {nrmexp}\n",
    "                shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "        '''))\n",
    "\n",
    "        #######################################\n",
    "        ###### M8: Subtracting the Mean #######\n",
    "        #######################################\n",
    "\n",
    "        # The magnitude reduction dimensions\n",
    "        mu7sig8_rdcdims = {'n_chem': [-1], 'k_bins': [-2], 'one': [-1, -2], 'n_chem, k_bins': []}[nrmscldim]\n",
    "\n",
    "        if nrmshft:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # M8 = M7 - mu7\n",
    "                m08:\n",
    "                    type: shift\n",
    "                    dim: {mu7sig8_rdcdims}\n",
    "                    shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "            '''))\n",
    "        else:\n",
    "            pp_mdllayers.update(ruyaml.safe_load('''\n",
    "                # M8 = M7\n",
    "                m08:\n",
    "                    type: add\n",
    "                    value: 0\n",
    "                    shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "            '''))\n",
    "\n",
    "        #######################################\n",
    "        ####### M9: Dividing the Scale ########\n",
    "        #######################################\n",
    "        if nrmscl:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # M9 = M8 / sig8\n",
    "                m09:\n",
    "                    type: scale\n",
    "                    dim: {mu7sig8_rdcdims}\n",
    "                    shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "            '''))\n",
    "        else:\n",
    "            pp_mdllayers.update(ruyaml.safe_load('''\n",
    "                # M9 = M8\n",
    "                m09:\n",
    "                    type: add\n",
    "                    value: 0\n",
    "                    shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "            '''))\n",
    "        \n",
    "        pp_mdllayers.update(ruyaml.safe_load('''\n",
    "            # Scalar shift and scale for properly-scaled noise injection\n",
    "            m09a:\n",
    "                type: shift\n",
    "                dim: [-1, -2]\n",
    "                shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "            m09b:\n",
    "                type: scale\n",
    "                dim: [-1, -2]\n",
    "                shape: [n_seeds, n_mb, n_chem, k_bins]\n",
    "        '''))\n",
    "\n",
    "        ############################################################\n",
    "        ################### Part III: The Count ####################\n",
    "        ############################################################\n",
    "        cnt_rdcdims = {'k_bins': [], 'one': [-1]}[cntscldim]\n",
    "\n",
    "        #######################################\n",
    "        ######## N1: Adding an Epsilon ########\n",
    "        #######################################\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # n1 = n0 + cnteps\n",
    "            n01:\n",
    "                type: add\n",
    "                input: n_prthst\n",
    "                value: {cnteps}\n",
    "                shape: [n_seeds, n_mb, 1, k_bins]\n",
    "        '''))\n",
    "\n",
    "        #######################################\n",
    "        ## N2: Exponentiating the magnitude ###\n",
    "        #######################################\n",
    "        pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "            # N2 = N1 ^ alpha\n",
    "            n02:\n",
    "                type: sgnpow\n",
    "                exponent: {cntexp}\n",
    "                shape: [n_seeds, n_mb, 1, k_bins]\n",
    "        '''))\n",
    "\n",
    "        #######################################\n",
    "        ###### N3: Subtracting the Mean #######\n",
    "        #######################################\n",
    "        if cntshft:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # N3 = N2 - mu2\n",
    "                n03:\n",
    "                    type: shift\n",
    "                    dim: {cnt_rdcdims}\n",
    "                    shape: [n_seeds, n_mb, 1, k_bins]\n",
    "            '''))\n",
    "        else:\n",
    "            pp_mdllayers.update(ruyaml.safe_load('''\n",
    "                # N3 = N2\n",
    "                n03:\n",
    "                    type: add\n",
    "                    value: 0\n",
    "                    shape: [n_seeds, n_mb, 1, k_bins]\n",
    "            '''))\n",
    "\n",
    "        #######################################\n",
    "        ####### N4: Dividing the Scale ########\n",
    "        #######################################\n",
    "        if cntscl:\n",
    "            pp_mdllayers.update(ruyaml.safe_load(f'''\n",
    "                # N4 = N3 / sigma3\n",
    "                n04:\n",
    "                    type: scale\n",
    "                    dim: {cnt_rdcdims}\n",
    "                    shape: [n_seeds, n_mb, 1, k_bins]\n",
    "            '''))\n",
    "        else:\n",
    "            pp_mdllayers.update(ruyaml.safe_load('''\n",
    "                # N4 = N3\n",
    "                n04:\n",
    "                    type: add\n",
    "                    value: 0\n",
    "                    shape: [n_seeds, n_mb, 1, k_bins]\n",
    "            '''))\n",
    "\n",
    "        pp_mdllayers.update(ruyaml.safe_load('''\n",
    "            # Scalar shift and scale for properly-scaled noise injection\n",
    "            n04a:\n",
    "                type: shift\n",
    "                dim: [-1]\n",
    "                shape: [n_seeds, n_mb, 1, k_bins]\n",
    "            n04b:\n",
    "                type: scale\n",
    "                dim: [-1]\n",
    "                shape: [n_seeds, n_mb, 1, k_bins]\n",
    "        '''))\n",
    "\n",
    "        pp_mdlcfg['layers'] = pp_mdllayers\n",
    "\n",
    "        return pp_mdlcfg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, rowdict):\n",
    "        data_dict = self.data_dict\n",
    "        hash_data = self.hash_data\n",
    "        trn_spltidxs = self.trn_spltidxs\n",
    "        n_seeds = self.n_seeds\n",
    "        eval_bs = self.eval_bs\n",
    "        n_snr = self.n_snr\n",
    "        n_t = self.n_t\n",
    "        tch_device = self.tch_device\n",
    "        tch_dtype = self.tch_dtype\n",
    "        verbose = self.verbose\n",
    "\n",
    "        pp_cache = self.pp_cache\n",
    "        orig_cache = self.orig_cache\n",
    "        rcnst_cache = self.rcnst_cache\n",
    "        n_ppcache = self.n_ppcache\n",
    "        n_origcache = self.n_origcache\n",
    "        n_rcnstcache = self.n_rcnstcache\n",
    "\n",
    "        rowdictcp = dict(rowdict)\n",
    "        if verbose:\n",
    "            print(f'Working on {rowdictcp}', flush=True)\n",
    "\n",
    "        ######### Creating the Pre-Processor\n",
    "        # Example:\n",
    "        #   pp_idkeys = ('magdim', 'magpnrm', 'magexp', 'magshft', 'magscl', \n",
    "        #                'nrmexp', 'nrmshft', 'nrmscl', 'nrmscldim')\n",
    "        #\n",
    "        #   pp_idvals = ('k_bins', 1, 1.0, False, True, 1.0, False, True, 'n_chem')\n",
    "        #\n",
    "        #   ppcfg = {'magdim': 'k_bins', 'magpnrm': 1, 'magexp': 1.0, 'magshft': False, \n",
    "        #       'magscl': True, 'nrmexp': 1.0, 'nrmshft': False, 'nrmscl': True, \n",
    "        #       'nrmscldim': 'n_chem'}\n",
    "        #\n",
    "        #   v_node = 'm_chmprthst'\n",
    "        #   v_varname = 'orig'\n",
    "        #   chems = 'every'\n",
    "        #   v_repr = 'pnts'\n",
    "        #   noise_amp = 0.0\n",
    "        #   noise_seed = 0\n",
    "        #   i_seed = 0\n",
    "\n",
    "        pp_idkeys = ('magdim', 'magpnrm', 'magexp', 'magshft', 'magscl', \n",
    "            'nrmexp', 'nrmshft', 'nrmscl', 'nrmscldim', 'cntexp', \n",
    "            'cntshft', 'cntscl', 'cntscldim')\n",
    "        pp_idvals = tuple(rowdictcp.pop(key) for key in pp_idkeys)\n",
    "        ppcfg = dict(zip(pp_idkeys, pp_idvals))\n",
    "        pp_mdlcfg = self.get_ppmdlcfg(ppcfg)\n",
    "        if pp_idvals in pp_cache:\n",
    "            (cri_pp, u_dims, x_dims) = pp_cache[pp_idvals]\n",
    "        else:\n",
    "            cri_pp = make_pp('cstm', dict(), {'cstm': pp_mdlcfg}, tch_device, tch_dtype)\n",
    "            cri_pp.infer(data_dict, trn_spltidxs, n_seeds, eval_bs, hash_data=hash_data, \n",
    "                cache_path=f'{cache_dir}/pptmp.tar')\n",
    "            u_dims = cri_pp.get_dims(data_dims, types='output', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "            x_dims = cri_pp.get_dims(data_dims, types='input', n_seeds=n_seeds, n_mb=1, n_dims=2)\n",
    "            pp_cache[pp_idvals] = (cri_pp, u_dims, x_dims)\n",
    "\n",
    "        ######### Getting the data\n",
    "        # The scenario / trajectory index\n",
    "        i_snr = rowdictcp.pop('i_snr')\n",
    "        # The time point index\n",
    "        i_time = rowdictcp.pop('i_time')\n",
    "        # The train/test split random seed index\n",
    "        i_seed = rowdictcp.pop('i_seed')\n",
    "        # Now, we collect these indices from the full data.\n",
    "\n",
    "        key_origcache = (*pp_idvals, i_snr, i_time)\n",
    "        if key_origcache not in orig_cache:\n",
    "            x_mbs = dict()\n",
    "            for x_node, (n_xchnls, n_xlen) in x_dims.items():\n",
    "                x_tnsrall_ = data_dict[x_node]\n",
    "                assert x_tnsrall_.shape == (n_snr * n_t, n_xchnls, n_xlen)\n",
    "                x_tnsrall = x_tnsrall_.reshape(n_snr, n_t, n_xchnls, n_xlen)\n",
    "                assert x_tnsrall.shape == (n_snr, n_t, n_xchnls, n_xlen)\n",
    "                x_mb_ = x_tnsrall[i_snr, i_time]\n",
    "                assert x_mb_.shape == (n_xchnls, n_xlen)\n",
    "                x_mb = x_mb_.reshape(1, 1, n_xchnls, n_xlen).expand(n_seeds, 1, n_xchnls, n_xlen)\n",
    "                assert x_mb.shape == (n_seeds, 1, n_xchnls, n_xlen)\n",
    "                x_mbs[x_node] = x_mb\n",
    "\n",
    "            ######### Applying the forward loop\n",
    "            u_mbs, u_shaper = cri_pp.forward(x_mbs, full=True)\n",
    "            for u_node, (n_uchnls, n_ulen) in u_dims.items():\n",
    "                u_mb = u_mbs[u_node]\n",
    "                assert u_mb.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "\n",
    "            # Storing the evaluations in the array cache dictionary\n",
    "            u_mbsnp = dict()\n",
    "            for u_node, u_mb in u_mbs.items():\n",
    "                (n_uchnls, n_ulen) = u_mb.shape[-2:]\n",
    "                assert u_mb.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "                u_mbnp = u_mb.detach().cpu().numpy()\n",
    "                assert u_mbnp.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "                u_mbsnp[u_node] = u_mbnp\n",
    "            \n",
    "            orig_cache[(*pp_idvals, i_snr, i_time)] = (u_mbsnp, u_shaper)\n",
    "\n",
    "        assert key_origcache in orig_cache, dedent(f'''\n",
    "            There must be a mistake with the cache ids in saving/loading since \n",
    "            the data should have been available at this point in the cache:\n",
    "                rowdict = {rowdict}''')\n",
    "\n",
    "        u_mbsnp, u_shaper = orig_cache[key_origcache]\n",
    "\n",
    "        # The visualization node\n",
    "        v_node = rowdictcp.pop('v_node')\n",
    "        # The variable name ('orig' or 'rcnst')\n",
    "        v_varname = rowdictcp.pop('v_varname')\n",
    "        # The learning noise amplitude for magnitude\n",
    "        noise_mag = rowdictcp.pop('noise_mag')\n",
    "        # The learning noise amplitude for the normalized image\n",
    "        noise_nrm = rowdictcp.pop('noise_nrm')\n",
    "        # The learning noise amplitude for the normalized counts\n",
    "        noise_cnt = rowdictcp.pop('noise_cnt')\n",
    "        # The noise seed\n",
    "        seed_noise = rowdictcp.pop('seed_noise')\n",
    "\n",
    "        if v_varname == 'orig':\n",
    "            u_mbnp = u_mbsnp[v_node]\n",
    "            n_uchnls, n_ulen = u_mbnp.shape[-2:]\n",
    "            assert u_mbnp.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "            output = u_mbnp[i_seed, 0]\n",
    "            assert output.shape == (n_uchnls, n_ulen)\n",
    "        elif v_varname == 'rcnst':\n",
    "            key_rcnstcache = (*pp_idvals, i_snr, i_time, seed_noise, noise_mag, noise_nrm, noise_cnt)\n",
    "            if key_rcnstcache not in rcnst_cache:\n",
    "                # Instantiating the random seed\n",
    "                np_random = np.random.RandomState(seed=seed_noise)\n",
    "\n",
    "                # Letting it run for a while!\n",
    "                np_random.randn(10000)\n",
    "\n",
    "                # Applying the noise injection business\n",
    "                uhat_mbs = dict()\n",
    "                uhat_noiseamps = {'m_prthstmag': noise_mag, 'm_chmprtnrm': noise_nrm, 'n_prthstnrm': noise_cnt}\n",
    "                for u_node, (n_uchnls, n_ulen) in u_dims.items():\n",
    "                    u_mbnp = u_mbsnp[u_node]\n",
    "                    assert u_mbnp.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "                    noise_amp = uhat_noiseamps[u_node]\n",
    "                    u_noisemb = np_random.randn(n_seeds, 1, n_uchnls, n_ulen) * noise_amp\n",
    "                    assert u_noisemb.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "                    uhat_mbnp = u_mbnp + u_noisemb\n",
    "                    assert uhat_mbnp.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "                    uhat_mb = torch.from_numpy(uhat_mbnp).to(device=tch_device, dtype=tch_dtype)\n",
    "                    assert uhat_mb.shape == (n_seeds, 1, n_uchnls, n_ulen)\n",
    "                    uhat_mbs[u_node] = uhat_mb\n",
    "                \n",
    "                xhat_mbs = cri_pp.inverse(uhat_mbs, u_shaper, strict=True, full=True)\n",
    "                \n",
    "                for x_node, (n_xchnls, n_xlen) in x_dims.items():\n",
    "                    xhat_mb = xhat_mbs[x_node]\n",
    "                    assert xhat_mb.shape == (n_seeds, 1, n_xchnls, n_xlen)\n",
    "\n",
    "                xhat_mbsnp = {x_node: xhat_mb.detach().cpu().numpy() \n",
    "                    for x_node, xhat_mb in xhat_mbs.items()}\n",
    "\n",
    "                rcnst_cache[key_rcnstcache] = xhat_mbsnp\n",
    "            \n",
    "            assert key_rcnstcache in rcnst_cache, dedent(f'''\n",
    "                There must be a mistake with the cache ids in saving/loading since \n",
    "                the data should have been available at this point in the cache:\n",
    "                    rowdict = {rowdict}''')\n",
    "            \n",
    "            xhat_mbsnp = rcnst_cache[key_rcnstcache]\n",
    "            xhat_mbnp = xhat_mbsnp[v_node]\n",
    "            n_xchnls, n_xlen = xhat_mbnp.shape[-2:]\n",
    "            assert xhat_mbnp.shape == (n_seeds, 1, n_xchnls, n_xlen)\n",
    "            output = xhat_mbnp[i_seed, 0]\n",
    "            assert output.shape == (n_xchnls, n_xlen)\n",
    "        else:\n",
    "            raise ValueError(f'undefined v_varname={v_varname}')\n",
    "\n",
    "        while (n_ppcache is not None) and (len(pp_cache) > n_ppcache):\n",
    "            pp_cache.pop(next(iter(pp_cache)))\n",
    "\n",
    "        while (n_origcache is not None) and (len(orig_cache) > n_origcache):\n",
    "            orig_cache.pop(next(iter(orig_cache)))\n",
    "\n",
    "        while (n_rcnstcache is not None) and (len(rcnst_cache) > n_rcnstcache):\n",
    "            rcnst_cache.pop(next(iter(rcnst_cache)))\n",
    "\n",
    "        rowdictcp.pop('chems', None)\n",
    "        rowdictcp.pop('v_repr', None)\n",
    "        rowdictcp.pop('glyph_name', None)\n",
    "        assert len(rowdictcp) == 0, f'unused options: {rowdictcp}'\n",
    "\n",
    "        if (v_node in ('m_prthstmag', 'm02', 'm03', 'm04', 'm05', 'm05a', 'm05b')) and (ppcfg['magdim'] == 'n_chem'):\n",
    "            output = output.T\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Changer:\n",
    "    def __init__(self, ctrl_refs, rio_handler, glyph_type2vreprs,\n",
    "        bk_srcshie, bk_glyphviewshie, glyph_infos, fig_infos, \n",
    "        max_strmdrows, verbose):\n",
    "        self.ctrl_refs = ctrl_refs\n",
    "        self.last_sceneid = None\n",
    "\n",
    "        self.rio_handler = rio_handler\n",
    "        self.bk_srcshie = bk_srcshie\n",
    "        self.bk_glyphviewshie = bk_glyphviewshie\n",
    "\n",
    "        # Applying a safe hie2deep, no matter glyph_keyvals is empty or not.\n",
    "        self.bk_srcs = defaultdict(dict)\n",
    "        for (glyph_name, *glyph_keyvals), bk_srcinfo in bk_srcshie.items():\n",
    "            self.bk_srcs[glyph_name][tuple(glyph_keyvals)] = bk_srcinfo\n",
    "\n",
    "        self.glyph_infos = glyph_infos\n",
    "        self.glyph_type2vreprs = glyph_type2vreprs\n",
    "        self.fig_infos = fig_infos\n",
    "        self.max_strmdrows = max_strmdrows\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.clear_data()\n",
    "\n",
    "    def clear_data(self):\n",
    "        if self.verbose:\n",
    "            print('Performing a data flush!', flush=True)\n",
    "        for (glyph_name, *glyph_keyvals), bk_srcinfo in self.bk_srcshie.items():\n",
    "            # Example: \n",
    "            #    glyph_name = 'sct'\n",
    "            #    glyph_keydims = ['lbl_name']\n",
    "            #    glyph_keyvals = ('train/orig',)\n",
    "            #    bk_src = ColumnDataSource(...)\n",
    "\n",
    "            # Getting the bokeh source and view and size\n",
    "            bk_src = bk_srcinfo['src']\n",
    "            # The mapping of each data id to the starting and ending \n",
    "            # row index within the Bokeh Data Source\n",
    "            bk_srcbook = bk_srcinfo['book']\n",
    "\n",
    "            # Clearing out any existing data in the bokeh sources\n",
    "            bk_src.data = {col: [] for col in bk_src.column_names}\n",
    "\n",
    "            # Zeroing out the length of the bokeh source\n",
    "            bk_srcinfo['len'] = 0\n",
    "            # Emptying the source book\n",
    "            bk_srcbook.clear()\n",
    "        \n",
    "        # Clearing out any existing data in the bokeh cds views\n",
    "        for (fig_name, glyph_name, *glyph_keyvals), bk_glyphinfo in self.bk_glyphviewshie.items():\n",
    "            bk_glyph = bk_glyphinfo['glyph']\n",
    "            bk_view = bk_glyphinfo['view']\n",
    "            bk_view.filter.indices = []\n",
    "\n",
    "    @property\n",
    "    def nrows_bksrcs(self):\n",
    "        return sum(bk_srcinfo['len'] for bk_srcinfo in self.bk_srcshie.values())\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_controls(ctrl_refs, verbose):\n",
    "        #######################################################################\n",
    "        ############### Reading the Controls and Adjusting Them ###############\n",
    "        #######################################################################\n",
    "        sceneid = dict()\n",
    "        for compid, ctrl_ref in ctrl_refs.items():\n",
    "            hpdf5 = ctrl_ref['hpdf']\n",
    "            ctrl_book = ctrl_ref['book']\n",
    "            ctrl_infos = ctrl_ref['infos']\n",
    "            ctrl_cols = ctrl_ref['cols']\n",
    "                \n",
    "            ctrl_bookrcrsd = ctrl_book\n",
    "            for ctrl_info in ctrl_infos:\n",
    "                ctrl_col = ctrl_info['col']\n",
    "                ctrl_ttl = ctrl_info['title']\n",
    "                ctrl_type = ctrl_info['type']\n",
    "                ctrl_enc = ctrl_info['enc']\n",
    "                ctrl_dec = ctrl_info['dec']\n",
    "                control = ctrl_info['control']\n",
    "\n",
    "                # The user-selected value for the control\n",
    "                if ctrl_type in ('catslider', 'slider', 'menu'):\n",
    "                    ctrl_userval = control.value\n",
    "                elif ctrl_type in ('radiobtngrp',):\n",
    "                    ctrl_userval = control.labels[control.active]\n",
    "                else:\n",
    "                    raise ValueError(f'undefined ctrl_type={ctrl_type}')\n",
    "\n",
    "                # The available set of values for the control\n",
    "                ctrl_avlblevals = list(ctrl_bookrcrsd)\n",
    "\n",
    "                if ctrl_userval in ctrl_bookrcrsd:\n",
    "                    # Looks like this selected control value is available and \n",
    "                    # has a scene defined in our data.\n",
    "                    ctrl_uservalnew = ctrl_userval                \n",
    "                else:\n",
    "                    # Looks like this selected control value is not available and \n",
    "                    # does not have a scene defined in our data.\n",
    "                    ctrl_uservalnew = ctrl_avlblevals[0]\n",
    "\n",
    "                # Updating the control attributes accordingly\n",
    "                if ctrl_type == 'catslider':\n",
    "                    if control.categories != ctrl_avlblevals:\n",
    "                        control.categories = ctrl_avlblevals\n",
    "                    if control.value != ctrl_uservalnew:\n",
    "                        control.value = ctrl_uservalnew\n",
    "                elif ctrl_type == 'radiobtngrp':\n",
    "                    if control.labels != ctrl_avlblevals:\n",
    "                        control.labels = ctrl_avlblevals\n",
    "                    if ctrl_userval != ctrl_uservalnew:\n",
    "                        control.active = ctrl_avlblevals.index(ctrl_uservalnew)\n",
    "                elif ctrl_type == 'menu':\n",
    "                    options = [(val, str(val)) for val in ctrl_avlblevals]\n",
    "                    if control.options != options:\n",
    "                        control.options = options\n",
    "                    if control.value != ctrl_uservalnew:\n",
    "                        control.value = ctrl_uservalnew\n",
    "                else:\n",
    "                    raise ValueError(f'undefined ctrl_type={ctrl_type}')\n",
    "\n",
    "                # Recursing over the control book\n",
    "                ctrl_bookrcrsd = ctrl_bookrcrsd[ctrl_uservalnew]\n",
    "            \n",
    "            # After we have recused over all the controls, the value of \n",
    "            # the control book should be the scene id.\n",
    "            comp_sceneid = ctrl_bookrcrsd\n",
    "\n",
    "            for ctrl_col, ctrl_val in zip(ctrl_cols, comp_sceneid):\n",
    "                assert ctrl_col not in sceneid, dedent(f'''\n",
    "                    I am not sure how a single column is controlled \n",
    "                    by two components:\n",
    "                        Offending column: {ctrl_col}\n",
    "                        First Value: {sceneid[ctrl_col]}\n",
    "                        Second Value: {sceneid[ctrl_col]}''')\n",
    "                sceneid[ctrl_col] = ctrl_val\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Control: scene={sceneid}')\n",
    "        \n",
    "        return sceneid\n",
    "    \n",
    "    @staticmethod\n",
    "    @without_property_validation\n",
    "    def load_data(sceneid, bk_glyphviewshie, glyph_infos, glyph_type2vreprs, bk_srcs, verbose):\n",
    "        #######################################################################\n",
    "        ################# Reading the New Data From the Disk ##################\n",
    "        #######################################################################\n",
    "        # The columns that define a unique hdf key data to be loaded\n",
    "        for (fig_name, glyph_name, *glyph_keyvals), bk_glyphinfo in bk_glyphviewshie.items():\n",
    "            fig_info = fig_infos[fig_name]\n",
    "            hpcdf_fig = fig_info['hpcdf']\n",
    "\n",
    "            # Example: \n",
    "            #    glyph_name = 'sct'\n",
    "            #    glyph_bkcols = ['x', 'y']\n",
    "            #    glyph_keydims = ['lbl_name']\n",
    "            #    glyph_keyvals = ['train/orig']\n",
    "            #    glyph_vars = (('lbl_name': 'train/orig'),)\n",
    "            #    bk_src = ColumnDataSource(...)\n",
    "            #    bk_view = CDSView(...)\n",
    "            #    bk_srclen = 0\n",
    "            #    glyph_idctrls = 'all'\n",
    "            #    glyph_idcols = ['fpidx', 'eid', 'vid', 'v_space', 'v_node', \n",
    "            #       'v_split', 'v_varname', 'ppid', 'i_seed', 'i_epoch', 'i_time']\n",
    "            glyph_info = glyph_infos[glyph_name]\n",
    "            # The glyph type\n",
    "            glyph_type = glyph_info['type']\n",
    "            # The glyph type v_reprs\n",
    "            glyph_vreprs = glyph_type2vreprs[glyph_type]\n",
    "\n",
    "            # The glyph key dimensions (e.g., the 'sct' glyph needs to be `lbl_name`-specific).\n",
    "            glyph_keydims = glyph_info['keydims']\n",
    "            assert len(glyph_keydims) == len(glyph_keyvals)\n",
    "\n",
    "            # Restricting the scene hpdf to this particular glyph and figure\n",
    "            glyph_vars = dict(zip(glyph_keydims, glyph_keyvals))\n",
    "            glyph_vars['glyph_name'] = glyph_name\n",
    "\n",
    "            hpcdf_fg = hpcdf_fig.select(incdict=glyph_vars, excdict=None, \n",
    "                copy='shallow', reset_index=False, has_wildcards=False)\n",
    "\n",
    "            # The set of columns identifying one set of data points from another\n",
    "            glyph_idcols = glyph_info['idcols']\n",
    "\n",
    "            # The subset of control columns responsible for identifying the current \n",
    "            # dataset for the glyph. Use 'all' to use all control columns.\n",
    "            glyph_idctrls = glyph_info['idctrls']\n",
    "            assert (glyph_idctrls == 'all') or isinstance(glyph_idctrls, (tuple, list))\n",
    "            # Extracting the control values related to this particular glyph\n",
    "            glyph_scnid = sceneid if (glyph_idctrls == 'all') else {col: sceneid[col] for col in glyph_idctrls}\n",
    "            \n",
    "            hpcdf_fgs = hpcdf_fg.select(incdict=glyph_scnid, excdict=None, \n",
    "                copy='shallow', reset_index=False, has_wildcards=False)\n",
    "\n",
    "            if hpcdf_fgs.shape[0] == 0:\n",
    "                # The `figure + glyph + scene` combination defines an empty data set\n",
    "                glyph_idvals, hpcdf_fgs2 = None, None\n",
    "            else:\n",
    "                # Making sure the `figure + glyph + scene` combination define a single data set\n",
    "                assert hpcdf_fgs.shape[0] == len(glyph_vreprs), dedent(f'''\n",
    "                    The `figure + glyph + scene` combination did not define \n",
    "                    a unique dataset to load. We expected the number of dataframe \n",
    "                    rows to be identical to the number of glyph v_reprs.\n",
    "                        figure name: {fig_name}\n",
    "                        glyph name: {glyph_name}\n",
    "                        glyph vars: {glyph_vars}\n",
    "                        The narrowed down df size: {hpcdf_fgs.shape[0]}\n",
    "                        The glyph v_reprs: {glyph_vreprs}\n",
    "                        The glyph scene id: {glyph_scnid}''')\n",
    "\n",
    "                hpcdf_fgsgb = list(hpcdf_fgs.groupby(glyph_idcols, sort=False, observed=True))\n",
    "                assert len(hpcdf_fgsgb) == 1, dedent(f'''\n",
    "                    Multiple datasets were found for this particular glyph and scene. The glyph \n",
    "                    id cols may be under-specified, or the glyph must be too special!\n",
    "                        figure name: {fig_name}\n",
    "                        glyph name: {glyph_name}\n",
    "                        glyph vars: {glyph_vars}\n",
    "                        The narrowed down df size: {hpcdf_fgs.shape[0]}\n",
    "                        The glyph v_reprs: {glyph_vreprs}\n",
    "                        The glyph scene id: {glyph_scnid}''')\n",
    "\n",
    "                # `glyph_idvals` is a tuple of values. It will be used as a key to \n",
    "                # the `bk_srcbook` to identify the starting and ending indices.\n",
    "                glyph_idvals, hpcdf_fgs2 = hpcdf_fgsgb[0]\n",
    "\n",
    "            # Now, we will open up the bokeh sources business\n",
    "            bk_srcinfo = bk_srcs[glyph_name][tuple(glyph_keyvals)]\n",
    "            # Getting the bokeh source and view and size\n",
    "            bk_src = bk_srcinfo['src']\n",
    "            # The current size of the bokeh source\n",
    "            n_bksrcrows = bk_srcinfo['len']\n",
    "            # The mapping of each data id to the starting and ending \n",
    "            # row indices within the Bokeh Data Source.\n",
    "            bk_srcbook = bk_srcinfo['book']\n",
    "\n",
    "            if glyph_idvals not in bk_srcbook:\n",
    "                # Example:\n",
    "                #   data_repr = {'pnts': np.randn(n_pnts, 2)}\n",
    "                #   data_repr = {\n",
    "                #       'mu': np.randn(n_pnts, 2),\n",
    "                #       'sig': np.randn(n_pnts, 2),\n",
    "                #       'phi': np.randn(n_pnts, 2)}\n",
    "\n",
    "                if hpcdf_fgs2 is not None:\n",
    "                    data_repr = dict()\n",
    "                    hpcdf_fgs3 = hpcdf_fgs2.dense()\n",
    "\n",
    "                    for rowvals in hpcdf_fgs3.itertuples(index=False, name='row01'):\n",
    "                        rowdict = dict(zip(hpcdf_fgs3.columns, rowvals))\n",
    "                        # Calling rio_handler to grab the data\n",
    "                        data_lblnp3 = rio_handler(rowdict)\n",
    "                        data_repr[rowdict['v_repr']] = data_lblnp3\n",
    "\n",
    "                    # Converting the `data_repr` into a dictionary that fits the needs of Bokeh\n",
    "                    bk_srcdata, n_pnts = Changer.get_bksrcdata(data_repr, glyph_info)\n",
    "                else:\n",
    "                    # There was no data to load!\n",
    "                    assert glyph_idvals is None\n",
    "                    bk_srcdata, n_pnts = None, 0\n",
    "        \n",
    "                if verbose:\n",
    "                    print(f'Streaming: {(glyph_name, *glyph_keyvals)}')\n",
    "\n",
    "                if n_pnts > 0:\n",
    "                    # Streaming the new data to the figure source\n",
    "                    bk_src.stream(bk_srcdata)\n",
    "                \n",
    "                # Updating the Bokeh source book to reflect the starting and ending row indices\n",
    "                bk_srcbook[glyph_idvals] = (n_bksrcrows, n_bksrcrows + n_pnts)\n",
    "                # Bumping up the current size of the bokeh source\n",
    "                bk_srcinfo['len'] = n_bksrcrows + n_pnts\n",
    "\n",
    "            # The starting and ending row indices of this data\n",
    "            i1_view, i2_view = bk_srcbook[glyph_idvals]\n",
    "\n",
    "            # Getting the bokeh view \n",
    "            bk_view = bk_glyphinfo['view']\n",
    "\n",
    "            if verbose:\n",
    "                print(f'View Update: {(fig_name, glyph_name, *glyph_keyvals)} -> {i1_view}: {i2_view}')\n",
    "\n",
    "            # Updating the indecis\n",
    "            bkview_indices = list(range(i1_view, i2_view))\n",
    "            bk_view.filter.indices = bkview_indices\n",
    "\n",
    "    @staticmethod\n",
    "    @without_property_validation\n",
    "    def get_bksrcdata(data_repr, glyph_info):\n",
    "        # Example:\n",
    "        #   data_repr = {'pnts': np.randn(n_pnts, 2)}\n",
    "        #   data_repr = {\n",
    "        #       'mu': np.randn(n_pnts, 2),\n",
    "        #       'sig': np.randn(n_pnts, 2),\n",
    "        #       'phi': np.randn(n_pnts, 2)}\n",
    "\n",
    "        # Example: \n",
    "        #    glyph_name = 'sctr'\n",
    "        #    glyph_type = 'sct'\n",
    "        #    glyph_bkcols = ['x', 'y']\n",
    "        #    glyph_keydims = ['lbl_name']\n",
    "        #    glyph_keyvals = ('train/orig',)\n",
    "        #    glyph_vars = {'lbl_name': 'train/orig'}\n",
    "        #    bk_src = ColumnDataSource(...)\n",
    "        #    bk_view = CDSView(...)\n",
    "        #    bk_srclen = 0\n",
    "        glyph_type = glyph_info['type']\n",
    "\n",
    "        # The bokeh column names for this glyph (e.g., glyph_bkcols = ['x', 'y'])\n",
    "        glyph_bkcols = glyph_info['bkcols']\n",
    "\n",
    "        # The number of points/rows to be streamed to the bokeh sources\n",
    "        n_pnts = 0\n",
    "        bk_srcdata = dict()\n",
    "        #######################################################################\n",
    "        ################## Preparing the Scatter Glyphs Data ##################\n",
    "        #######################################################################\n",
    "        if (glyph_type == 'sct') and ('pnts' in data_repr):\n",
    "            pnts_np = data_repr.pop('pnts')\n",
    "            n_pnts = pnts_np.shape[0]\n",
    "            assert pnts_np.shape == (n_pnts, 2)\n",
    "            \n",
    "            bk_srcdata['x'] = pnts_np[:, 0]\n",
    "            bk_srcdata['y'] = pnts_np[:, 1]\n",
    "            assert set(bk_srcdata.keys()) == set(glyph_bkcols)\n",
    "\n",
    "        #######################################################################\n",
    "        ################## Preparing the Ellipse Glyphs Data ##################\n",
    "        #######################################################################\n",
    "        if (glyph_type == 'blb') and all(key in data_repr for key in ('mu', 'sig', 'phi')):\n",
    "            mus_np = data_repr.pop('mu')\n",
    "            n_pnts = mus_np.shape[0]\n",
    "            assert mus_np.shape == (n_pnts, 2)\n",
    "            sigs_np = data_repr.pop('sig')\n",
    "            assert sigs_np.shape == (n_pnts, 2)\n",
    "            phis_np = data_repr.pop('phi')\n",
    "            assert phis_np.shape == (n_pnts, 1)\n",
    "\n",
    "            bk_srcdata['x'] = mus_np[:, 0]\n",
    "            bk_srcdata['y'] = mus_np[:, 1]\n",
    "            bk_srcdata['w'] = (2 * sigs_np[:, 0])\n",
    "            bk_srcdata['h'] = (2 * sigs_np[:, 1])\n",
    "            bk_srcdata['a'] = phis_np[:, 0]\n",
    "            assert set(bk_srcdata.keys()) == set(glyph_bkcols)\n",
    "        \n",
    "        if (glyph_type == 'blb') and ('mu' in data_repr):\n",
    "            mus_np = data_repr.pop('mu')\n",
    "            # We cannot do anything for this odd case!\n",
    "\n",
    "        #######################################################################\n",
    "        #################### Preparing the Area Glyphs Data ###################\n",
    "        #######################################################################\n",
    "        if (glyph_type == 'vas') and ('pnts' in data_repr):\n",
    "            n_bins, n_chem = glyph_info['n_x'], glyph_info['n_y']\n",
    "            pnts_np = data_repr.pop('pnts')\n",
    "            assert pnts_np.shape == (n_chem, n_bins)\n",
    "            \n",
    "            x_values = glyph_info['x_values']\n",
    "            assert len(x_values) == n_bins + 1\n",
    "            y_nm2idx = glyph_info['y_nm2idx']\n",
    "            \n",
    "            # Note: Use the following for a simple implementation\n",
    "            # n_pnts = n_bins\n",
    "            # bk_srcdata['x'] = x_values[:-1]\n",
    "            # for y_name, i_chem in y_nm2idx.items():\n",
    "            #     y_values1 = pnts_np[i_chem]\n",
    "            #     bk_srcdata[y_name] = y_values1\n",
    "\n",
    "            n_pnts =  2 * (n_bins + 1)\n",
    "            x_values1 = np.stack([x_values, x_values], axis=1)\n",
    "            assert x_values1.shape == (n_bins + 1, 2)\n",
    "            x_values2 = x_values1.ravel().tolist()\n",
    "            assert len(x_values2) == (n_bins + 1) * 2\n",
    "            # Example:\n",
    "            #    x_values  =  [x_0, x_1, x_2, ..., x_{n-1}]\n",
    "            #    x_values1 = [[x_0,     x_0 + eps],\n",
    "            #                 [x_1,     x_1 + eps],\n",
    "            #                 [x_2,     x_2 + eps],\n",
    "            #                 ...,\n",
    "            #                 [x_{n-1}, x_{n-1} + eps]]\n",
    "            #    x_values2 = [x_0, x_0, x_1, x_1, x_2, x_2, ..., x_{n-2}, x_{n-1}, x_{n-1}]\n",
    "            #    y_values3 = [0.0, y_0, y_0, y_1, y_1, y_2, ..., y_{n-1}, y_{n-1}, 0.0    ]\n",
    "            bk_srcdata['x'] = x_values2\n",
    "            for y_name, i_chem in y_nm2idx.items():\n",
    "                y_values1 = pnts_np[i_chem]\n",
    "                assert y_values1.shape == (n_bins,)\n",
    "                y_values2 = np.stack([y_values1, y_values1], axis=1)\n",
    "                assert y_values2.shape == (n_bins, 2)\n",
    "                y_values3 = [0.0] + y_values2.ravel().tolist() + [0.0]\n",
    "                assert len(y_values3) == ((n_bins + 1) * 2)\n",
    "                bk_srcdata[y_name] = y_values3\n",
    "\n",
    "            assert set(bk_srcdata.keys()) == set(glyph_bkcols)\n",
    "        \n",
    "        #######################################################################\n",
    "        #################### Preparing the Bar Glyphs Data ####################\n",
    "        #######################################################################\n",
    "        if (glyph_type == 'vbs') and ('pnts' in data_repr):\n",
    "            n_bins, n_chem = glyph_info['n_x'], glyph_info['n_y']\n",
    "\n",
    "            pnts_np = data_repr.pop('pnts')\n",
    "            assert pnts_np.shape == (n_chem, n_bins)\n",
    "            \n",
    "            x_names = glyph_info['x_names']\n",
    "            assert len(x_names) == n_bins\n",
    "            y_nm2idx = glyph_info['y_nm2idx']\n",
    "            \n",
    "            # Note: Use the following for a simple implementation\n",
    "            n_pnts = n_bins\n",
    "            bk_srcdata['x'] = x_names\n",
    "            for y_name, i_chem in y_nm2idx.items():\n",
    "                y_values1 = pnts_np[i_chem]\n",
    "                bk_srcdata[y_name] = y_values1\n",
    "\n",
    "            assert set(bk_srcdata.keys()) == set(glyph_bkcols)\n",
    "\n",
    "        #######################################################################\n",
    "        #################### Preparing the Bar Glyphs Data ####################\n",
    "        #######################################################################\n",
    "        if (glyph_type == 'hm') and ('pnts' in data_repr):\n",
    "            n_bins, n_chem = glyph_info['n_x'], glyph_info['n_y']\n",
    "            \n",
    "            pnts_np = data_repr.pop('pnts')\n",
    "            assert pnts_np.shape == (n_chem, n_bins)\n",
    "            \n",
    "            x_values = glyph_info['x_values']\n",
    "            assert len(x_values) == n_bins + 1\n",
    "            y_nm2idx = glyph_info['y_nm2idx']\n",
    "            y_names = glyph_info['y_names']\n",
    "            n_y = len(y_names)\n",
    "            assert len(y_names) == n_y\n",
    "            y_idxs = [y_nm2idx[y_name] for y_name in y_names]\n",
    "            assert len(y_idxs) == n_y\n",
    "\n",
    "            x_cntrs = (x_values[:-1] + x_values[1:]) / 2.0\n",
    "            assert x_cntrs.shape == (n_bins,)\n",
    "            x_widths = (x_values[1:] - x_values[:-1])\n",
    "            assert x_widths.shape == (n_bins,)\n",
    "            v_2d = pnts_np[y_idxs]\n",
    "            assert v_2d.shape == (n_y, n_bins)\n",
    "\n",
    "            x_1d = np.broadcast_to(x_cntrs.reshape(1, n_bins), (n_y, n_bins)).ravel()\n",
    "            assert x_1d.shape == (n_y * n_bins,)\n",
    "\n",
    "            w_1d = np.broadcast_to(x_widths.reshape(1, n_bins), (n_y, n_bins)).ravel()\n",
    "            assert w_1d.shape == (n_y * n_bins,)\n",
    "\n",
    "            y_1d = [y_name for y_name in y_names for _ in range(n_bins)]\n",
    "            assert len(y_1d) == (n_y * n_bins)\n",
    "\n",
    "            v_1d = v_2d.ravel()\n",
    "            assert v_1d.shape == (n_y * n_bins,)\n",
    "\n",
    "            n_pnts = n_y * n_bins\n",
    "            bk_srcdata['x'] = x_1d\n",
    "            bk_srcdata['y'] = y_1d\n",
    "            bk_srcdata['w'] = w_1d\n",
    "            bk_srcdata['v'] = v_1d\n",
    "\n",
    "            assert set(bk_srcdata.keys()) == set(glyph_bkcols)\n",
    "        \n",
    "        assert all(len(vals) == n_pnts for key, vals in bk_srcdata.items())\n",
    "        assert len(data_repr) == 0, f'unused keys: {data_repr}'\n",
    "\n",
    "        return bk_srcdata, n_pnts\n",
    "     \n",
    "    @staticmethod\n",
    "    def update_legends(fig_infos, bk_glyphviewshie, bk_srcshie):\n",
    "        # Disabling legend labels without data\n",
    "        for fig_name, fig_info in fig_infos.items():\n",
    "            fig = fig_info['fig']\n",
    "            fig_type = fig_info['type']\n",
    "            if fig_type == 'scatter':\n",
    "                for handle in fig.legend.items:\n",
    "                    n_hndlpnts = sum(\n",
    "                        len(bk_glyph.view.filter.indices) \n",
    "                        for bk_glyph in handle.renderers)\n",
    "                        \n",
    "                    handle.visible = (n_hndlpnts > 0)\n",
    "\n",
    "        # Sharing the same color-bar range for all heatmaps within the same frame\n",
    "        framefig_vrngs = dict()\n",
    "        for (fig_name, glyph_name, *glyph_keyvals), bk_glyphinfo in bk_glyphviewshie.items():\n",
    "            bk_glyph = bk_glyphinfo['glyph']\n",
    "            bk_view = bk_glyphinfo['view']\n",
    "            fig_info = fig_infos[fig_name]\n",
    "            fig_frame = fig_info['frame']\n",
    "            fig_type = fig_info['type']\n",
    "            if fig_type == 'hmap':\n",
    "                bk_src = bk_srcshie[(glyph_name, *glyph_keyvals)]['src']\n",
    "                fig_vals = bk_src.data['v'][bk_view.filter.indices]\n",
    "                if len(fig_vals) > 0:\n",
    "                    framefig_vrngs[(fig_frame, fig_name)] = (fig_vals.min(), fig_vals.max())\n",
    "\n",
    "        for frame_name, fig_vrngs in hie2deep(framefig_vrngs, kind='tuple').items():\n",
    "            # Finding the minimum and maximum range of values in all figures\n",
    "            if len(fig_vrngs) > 0:\n",
    "                v_min = min(minmax[0] for minmax in fig_vrngs.values())\n",
    "                v_max = max(minmax[1] for minmax in fig_vrngs.values())\n",
    "            else:\n",
    "                v_min, v_max = None, None\n",
    "\n",
    "            # Setting the same range of values for all figures and color-bars\n",
    "            for (fig_name, glyph_name, *glyph_keyvals), bk_glyphinfo in bk_glyphviewshie.items():\n",
    "                if fig_name not in fig_vrngs:\n",
    "                    continue\n",
    "                fig_info = fig_infos[fig_name]\n",
    "                fig_type = fig_info['type']\n",
    "                assert fig_type == 'hmap'\n",
    "\n",
    "                bk_glyph = bk_glyphinfo['glyph']\n",
    "                bk_rectglyph, bk_cbglyph = bk_glyph\n",
    "                bk_rectglyph.glyph.fill_color.transform.low = v_min\n",
    "                bk_rectglyph.glyph.fill_color.transform.high = v_max\n",
    "\n",
    "    def __call__(self, attr, old, new, sceneid=None):\n",
    "        # Adjusting the controls and finding the right fpidx\n",
    "        if sceneid is None:\n",
    "            sceneid = self.adjust_controls(self.ctrl_refs, self.verbose)\n",
    "\n",
    "        # Only updating the bokeh data sources when the controls actually changed\n",
    "        if sceneid != self.last_sceneid:\n",
    "            if self.verbose:\n",
    "                print(f'Got a new scene: {sceneid}')\n",
    "                \n",
    "            # Clearing the client data if we have too much of it!\n",
    "            if self.nrows_bksrcs >= self.max_strmdrows:\n",
    "                self.clear_data()\n",
    "\n",
    "            # Loading the data from the disk\n",
    "            self.load_data(sceneid, self.bk_glyphviewshie, self.glyph_infos, \n",
    "                self.glyph_type2vreprs, self.bk_srcs, self.verbose)\n",
    "\n",
    "            # Disabling and enabling the legend handles\n",
    "            self.update_legends(self.fig_infos, self.bk_glyphviewshie, self.bk_srcshie)\n",
    "\n",
    "            # Updating the latest control scene\n",
    "            self.last_sceneid = sceneid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rio_handler = RioHandler(data_dict=data_dict, hash_data=hash_data, \n",
    "    trn_spltidxs=trn_spltidxs, n_seeds=n_seeds, eval_bs=eval_bs, \n",
    "    n_snr=n_snr, n_t=n_t, tch_device=tch_device, \n",
    "    tch_dtype=tch_dtype, n_ppcache=None, n_origcache=None, \n",
    "    n_rcnstcache=None, verbose=False)\n",
    "\n",
    "changer = Changer(ctrl_refs=ctrl_refs, glyph_type2vreprs=glyph_type2vreprs, \n",
    "    rio_handler=rio_handler, glyph_infos=glyph_infos, \n",
    "    bk_srcshie=bk_srcshie, bk_glyphviewshie=bk_glyphviewshie, \n",
    "    fig_infos=fig_infos, max_strmdrows=max_strmdrows, verbose=False)\n",
    "\n",
    "changer(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a control column to info mapping\n",
    "ctrl_col2info = dict()\n",
    "for compid, ctrl_ref in ctrl_refs.items():\n",
    "    for ctrl_info in ctrl_ref['infos']:\n",
    "        ctrl_col = ctrl_info['col']\n",
    "        assert ctrl_col not in ctrl_col2info\n",
    "        ctrl_col2info[ctrl_col] = ctrl_info\n",
    "\n",
    "# # The simple old code without any control layouts\n",
    "# controls_datalst = [ctrl_info['control'] \n",
    "#     for compid, ctrl_ref in ctrl_refs.items()\n",
    "#     for ctrl_info in ctrl_ref['infos']]\n",
    "# # Adding the control callbacks\n",
    "# for control in controls_datalst:\n",
    "#     control.on_change('value', changer)\n",
    "\n",
    "# Listing the actual controls in the same order as the user-specified control layout\n",
    "controls_cntnrlst = []\n",
    "for i_rowctrl, ctrl_layrow in enumerate(ctrl_layout):\n",
    "    for i_colctrl, ctrl_col in enumerate(ctrl_layrow):\n",
    "        container = None\n",
    "        if ctrl_col is not None:\n",
    "            ctrl_info = ctrl_col2info[ctrl_col]\n",
    "            container = ctrl_info['container']\n",
    "            # Adding the control callbacks\n",
    "            ctrl_type = ctrl_info['type']\n",
    "            control = ctrl_info['control']\n",
    "            control.on_change('active' if ctrl_type == 'radiobtngrp' else 'value', changer)\n",
    "            \n",
    "        controls_cntnrlst.append(container)\n",
    "\n",
    "if need_sctvisualctrls:\n",
    "    controls_cntnrlst += [alpha_slider, size_slider]\n",
    "\n",
    "frame_info = {'title': 'Data Selection', 'type': 'ctrl', 'nrows': n_ctrlrows, 'ncols': n_ctrlcols}\n",
    "frame_infos = {'datasel': frame_info, **frame_infos}\n",
    "\n",
    "# Making the data selection control frame\n",
    "for frame_name, frame_info in frame_infos.items():\n",
    "    frame_type = frame_info['type']\n",
    "    if frame_type in ('bar', 'hmap', 'scatter'):\n",
    "        frame_info['bkchilds'] = [fig_infos[fig_name]['fig'] \n",
    "            for fig_name in frame_info['fig_names']]\n",
    "    elif frame_type in ('ctrl',):\n",
    "        frame_info['bkchilds'] = controls_cntnrlst\n",
    "    else:\n",
    "        raise ValueError(f'undefined frame_type={frame_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dirctn = 'col'\n",
    "\n",
    "for frame_name, frame_info in frame_infos.items():\n",
    "    frame_type = frame_info['type']\n",
    "    n_framerows = frame_info['nrows']\n",
    "    n_framecols = frame_info['ncols']\n",
    "    frame_bkchilds = frame_info['bkchilds']\n",
    "    n_bkchilds = len(frame_bkchilds)\n",
    "\n",
    "    if frame_type == 'ctrl':\n",
    "        sizing_mode = {'col': 'stretch_width', 'row': 'stretch_width'}[layout_dirctn]\n",
    "        grid_layout = grid(frame_bkchilds, ncols=n_framecols, \n",
    "            nrows=n_framerows, sizing_mode=sizing_mode)\n",
    "    elif frame_type in ('bar', 'hmap', 'scatter'):\n",
    "        sizing_mode = {'col': 'stretch_both', 'row': 'stretch_both'}[layout_dirctn]\n",
    "        grid_layout = gridplot(frame_bkchilds, ncols=n_framecols,\n",
    "            merge_tools=True, sizing_mode=sizing_mode)\n",
    "    else:\n",
    "        raise ValueError(f'undefined frame_type={frame_type}')\n",
    "\n",
    "    frame_title = frame_info['title']\n",
    "    headszng_kwargs = {\n",
    "        'col': dict(width=3000, height=ctrl_height, sizing_mode='fixed'),\n",
    "        'row': dict(width=ctrl_width, height=ctrl_height, sizing_mode='fixed')}[layout_dirctn]\n",
    "    frame_heading = Div(text=f'<h1 style=\"text-align: center\">{frame_title}</h1>',\n",
    "        styles={'color': header_color}, disable_math=False, **headszng_kwargs)\n",
    "\n",
    "    if layout_dirctn == 'col':\n",
    "        sizing_mode = 'stretch_width' if frame_type == 'ctrl' else \"scale_height\"\n",
    "        frame_szngkwargs = dict(sizing_mode=sizing_mode, width=3000)\n",
    "    elif layout_dirctn == 'row':\n",
    "        sizing_mode = 'fixed' if frame_type == 'ctrl' else \"stretch_both\"\n",
    "        frame_szngkwargs = dict(sizing_mode=sizing_mode, height=1500)\n",
    "    else:\n",
    "        raise ValueError(f'undefined layout_dirctn={layout_dirctn}')\n",
    "        \n",
    "    frame_layout = column([frame_heading, grid_layout], background=background_color, \n",
    "        **frame_szngkwargs)\n",
    "    frame_info['layouts'] = {'heading': frame_heading, 'grid': grid_layout, 'frame': frame_layout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if layout_dirctn == 'col':\n",
    "    ctrl_frame = frame_infos['datasel']['layouts']['frame']\n",
    "    \n",
    "    # The PP Normalization Bar Plots\n",
    "    bar01_frame = frame_infos['stckbar1']['layouts']['frame']\n",
    "    bar02_frame = frame_infos['stckbar2']['layouts']['frame']\n",
    "    bar03_frame = frame_infos['stckbar3']['layouts']['frame']\n",
    "    bar04_frame = frame_infos['stckbar4']['layouts']['frame']\n",
    "    bar05_frame = frame_infos['stckbar5']['layouts']['frame']\n",
    "    bar06_frame = frame_infos['stckbar6']['layouts']['frame']\n",
    "\n",
    "    # The PP Magnitude Bar Plots with the diameter bins as the x axis\n",
    "    bar07_frame = frame_infos['magbins1']['layouts']['frame']\n",
    "    bar08_frame = frame_infos['magbins2']['layouts']['frame']\n",
    "    bar09_frame = frame_infos['magbins3']['layouts']['frame']\n",
    "    bar10_frame = frame_infos['magbins4']['layouts']['frame']\n",
    "\n",
    "    # The PP Magnitude Bar Plots with the chemicals as the x axis\n",
    "    bar11_frame = frame_infos['magchems1']['layouts']['frame']\n",
    "    bar12_frame = frame_infos['magchems2']['layouts']['frame']\n",
    "    bar13_frame = frame_infos['magchems3']['layouts']['frame']\n",
    "    bar14_frame = frame_infos['magchems4']['layouts']['frame']\n",
    "\n",
    "    # The PP Magnitude Bar Plots with a single column on the x axis\n",
    "    bar15_frame = frame_infos['magbar1']['layouts']['frame']\n",
    "    bar16_frame = frame_infos['magbar2']['layouts']['frame']\n",
    "    bar17_frame = frame_infos['magbar3']['layouts']['frame']\n",
    "    bar18_frame = frame_infos['magbar4']['layouts']['frame']\n",
    "    \n",
    "    # The PP Count Bar Plots with a single column on the x axis\n",
    "    bar19_frame = frame_infos['cntbins1']['layouts']['frame']\n",
    "    bar20_frame = frame_infos['cntbins2']['layouts']['frame']\n",
    "    bar21_frame = frame_infos['cntbins3']['layouts']['frame']\n",
    "    bar22_frame = frame_infos['cntbins4']['layouts']['frame']\n",
    "\n",
    "    # The PP Normalization Heatmaps\n",
    "    # hmap01_frame = frame_infos['heatmap1']['layouts']['frame']\n",
    "    # hmap02_frame = frame_infos['heatmap2']['layouts']['frame']\n",
    "    # hmap03_frame = frame_infos['heatmap3']['layouts']['frame']\n",
    "    # # hmap04_frame = frame_infos['heatmap4']['layouts']['frame']\n",
    "    # hmap05_frame = frame_infos['heatmap5']['layouts']['frame']\n",
    "    chmbars_frame = frame_infos['chembars']['layouts']['frame']\n",
    "\n",
    "    # The PP Normalization Frame Rows\n",
    "    frame_row1a = row([bar01_frame, bar02_frame, bar03_frame], height=400, \n",
    "        sizing_mode=\"stretch_width\", background=background_color)\n",
    "    frame_row1b = row([bar04_frame, bar05_frame, bar06_frame], height=400, \n",
    "        sizing_mode=\"stretch_width\", background=background_color)\n",
    "    frame_row4 = row([bar19_frame, bar20_frame, bar21_frame, bar22_frame], \n",
    "        height=400, sizing_mode=\"stretch_width\", background=background_color)\n",
    "    frame_row5 = row([chmbars_frame], height=1100, sizing_mode=\"stretch_width\", \n",
    "        background=background_color)\n",
    "    \n",
    "    # # The PP Magnitude Frame Rows\n",
    "    # frame_row2a = row([bar07_frame, bar08_frame], height=400, \n",
    "    #     sizing_mode=\"stretch_width\", background=background_color)\n",
    "    # frame_row3a = row([bar09_frame, bar10_frame], height=400, \n",
    "    #     sizing_mode=\"stretch_width\", background=background_color)\n",
    "    # frame_row2b = row([bar11_frame, bar12_frame], height=400, \n",
    "    #     sizing_mode=\"stretch_width\", background=background_color)\n",
    "    # frame_row3b = row([bar13_frame, bar14_frame], height=400, \n",
    "    #     sizing_mode=\"stretch_width\", background=background_color)\n",
    "    # frame_row2c = row([bar15_frame, bar16_frame], height=400, \n",
    "    #     sizing_mode=\"stretch_width\", background=background_color)\n",
    "    # frame_row3c = row([bar17_frame, bar18_frame], height=400, \n",
    "    #     sizing_mode=\"stretch_width\", background=background_color)\n",
    "\n",
    "    # tab_rows23 = Tabs(tabs=[\n",
    "    #     TabPanel(child=column([frame_row2a, frame_row3a], \n",
    "    #         sizing_mode=\"stretch_both\", background=background_color), \n",
    "    #         title=renamer.encode('k_bins')),\n",
    "    #     TabPanel(child=column([frame_row2b, frame_row3b], \n",
    "    #         sizing_mode=\"stretch_both\", background=background_color), \n",
    "    #         title=renamer.encode('n_chem')),\n",
    "    #     TabPanel(child=column([frame_row2c, frame_row3c], \n",
    "    #         sizing_mode=\"stretch_both\", background=background_color), \n",
    "    #         title=renamer.encode('one'))])\n",
    "\n",
    "    # The PP Magnitude Frame Rows\n",
    "    frame_row2a = row([bar07_frame, bar08_frame, bar09_frame, bar10_frame], \n",
    "        height=400, sizing_mode=\"stretch_width\", background=background_color)\n",
    "    frame_row2b = row([bar11_frame, bar12_frame, bar13_frame, bar14_frame], \n",
    "        height=400, sizing_mode=\"stretch_width\", background=background_color)\n",
    "    frame_row2c = row([bar15_frame, bar16_frame, bar17_frame, bar18_frame], \n",
    "        height=400, sizing_mode=\"stretch_width\", background=background_color)\n",
    "\n",
    "    tab_rows23 = Tabs(tabs=[\n",
    "        TabPanel(child=frame_row2a, title=renamer.encode('k_bins')),\n",
    "        TabPanel(child=frame_row2b, title=renamer.encode('n_chem')),\n",
    "        TabPanel(child=frame_row2c, title=renamer.encode('one'))])\n",
    "\n",
    "    layout = column([ctrl_frame, frame_row1a, frame_row1b, tab_rows23, \n",
    "        frame_row4, frame_row5], sizing_mode=\"stretch_both\", \n",
    "        background=background_color)\n",
    "elif layout_dirctn == 'row':\n",
    "    ctrl_frame = frame_infos['datasel']['layofuts']['frame']\n",
    "    sctr_frame = frame_infos['sctrplts']['layouts']['frame']\n",
    "    layout = row([ctrl_frame, sctr_frame],\n",
    "        sizing_mode=\"stretch_both\", background=background_color)\n",
    "else:\n",
    "    raise ValueError(f'undefined layout_dirctn={layout_dirctn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrng_layout(attr, old, new):\n",
    "    magdim = changer.last_sceneid['magdim']\n",
    "    magdims = ['k_bins', 'n_chem', 'one']\n",
    "    idx_tab = magdims.index(magdim)\n",
    "    tab_rows23.active = idx_tab\n",
    "\n",
    "ctrl_info = ctrl_refs['magdim']['infos'][0]\n",
    "control = ctrl_info['control']\n",
    "ctrl_type = ctrl_info['type']\n",
    "control.on_change('active' if ctrl_type == 'radiobtngrp' else 'value', rearrng_layout)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "if doc_theme is not None:\n",
    "   curdoc().theme = doc_theme\n",
    "curdoc().add_root(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "bokeh_pane = pn.pane.Bokeh(layout, theme=doc_theme)\n",
    "bokeh_pane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
