# 数据预处理技术细节与数学原理

## 目录
1. [预处理模块架构](#预处理模块架构)
2. [基础变换操作](#基础变换操作)
3. [高级变换操作](#高级变换操作)
4. [参数推断机制](#参数推断机制)
5. [前向与反向变换](#前向与反向变换)
6. [预处理流水线](#预处理流水线)
7. [实际应用案例](#实际应用案例)

---

## 预处理模块架构

### 核心组件

项目中的预处理系统基于 `PreProcessor` 类实现，该类继承自 `nn.Module`，采用有向无环图（DAG）结构来组织变换操作。

#### 1. 图结构表示

预处理流程被组织为一个DAG，其中：
- **节点（Layers）**：表示各种变换操作
- **边（Edges）**：表示数据流向
- **拓扑排序**：确保变换按正确顺序执行

```python
# 从 n22_utils.py 中的实现
class PreProcessor(nn.Module):
    def __init__(self, pp_config: dict, device, dtype, cdtype):
        # 解析配置，构建DAG
        pp_layers = self.parse_config(pp_config)
        # 拓扑排序
        pp_layers, invpp_ts = self.sort_topological(pp_layers)
```

#### 2. 层类型分类

根据可逆性，层被分为三类：

**可逆层（Invertible Layers）**：
- `add`, `mul`, `shift`, `scale`
- `log`, `exp`, `sgnpow`, `pwr`, `pow`
- `normalize`, `cdf`
- `cat`, `stack`, `reshape`

**不可逆层（Non-invertible Layers）**：
- `pnorm`（单独使用时）
- 各种 `torch.*` 函数（如 `torch.sum`, `torch.mean`）

**特殊层（Special Layers）**：
- `input`: 输入层
- `output`: 输出层
- `pipe`: 管道层（直接传递）

---

## 基础变换操作

### 1. 加法变换（Add）

**数学公式**：

$$
y = x + c
$$

**实现**：
```python
# 前向变换
layer_mb = layer_input + layer_value

# 反向变换
layer_mb = layer_outmb - layer_outvalue
```

**应用场景**：
- 添加小的常数（如 `cnteps = 1000`）避免零值问题
- 数据偏移调整

### 2. 乘法变换（Mul）

**数学公式**：

$$
y = x \cdot c
$$

**实现**：
```python
# 前向变换
layer_mb = layer_input * layer_value

# 反向变换
layer_mb = layer_outmb / layer_outvalue
```

**约束条件**：
- 反向变换要求 $c \neq 0$

### 3. 平移变换（Shift）

**数学公式**：

$$
y = x - \mu
$$

其中 $\mu$ 是从训练数据推断的均值。

**实现**：
```python
# 参数推断（infer阶段）
mb_valsmean = layer_inpmbvals.sum(dim=mb_order, keepdim=True) / n_trn
pp_params[f'{layer_name}/value'] = nn.Parameter(mb_valsmean)

# 前向变换
layer_mb = layer_input - pp_params[f'{layer_name}/value']

# 反向变换
layer_mb = layer_outmb + layer_outvalue
```

**数学原理**：
- **均值中心化**：将数据分布的中心移到原点
- **维度约简**：可以沿指定维度计算均值

  $$
  \mu_d = \frac{1}{N} \sum_{i=1}^{N} x_{i,d}
  $$

  其中 $d$ 是约简维度，$N$ 是样本数

### 4. 缩放变换（Scale）

**数学公式**：

$$
y = \frac{x}{\sigma + \epsilon}
$$

其中 $\sigma$ 是从训练数据推断的标准差（或p-范数），$\epsilon$ 是防止除零的小常数。

**实现**：
```python
# 参数推断
layer_pnorm = layer_opts.get('pnorm', 2)  # 默认L2范数
mb_valssqmean = layer_inpmbvals.abs().pow(layer_pnorm).sum(dim=mb_order, keepdim=True) / n_trn
layer_eps = layer_opts.get('eps', 0.0)
layer_parammean2 = (layer_eps ** layer_pnorm) + mb_valssqmean
pp_params[f'{layer_name}/value'] = nn.Parameter(layer_parammean2.pow(1.0 / layer_pnorm))

# 前向变换
layer_mb = layer_input / pp_params[f'{layer_name}/value']

# 反向变换
layer_mb = layer_outmb * layer_outvalue
```

**数学原理**：

**L2范数标准化（pnorm=2）**：

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} |x_i|^2 + \epsilon^2}
$$

**L1范数标准化（pnorm=1）**：

$$
\sigma = \frac{1}{N} \sum_{i=1}^{N} |x_i| + \epsilon
$$

**维度约简**：
- 可以沿指定维度计算范数
- 结果形状：约简维度变为1，其他维度保持不变

**数值稳定性**：
- $\epsilon$ 防止除零错误
- 对于L2范数：$\sigma = \sqrt{\mathbb{E}[x^2] + \epsilon^2}$

---

## 高级变换操作

### 1. 对数变换（Log）

**数学公式**：

$$
y = \frac{\ln(x)}{\ln(b)}
$$

其中 $b$ 是对数底数（默认为自然对数 $e$）。

**实现**：
```python
# 前向变换
layer_logbase = pp_params[f'{layer_name}/logbase']
layer_mb = layer_input / layer_logbase

# 反向变换
layer_mb = (layer_outmb * layer_outlogbase).exp()
```

**数学原理**：
- **对数变换**将乘法关系转换为加法关系
- **压缩大值**：对数值域压缩，适合处理跨度大的数据
- **可逆性**：要求 $x > 0$

**常见应用**：
- 处理粒子计数数据（添加epsilon后取对数）
- 处理质量分布数据

### 2. 指数变换（Exp）

**数学公式**：

$$
y = b^x
$$

**实现**：
```python
# 前向变换
layer_expbase = pp_params[f'{layer_name}/expbase']
layer_mb = (layer_input * layer_expbase).exp()

# 反向变换
layer_mb = layer_outmb.log() / layer_outexpbase
```

### 3. 符号保持幂变换（SgnPow）

**数学公式**：

$$
y = \text{sign}(x) \cdot |x|^{\alpha}
$$

**实现**：
```python
# 前向变换
layer_power = pp_params[f'{layer_name}/exponent']
layer_mb = layer_input.abs().pow(layer_power) * layer_input.sign()

# 反向变换
layer_mb = layer_outmb.abs().pow(1.0 / layer_outpower) * layer_outmb.sign()
```

**数学原理**：
- **保持符号**：对于负值，先取绝对值，应用幂次，再恢复符号
- **可处理负值**：与普通幂变换不同，可以处理负输入
- **常用幂次**：
  - $\alpha = 0.5$：平方根变换
  - $\alpha = 2$：平方变换
  - $\alpha < 1$：压缩大值，扩展小值

### 4. P-范数（PNorm）

**数学公式**：

$$
\|x\|_p = \left(\sum_{i} |x_i|^p\right)^{1/p}
$$

**实现**：
```python
# 前向变换
layer_dim = layer_opts['dim']  # 约简维度
layer_pnorm = layer_opts['pnorm']  # p值（1或2）
layer_mb = layer_input.abs().pow(layer_pnorm).sum(dim=layer_dim, keepdim=True).pow(1.0 / layer_pnorm)
```

**数学原理**：
- **L1范数（p=1）**：$\|x\|_1 = \sum_i |x_i|$
- **L2范数（p=2）**：$\|x\|_2 = \sqrt{\sum_i x_i^2}$
- **维度约简**：沿指定维度计算范数，该维度变为1

**应用场景**：
- 计算质量直方图的总体量级
- 用于后续的归一化操作

### 5. 归一化（Normalize）

**数学公式**：

$$
y = \frac{x}{\|x\|_p}
$$

**实现**：
```python
# 前向变换
layer_dim = layer_opts['dim']
layer_pnorm = layer_opts['pnorm']
layer_mb = layer_input / layer_input.norm(p=layer_pnorm, dim=layer_dim, keepdim=True)
```

**数学原理**：
- **单位向量化**：将向量归一化为单位向量
- **方向保持**：只改变大小，不改变方向
- **可逆性**：需要保存原始范数值才能完全恢复

**反向变换**：
```python
# 需要同时有归一化层和pnorm层
lyrnrmlz_mbpnrm = lyrnrmlz_mb.abs().pow(lyrpnrm_pnorm).sum(dim=lyrpnrm_dim, keepdim=True).pow(1./lyrpnrm_pnorm)
layer_mbnrmlzd = lyrnrmlz_mb / lyrnrmlz_mbpnrm
layer_mb = lyrpnrm_mb * layer_mbnrmlzd
```

### 6. 累积分布函数变换（CDF）

**数学公式**：

$$
y = F_X(x) = P(X \leq x)
$$

**实现**：
```python
# 参数推断：计算分位数
n_qnts = layer_opts['bins'] + 1
q_cdfs = torch.linspace(0, 1, n_qnts)
layer_qntls = torch_quantile(layer_alldata, q_cdfs, dim=cdf_dims, keepdim=True)

# 前向变换：线性插值
alpha = (x - q_lo) / (q_hi - q_lo)
y = alpha * F(q_hi) + (1 - alpha) * F(q_lo)

# 反向变换：逆CDF（分位数函数）
alpha3 = layer_cdfs3 - layer_cdfs3.floor()
layer_mb = alpha3 * srch_valshi2 + (1 - alpha3) * srch_valslo2
```

**数学原理**：
- **分位数估计**：使用训练数据估计分位数
- **线性插值**：在分位数之间进行线性插值
- **均匀化**：将任意分布映射到[0,1]均匀分布
- **可逆性**：通过逆CDF可以恢复原始分布

**分位数计算**：

$$
Q(p) = \inf\{x: F_X(x) \geq p\}
$$

**线性插值**：

$$
F_X(x) \approx F_X(q_i) + \frac{x - q_i}{q_{i+1} - q_i} \cdot (F_X(q_{i+1}) - F_X(q_i))
$$

---

## 参数推断机制

### 推断流程

参数推断（`infer`方法）用于从训练数据中自动计算 `shift`、`scale` 和 `cdf` 层的参数。

#### 1. 推断轮次（Rounds）

由于某些层（如 `shift`、`scale`）的参数依赖于前面层的输出，系统采用**多轮推断**机制：

```python
# 计算最大轮次
n_totrnds = max(pp_layers[layer_name]['round'] for layer_name in pp_layers)

for i_rnd in range(n_totrnds):
    # 只推断当前轮次的参数
    for layer_name, layer_opts in pp_layers.items():
        layer_round = layer_opts['round']
        if (i_rnd == layer_round - 1) and (layer_type in ('shift', 'scale', 'cdf')):
            # 推断参数
```

**轮次分配规则**：
- `input` 层：round = 0
- `shift`/`scale` 层：round = 输入层的round + 1
- 其他层：round = 输入层的round

#### 2. 均值推断（Shift）

**数学公式**：

$$
\mu = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

**实现**：
```python
mb_valsmean = layer_inpmbvals.sum(dim=mb_order, keepdim=True) / n_trn
layer_parammean = layer_params.mean(dim=layer_dim, keepdim=True)  # 维度约简
```

**维度约简**：
- 可以沿指定维度计算均值
- 例如：`dim=[-1, -2]` 表示沿最后两个维度约简

#### 3. 标准差推断（Scale）

**L2范数（默认）**：

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} |x_i|^2 + \epsilon^2}
$$

**实现**：
```python
layer_pnorm = layer_opts.get('pnorm', 2)
mb_valssqmean = layer_inpmbvals.abs().pow(layer_pnorm).sum(dim=mb_order, keepdim=True) / n_trn
layer_eps = layer_opts.get('eps', 0.0)
layer_parammean2 = (layer_eps ** layer_pnorm) + mb_valssqmean
pp_params[f'{layer_name}/value'] = layer_parammean2.pow(1.0 / layer_pnorm)
```

**L1范数**：

$$
\sigma = \frac{1}{N} \sum_{i=1}^{N} |x_i| + \epsilon
$$

#### 4. CDF分位数推断

**实现**：
```python
# 收集所有mini-batch数据
layer_alldata = torch.cat(layer_params, dim=mb_order)

# 计算分位数
q_cdfs = torch.linspace(0, 1, n_qnts)
layer_qntls = torch_quantile(layer_alldata, q_cdfs, dim=cdf_dims, keepdim=True)

# 添加域边界
layer_qntls2 = torch.cat([
    torch.full(flshp, domain_min),  # 下界
    layer_qntls1,
    torch.full(flshp, domain_max)   # 上界
], dim=d_qtmp)
```

**数学原理**：
- **分位数定义**：$Q(p) = \inf\{x: F_X(x) \geq p\}$
- **线性插值**：在分位数之间使用线性插值
- **域扩展**：添加域边界确保所有值都在范围内

---

## 前向与反向变换

### 前向变换（Forward）

前向变换按照拓扑排序顺序执行：

```python
def forward(self, inputs_mb: dict, full: bool = False):
    # 拓扑排序后的层顺序
    for layer_name in pp_ts:
        layer_type = layer_opts['type']
        layer_input = layer_opts['input']
        
        # 根据类型执行变换
        if layer_type == 'add':
            layer_mb = layers_mb[layer_input] + pp_params[f'{layer_name}/value']
        elif layer_type == 'shift':
            layer_mb = layers_mb[layer_input] - pp_params[f'{layer_name}/value']
        # ... 其他变换
```

### 反向变换（Inverse）

反向变换按照逆拓扑排序顺序执行：

```python
def inverse(self, outputs_mb: dict, shaper=None, strict=True):
    # 逆拓扑排序
    for layer_name in invpp_ts:
        layer_outtype = layer_outopts['type']
        layer_output = layer_invinputs[0]
        
        # 根据类型执行逆变换
        if layer_outtype == 'add':
            layer_mb = layer_outmb - layer_outvalue
        elif layer_outtype == 'shift':
            layer_mb = layer_outmb + layer_outvalue
        # ... 其他逆变换
```

**可逆性约束**：
- `strict=True`：严格检查可逆性（如检查NaN、零值等）
- 某些变换（如 `normalize`）需要额外的信息才能完全恢复

---

## 预处理流水线

### 典型预处理流程

以气溶胶直方图数据为例（`polyhst` 模块）：

#### 第一部分：质量（Mass）处理

1. **添加epsilon**：

   $$
   M_1 = M_0 + \epsilon
   $$

2. **计算Lp范数**：

   $$
   M_2 = \|M_1\|_p
   $$

3. **幂变换**：

   $$
   M_3 = M_2^{\alpha}
   $$

4. **均值中心化**（可选）：

   $$
   M_4 = M_3 - \mu_3
   $$

5. **标准化**（可选）：

   $$
   M_5 = \frac{M_4}{\sigma_4 + \epsilon}
   $$

#### 第二部分：归一化（Normalization）

1. **归一化**：

   $$
   M_6 = \frac{M_1}{M_2} = \frac{M_1}{\|M_1\|_p}
   $$

2. **幂变换**：

   $$
   M_7 = M_6^{\alpha}
   $$

3. **均值中心化**（可选）：

   $$
   M_8 = M_7 - \mu_7
   $$

4. **标准化**（可选）：

   $$
   M_9 = \frac{M_8}{\sigma_8 + \epsilon}
   $$

#### 第三部分：计数（Count）处理

1. **添加epsilon**：

   $$
   N_1 = N_0 + \epsilon
   $$

2. **幂变换**：

   $$
   N_2 = N_1^{\alpha}
   $$

3. **均值中心化**（可选）：

   $$
   N_3 = N_2 - \mu_2
   $$

4. **标准化**（可选）：

   $$
   N_4 = \frac{N_3}{\sigma_3 + \epsilon}
   $$

### 配置示例

```yaml
ppmodules/polyhst:
  input:
    m_chmprthst: [n_seeds, n_mb, n_chem, n_bins]
    n_prthst:    [n_seeds, n_mb, 1, n_bins]
  
  hparams:
    nrmeps: 1e-100      # 质量epsilon
    magpnrm: 2          # L2范数
    magexp: null        # 质量幂次
    magshft: true       # 是否中心化
    magscl: true        # 是否标准化
    cnteps: 1000        # 计数epsilon
    cntexp: null        # 计数幂次
    # ... 其他参数
```

---

## 实际应用案例

### 案例1：质量直方图预处理

**输入**：`m_chmprthst` - 化学物质质量分布直方图

**处理流程**：
1. 添加小epsilon避免零值
2. 计算L2范数得到总质量
3. 归一化得到质量分数
4. 应用幂变换调整分布
5. 中心化和标准化

**数学表示**：

$$
\begin{align}
M_1 &= M_0 + \epsilon \\
M_2 &= \|M_1\|_2 \\
M_6 &= \frac{M_1}{M_2} \\
M_7 &= M_6^{\alpha} \\
M_8 &= M_7 - \mu_7 \\
M_9 &= \frac{M_8}{\sigma_8 + \epsilon}
\end{align}
$$

### 案例2：计数数据预处理

**输入**：`n_prthst` - 粒子计数直方图

**处理流程**：
1. 添加较大epsilon（如1000）避免零计数
2. 应用对数或幂变换
3. 中心化和标准化

**数学表示**：

$$
\begin{align}
N_1 &= N_0 + \epsilon \\
N_2 &= N_1^{\alpha} \quad \text{或} \quad \ln(N_1) \\
N_3 &= N_2 - \mu_2 \\
N_4 &= \frac{N_3}{\sigma_3 + \epsilon}
\end{align}
$$

### 案例3：条件VAE标签生成

**输入**：`m_chmprthst` - 化学物质质量分布

**处理流程**：
1. 计算每种化学物质的总质量
2. 计算质量分数
3. 选择特定化学物质（如OIN）
4. 计算CDF
5. 分位数分类

**数学表示**：

$$
\begin{align}
M_{\text{total}} &= \sum_{\text{bins}} M_{\text{chem}} \\
M_{\text{frac}} &= \frac{M_{\text{chem}}}{M_{\text{total}}} \\
F(x) &= P(M_{\text{frac}} \leq x) \\
\text{Label} &= \lfloor F(x) \cdot n_{\text{bins}} \rfloor
\end{align}
$$

---

## 总结

### 核心设计原则

1. **可逆性**：大多数变换都是可逆的，支持从输出恢复输入
2. **参数推断**：自动从训练数据推断参数（均值、标准差、分位数）
3. **维度灵活性**：支持沿任意维度进行约简操作
4. **数值稳定性**：使用epsilon防止除零和数值溢出

### 数学基础

- **线性代数**：范数、归一化、矩阵运算
- **概率论**：CDF、分位数、统计量
- **数值分析**：插值、数值稳定性

### 实现特点

- **图结构**：DAG表示变换流程
- **拓扑排序**：确保正确执行顺序
- **批处理**：支持mini-batch处理
- **缓存机制**：参数推断结果可缓存

---

## 参考文献

1. 项目代码：`partnn/notebooks/n22_utils.py` - `PreProcessor` 类
2. 配置文件：`configs/*/02_pp.yml` - 预处理模块配置
3. 主训练脚本：`partnn/vaepart.py` - 预处理集成

